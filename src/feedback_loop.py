"""Self-Learning Feedback Loop v2 â€” learns from outcomes and adapts AI behavior.

v1: passive tracking (log responses + outcomes)
v2: active learning:
  - Tags each AI response with detected closing technique + business niche
  - Aggregates conversion rates per technique/niche/style (30-day rolling window)
  - Generates concise adaptive instructions for AI prompt injection
  - Minimum sample sizes to avoid noise (Wilson score confidence)
  - Cached summaries (TTL 5 min) to avoid DB pressure
"""
import logging
import re
import time
import math
from typing import Optional, Dict, List, Tuple

from src.database import get_connection, DATABASE_URL

logger = logging.getLogger(__name__)

CLOSING_TECHNIQUES: Dict[str, Dict[str, str]] = {
    "trial_close": {
        "label": "Trial close",
        "patterns": r"ÐµÑÐ»Ð¸ Ð±Ñ‹ Ð¼Ñ‹ Ð¼Ð¾Ð³Ð»Ð¸|ÐµÑÐ»Ð¸ Ð±Ñ‹ ÑÑ‚Ð¾ Ð±Ñ‹Ð»Ð¾ Ð²Ð¾Ð·Ð¼Ð¾Ð¶Ð½Ð¾|Ð° ÐµÑÐ»Ð¸ Ð±Ñ‹|Ð¿Ñ€ÐµÐ´Ð¿Ð¾Ð»Ð¾Ð¶Ð¸Ð¼.{0,30}Ð½Ð°Ñ‡Ð°Ñ‚ÑŒ|Ð´Ð¾Ð¿ÑƒÑÑ‚Ð¸Ð¼.{0,30}Ð·Ð°Ð¿ÑƒÑÑ‚",
    },
    "assumptive_close": {
        "label": "Assumptive close",
        "patterns": r"Ð´Ð°Ð²Ð°Ð¹Ñ‚Ðµ Ð¾Ð¿Ñ€ÐµÐ´ÐµÐ»Ð¸Ð¼ÑÑ|Ð¾Ð¿Ñ€ÐµÐ´ÐµÐ»Ð¸Ð¼ÑÑ Ñ|ÐºÐ°ÐºÐ¾Ð¹ Ð²Ð°Ñ€Ð¸Ð°Ð½Ñ‚ Ð²Ð°Ð¼ Ð±Ð»Ð¸Ð¶Ðµ|ÑˆÐ°Ð±Ð»Ð¾Ð½.*Ð¿Ð¾Ð´Ñ…Ð¾Ð´Ð¸Ñ‚ Ð¸Ð»Ð¸|Ð¸Ñ‚Ð°Ðº.*Ð¾Ñ„Ð¾Ñ€Ð¼Ð»ÑÐµÐ¼",
    },
    "alternative_close": {
        "label": "ÐÐ»ÑŒÑ‚ÐµÑ€Ð½Ð°Ñ‚Ð¸Ð²Ð½Ñ‹Ð¹",
        "patterns": r"Ð²Ð°Ð¼ ÑƒÐ´Ð¾Ð±Ð½ÐµÐµ.{0,20}Ð¸Ð»Ð¸|Ð¿ÐµÑ€Ð²Ñ‹Ð¹ Ð²Ð°Ñ€Ð¸Ð°Ð½Ñ‚.{0,15}Ð²Ñ‚Ð¾Ñ€Ð¾Ð¹|Ð²Ð°Ñ€Ð¸Ð°Ð½Ñ‚ Ð.{0,15}Ð²Ð°Ñ€Ð¸Ð°Ð½Ñ‚ Ð‘|Ð½Ð° ÑÑ‚Ð¾Ð¹ Ð½ÐµÐ´ÐµÐ»Ðµ Ð¸Ð»Ð¸ Ð½Ð° ÑÐ»ÐµÐ´ÑƒÑŽÑ‰ÐµÐ¹|Ð½Ð°Ñ‡Ð°Ñ‚ÑŒ Ñ.{0,20}Ð¸Ð»Ð¸ Ñ",
    },
    "ben_franklin_close": {
        "label": "Ben Franklin close",
        "patterns": r"Ð´Ð°Ð²Ð°Ð¹Ñ‚Ðµ.*Ð¿Ð»ÑŽÑ.*Ð¼Ð¸Ð½ÑƒÑ|Ð·Ð° Ð¸ Ð¿Ñ€Ð¾Ñ‚Ð¸Ð²|Ð¿Ñ€ÐµÐ¸Ð¼ÑƒÑ‰ÐµÑÑ‚Ð²|Ð²Ð·Ð²ÐµÑÐ¸Ð¼|Ñ€Ð°Ð·Ð±ÐµÑ€Ñ‘Ð¼.*Ð·Ð° Ð¸|Ð¿Ð»ÑŽÑÑ‹.*Ð¼Ð¸Ð½ÑƒÑÑ‹",
    },
    "puppy_dog_close": {
        "label": "Puppy dog close",
        "patterns": r"Ð±ÐµÑÐ¿Ð»Ð°Ñ‚Ð½\w* Ñ€Ð°ÑÑ‡Ñ‘Ñ‚|Ð±ÐµÐ· Ð¾Ð±ÑÐ·Ð°Ñ‚ÐµÐ»ÑŒÑÑ‚Ð²|Ð¿Ñ€Ð¾ÑÑ‚Ð¾ Ð¿Ð¾ÑÐ¼Ð¾Ñ‚Ñ€Ð¸Ñ‚Ðµ|Ð½Ð¾Ð»ÑŒ Ð¾Ð±ÑÐ·Ð°Ñ‚ÐµÐ»ÑŒÑÑ‚Ð²|Ð½Ð¸ Ðº Ñ‡ÐµÐ¼Ñƒ Ð½Ðµ Ð¾Ð±ÑÐ·Ñ‹Ð²Ð°ÐµÑ‚|Ð±ÐµÑÐ¿Ð»Ð°Ñ‚Ð½\w* Ð°ÑƒÐ´Ð¸Ñ‚",
    },
    "summary_close": {
        "label": "Summary close",
        "patterns": r"Ð¸Ñ‚Ð°Ðº.*Ð´Ð¾Ð³Ð¾Ð²Ð¾Ñ€Ð¸Ð»Ð¸ÑÑŒ|Ð¿Ð¾Ð´Ð²ÐµÐ´Ñ‘Ð¼ Ð¸Ñ‚Ð¾Ð³|Ñ€ÐµÐ·ÑŽÐ¼Ð¸Ñ€ÑƒÑŽ|Ð´Ð°Ð²Ð°Ð¹Ñ‚Ðµ Ð¿Ð¾Ð´Ð²ÐµÐ´Ñ‘Ð¼|Ð¼Ñ‹ Ð¾Ð±ÑÑƒÐ´Ð¸Ð»Ð¸.*ÑÐ»ÐµÐ´ÑƒÑŽÑ‰Ð¸Ð¹ ÑˆÐ°Ð³|Ð¿Ð¾Ð´Ñ‹Ñ‚Ð¾Ð¶",
    },
    "inversion_close": {
        "label": "Inversion close (Sandler)",
        "patterns": r"Ð¼Ð¾Ð¶ÐµÑ‚.*Ð²Ð°Ð¼.*Ð½Ðµ Ð½ÑƒÐ¶Ð½Ð¾|Ð¼Ð¾Ð¶ÐµÑ‚.*ÑÑ‚Ð¾.*Ð½Ðµ Ð´Ð»Ñ Ð²Ð°Ñ|Ð´Ð°Ð²Ð°Ð¹Ñ‚Ðµ Ñ‡ÐµÑÑ‚Ð½Ð¾ Ñ€Ð°Ð·Ð±ÐµÑ€Ñ‘Ð¼ÑÑ|ÐµÑÐ»Ð¸ ÑÑ‚Ð¾ Ð½Ðµ Ð²Ð°ÑˆÐµ|Ð²Ð¾Ð·Ð¼Ð¾Ð¶Ð½Ð¾.*Ð½Ðµ Ð¿Ð¾Ð´Ñ…Ð¾Ð´Ð¸Ñ‚",
    },
    "takeaway_close": {
        "label": "Takeaway close",
        "patterns": r"Ð¼Ð¾Ð¶ÐµÐ¼ Ð¿Ð¾ÐºÐ° Ð½Ðµ Ð²ÐºÐ»ÑŽÑ‡Ð°Ñ‚ÑŒ|Ð½Ð°Ñ‡Ð½Ñ‘Ñ‚Ðµ Ñ Ð±Ð°Ð·Ñ‹|Ð±ÐµÐ·.*Ð¼Ð¾Ð´ÑƒÐ»Ñ|Ð¼Ð¾Ð¶Ð½Ð¾ Ð¾Ð±Ð¾Ð¹Ñ‚Ð¸ÑÑŒ Ð±ÐµÐ·|Ð¿Ð¾ÐºÐ° Ð½Ðµ Ð´Ð¾Ð±Ð°Ð²Ð»ÑÑ‚ÑŒ|Ð½Ðµ Ð¾Ð±ÑÐ·Ð°Ñ‚ÐµÐ»ÑŒÐ½Ð¾ Ð±Ñ€Ð°Ñ‚ÑŒ",
    },
    "future_pacing": {
        "label": "Future pacing (NLP)",
        "patterns": r"Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²ÑŒÑ‚Ðµ|Ñ‡ÐµÑ€ÐµÐ·.*Ð½ÐµÐ´ÐµÐ»\w.*ÐºÐ»Ð¸ÐµÐ½Ñ‚|Ñ‡ÐµÑ€ÐµÐ·.*Ð¼ÐµÑÑÑ†|Ð²Ð¾Ð¾Ð±Ñ€Ð°Ð·Ð¸Ñ‚Ðµ|Ð° Ñ‚ÐµÐ¿ÐµÑ€ÑŒ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð°Ð²ÑŒÑ‚Ðµ|Ñ‡ÐµÑ€ÐµÐ·.*Ð´Ð½ÐµÐ¹.*Ð²Ð°Ñˆ",
    },
    "sharp_angle_close": {
        "label": "Sharp angle close",
        "patterns": r"ÐµÑÐ»Ð¸ Ñ.*Ð´Ð¾Ð±Ð°Ð²Ð»ÑŽ|ÐµÑÐ»Ð¸ Ð¼Ñ‹.*Ð²ÐºÐ»ÑŽÑ‡Ð¸Ð¼|ÐµÑÐ»Ð¸.*Ð±ÐµÑÐ¿Ð»Ð°Ñ‚Ð½Ñ‹Ð¹ Ð¼ÐµÑÑÑ†|ÐµÑÐ»Ð¸.*ÑÐºÐ¸Ð´ÐºÑƒ.*Ð½Ð°Ñ‡Ð½Ñ‘Ð¼|Ð¿Ñ€Ð¸ ÑƒÑÐ»Ð¾Ð²Ð¸Ð¸.*Ð½Ð°Ñ‡Ð¸Ð½Ð°ÐµÐ¼",
    },
    "jolt_close": {
        "label": "JOLT close",
        "patterns": r"Ñ Ñ€ÐµÐºÐ¾Ð¼ÐµÐ½Ð´ÑƒÑŽ Ð¸Ð¼ÐµÐ½Ð½Ð¾|Ð²Ð¾Ñ‚ Ð¿Ð¾Ñ‡ÐµÐ¼Ñƒ.*Ñ€Ð¸ÑÐº Ð½ÑƒÐ»ÐµÐ²Ð¾Ð¹|Ð¼Ð¾Ñ Ñ€ÐµÐºÐ¾Ð¼ÐµÐ½Ð´Ð°Ñ†Ð¸Ñ|ÐºÐ°Ðº ÑÐºÑÐ¿ÐµÑ€Ñ‚.*ÑÐ¾Ð²ÐµÑ‚ÑƒÑŽ|Ð¿Ñ€ÐµÐ´Ð¾Ð¿Ð»Ð°Ñ‚Ð°.*Ð¿Ñ€Ð°Ð²Ðº.*Ð²Ð¾Ð·Ð²Ñ€Ð°Ñ‚",
    },
    "negative_reverse_close": {
        "label": "Negative reverse close",
        "patterns": r"Ð²Ð°Ð¼ ÑÑ‚Ð¾ Ð²Ð¾Ð¾Ð±Ñ‰Ðµ Ð½Ðµ Ð½ÑƒÐ¶Ð½Ð¾|Ð¼Ð¾Ð¶ÐµÑ‚.*Ð½Ðµ ÑÑ‚Ð¾Ð¸Ñ‚|ÐºÐ°ÐºÑƒÑŽ Ð·Ð°Ð´Ð°Ñ‡Ñƒ.*Ð¿Ñ‹Ñ‚Ð°ÐµÑ‚ÐµÑÑŒ Ñ€ÐµÑˆÐ¸Ñ‚ÑŒ|Ð´Ð°Ð²Ð°Ð¹Ñ‚Ðµ.*Ñ‡ÐµÑÑ‚Ð½Ð¾.*Ð½ÑƒÐ¶Ð½Ð¾ Ð»Ð¸",
    },
    "nepq_close": {
        "label": "NEPQ commitment close",
        "patterns": r"ÐµÑÐ»Ð¸ Ð±Ñ‹ Ð¼Ñ‹ Ð¼Ð¾Ð³Ð»Ð¸ Ñ€ÐµÑˆÐ¸Ñ‚ÑŒ|ÑÑ‚Ð¾ Ð±Ñ‹Ð»Ð¾ Ð±Ñ‹ Ð¿Ð¾Ð»ÐµÐ·Ð½Ð¾ Ð´Ð»Ñ|Ð¿Ð¾Ð¼Ð¾Ð³Ð»Ð¾ Ð±Ñ‹.*Ð±Ð¸Ð·Ð½ÐµÑ|Ñ€ÐµÑˆÐ¸Ñ‚ÑŒ.*Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼Ñƒ.*Ð·Ð°.*Ð´Ð½ÐµÐ¹",
    },
}

NICHE_PATTERNS: Dict[str, Dict[str, str]] = {
    "restaurant": {
        "label": "Ð ÐµÑÑ‚Ð¾Ñ€Ð°Ð½Ñ‹/ÐšÐ°Ñ„Ðµ",
        "patterns": r"Ñ€ÐµÑÑ‚Ð¾Ñ€|ÐºÐ°Ñ„Ðµ|ÐºÐ¾Ñ„ÐµÐ¹Ð½|Ð±Ð°Ñ€|ÑÑ‚Ð¾Ð»Ð¾Ð²|Ð¿ÐµÐºÐ°Ñ€Ð½|ÐµÐ´Ð°|food|ÐºÑƒÑ…Ð½|Ð¿Ð¾Ð²Ð°Ñ€|Ð¼ÐµÐ½ÑŽ|Ð±Ð»ÑŽÐ´",
    },
    "shop": {
        "label": "ÐœÐ°Ð³Ð°Ð·Ð¸Ð½Ñ‹/E-commerce",
        "patterns": r"Ð¼Ð°Ð³Ð°Ð·Ð¸Ð½|Ð±ÑƒÑ‚Ð¸Ðº|Ð¸Ð½Ñ‚ÐµÑ€Ð½ÐµÑ‚-Ð¼Ð°Ð³Ð°Ð·Ð¸Ð½|Ð¼Ð°Ñ€ÐºÐµÑ‚Ð¿Ð»ÐµÐ¹Ñ|Ñ‚Ð¾Ð²Ð°Ñ€|Ð¿Ñ€Ð¾Ð´Ð°Ð¶|ÐºÐ°Ñ‚Ð°Ð»Ð¾Ð³|Ð°ÑÑÐ¾Ñ€Ñ‚Ð¸Ð¼ÐµÐ½Ñ‚|ÑÐºÐ»Ð°Ð´|Ð¾Ð¿Ñ‚|Ñ€Ð¾Ð·Ð½Ð¸Ñ†",
    },
    "beauty": {
        "label": "Ð‘ÑŒÑŽÑ‚Ð¸/Ð¡Ð°Ð»Ð¾Ð½Ñ‹",
        "patterns": r"ÑÐ°Ð»Ð¾Ð½|Ð¿Ð°Ñ€Ð¸ÐºÐ¼Ð°Ñ…ÐµÑ€|Ð±Ð°Ñ€Ð±ÐµÑ€ÑˆÐ¾Ð¿|Ð¼Ð°Ð½Ð¸ÐºÑŽÑ€|ÐºÐ¾ÑÐ¼ÐµÑ‚Ð¾Ð»Ð¾Ð³|Ð²Ð¸Ð·Ð°Ð¶|ÑÐ¿Ð°|spa|nail|ÐºÑ€Ð°ÑÐ¾Ñ‚|ÑÑ‚Ñ€Ð¸Ð¶Ðº",
    },
    "fitness": {
        "label": "Ð¤Ð¸Ñ‚Ð½ÐµÑ/Ð¡Ð¿Ð¾Ñ€Ñ‚",
        "patterns": r"Ñ„Ð¸Ñ‚Ð½ÐµÑ|ÑÐ¿Ð¾Ñ€Ñ‚Ð·Ð°Ð»|Ñ‚Ñ€ÐµÐ½Ð°Ð¶Ñ‘Ñ€|Ð¹Ð¾Ð³Ð°|Ð¿Ð¸Ð»Ð°Ñ‚ÐµÑ|ÐºÑ€Ð¾ÑÑÑ„Ð¸Ñ‚|Ñ‚Ñ€ÐµÐ½ÐµÑ€|Ð±Ð°ÑÑÐµÐ¹Ð½|ÑÐ¿Ð¾Ñ€Ñ‚|Ð·Ð°Ð»",
    },
    "medical": {
        "label": "ÐœÐµÐ´Ð¸Ñ†Ð¸Ð½Ð°/ÐšÐ»Ð¸Ð½Ð¸ÐºÐ¸",
        "patterns": r"ÐºÐ»Ð¸Ð½Ð¸Ðº|Ð±Ð¾Ð»ÑŒÐ½Ð¸Ñ†|ÑÑ‚Ð¾Ð¼Ð°Ñ‚Ð¾Ð»Ð¾Ð³|Ð°Ð¿Ñ‚ÐµÐº|Ð»Ð°Ð±Ð¾Ñ€Ð°Ñ‚Ð¾Ñ€|Ð²Ñ€Ð°Ñ‡|Ð¼ÐµÐ´Ð¸Ñ†|Ð·Ð´Ð¾Ñ€Ð¾Ð²|Ð´Ð¸Ð°Ð³Ð½Ð¾ÑÑ‚|Ð°Ð½Ð°Ð»Ð¸Ð·",
    },
    "education": {
        "label": "ÐžÐ±Ñ€Ð°Ð·Ð¾Ð²Ð°Ð½Ð¸Ðµ/ÐšÑƒÑ€ÑÑ‹",
        "patterns": r"ÐºÑƒÑ€Ñ|ÑˆÐºÐ¾Ð»|Ð¾Ð±ÑƒÑ‡ÐµÐ½|ÑƒÐ½Ð¸Ð²ÐµÑ€ÑÐ¸Ñ‚ÐµÑ‚|Ñ€ÐµÐ¿ÐµÑ‚Ð¸Ñ‚Ð¾Ñ€|Ð¾Ð±Ñ€Ð°Ð·Ð¾Ð²Ð°Ð½|ÑƒÑ€Ð¾Ðº|ÑÑ‚ÑƒÐ´ÐµÐ½Ñ‚|Ð»ÐµÐºÑ†Ð¸|Ñ‚Ñ€ÐµÐ½Ð¸Ð½Ð³",
    },
    "delivery": {
        "label": "Ð”Ð¾ÑÑ‚Ð°Ð²ÐºÐ°",
        "patterns": r"Ð´Ð¾ÑÑ‚Ð°Ð²Ðº|ÐºÑƒÑ€ÑŒÐµÑ€|ÑÑƒÑˆÐ¸|Ð¿Ð¸Ñ†Ñ†|food delivery|Ð»Ð¾Ð³Ð¸ÑÑ‚Ð¸Ðº|Ð¿ÐµÑ€ÐµÐ²Ð¾Ð·",
    },
    "services": {
        "label": "Ð£ÑÐ»ÑƒÐ³Ð¸/Ð¡ÐµÑ€Ð²Ð¸Ñ",
        "patterns": r"ÑƒÑÐ»ÑƒÐ³|Ñ€ÐµÐ¼Ð¾Ð½Ñ‚|ÐºÐ»Ð¸Ð½Ð¸Ð½Ð³|Ð°Ð²Ñ‚Ð¾ÑÐµÑ€Ð²Ð¸Ñ|Ñ…Ð¸Ð¼Ñ‡Ð¸ÑÑ‚Ðº|Ð¼Ð°ÑÑ‚ÐµÑ€|ÑÐµÑ€Ð²Ð¸Ñ|Ð¼Ð¾Ð½Ñ‚Ð°Ð¶|ÑƒÑÑ‚Ð°Ð½Ð¾Ð²Ðº",
    },
    "realestate": {
        "label": "ÐÐµÐ´Ð²Ð¸Ð¶Ð¸Ð¼Ð¾ÑÑ‚ÑŒ",
        "patterns": r"Ð½ÐµÐ´Ð²Ð¸Ð¶Ð¸Ð¼Ð¾ÑÑ‚|ÐºÐ²Ð°Ñ€Ñ‚Ð¸Ñ€|Ð´Ð¾Ð¼|Ð°Ñ€ÐµÐ½Ð´|Ñ€Ð¸ÑÐ»Ñ‚Ð¾Ñ€|Ð¶Ð¸Ð»ÑŒÑ‘|Ð¸Ð¿Ð¾Ñ‚ÐµÐº|Ð·Ð°ÑÑ‚Ñ€Ð¾Ð¹Ñ‰Ð¸Ðº",
    },
    "travel": {
        "label": "Ð¢ÑƒÑ€Ð¸Ð·Ð¼/ÐŸÑƒÑ‚ÐµÑˆÐµÑÑ‚Ð²Ð¸Ñ",
        "patterns": r"Ñ‚ÑƒÑ€|Ð¿ÑƒÑ‚ÐµÑˆÐµÑÑ‚Ð²|Ð¾Ñ‚ÐµÐ»ÑŒ|Ð³Ð¾ÑÑ‚Ð¸Ð½Ð¸Ñ†|Ð±Ñ€Ð¾Ð½Ð¸Ñ€Ð¾Ð²Ð°Ð½|ÑÐºÑÐºÑƒÑ€ÑÐ¸|Ð°Ð²Ð¸Ð°Ð±Ð¸Ð»ÐµÑ‚|travel|Ð¾Ñ‚Ð´Ñ‹Ñ…",
    },
}

STYLE_PATTERNS: Dict[str, str] = {
    "formal": r"ÑƒÐ²Ð°Ð¶Ð°ÐµÐ¼\w+|Ð¿Ñ€Ð¾ÑˆÑƒ.*Ñ€Ð°ÑÑÐ¼Ð¾Ñ‚Ñ€|Ð±Ð»Ð°Ð³Ð¾Ð´Ð°Ñ€\w+|Ð¾Ñ„Ð¸Ñ†Ð¸Ð°Ð»ÑŒÐ½|Ð´Ð¾Ð³Ð¾Ð²Ð¾Ñ€|ÑŽÑ€Ð¸Ð´Ð¸Ñ‡ÐµÑÐº|ÐžÐžÐž|Ð˜ÐŸ\s",
    "casual": r"\)\s*$|Ñ…Ð°Ñ…|Ð°Ñ…Ð°Ñ…|Ð»Ð¾Ð»|Ð¾Ðº\b|Ð¿Ñ€Ð¸Ð²|Ð·Ð´Ð°Ñ€Ð¾Ð²|Ñ‡ÐµÐ»\b|Ð³Ð¾\b|Ð½Ð¾Ñ€Ð¼\b|ÐºÑÑ‚|Ð¾Ñ‡\b",
    "analytical": r"ÑÑ€Ð°Ð²Ð½Ð¸Ñ‚|Ð°Ð½Ð°Ð»Ð¸Ñ‚Ð¸Ðº|ROI|Ð¼ÐµÑ‚Ñ€Ð¸Ðº|ÐºÐ¾Ð½Ð²ÐµÑ€ÑÐ¸|ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸Ðº|Ð´Ð°Ð½Ð½\w+|Ð¿Ð¾ÐºÐ°Ð·Ð°Ñ‚ÐµÐ»|KPI|Ð¿Ñ€Ð¾Ñ†ÐµÐ½Ñ‚",
    "emotional": r"Ð¼ÐµÑ‡Ñ‚Ð°|Ñ…Ð¾Ñ‡Ñƒ|Ð½Ñ€Ð°Ð²Ð¸Ñ‚ÑÑ|ÐºÑ€ÑƒÑ‚Ð¾|Ð²Ð°Ñƒ|Ð¾Ñ„Ð¸Ð³ÐµÐ½Ð½Ð¾|Ð¾Ð±Ð¾Ð¶Ð°ÑŽ|ÑÑƒÐ¿ÐµÑ€|ÐºÐ»Ð°ÑÑ|Ð²Ð¾ÑÑ‚Ð¾Ñ€Ð³|Ð¿Ð¾Ñ‚Ñ€ÑÑÐ°ÑŽÑ‰Ðµ",
    "skeptical": r"ÑÐ¾Ð¼Ð½ÐµÐ²Ð°|Ð½Ðµ ÑƒÐ²ÐµÑ€ÐµÐ½|Ð° Ð²Ð´Ñ€ÑƒÐ³|Ð³Ð°Ñ€Ð°Ð½Ñ‚Ð¸Ð¸|Ñ€Ð¸ÑÐºÐ¾Ð²|Ð¾Ð±Ð¼Ð°Ð½|Ñ€Ð°Ð·Ð²Ð¾Ð´|ÐºÐ¸Ð´Ð°Ð»|Ð½Ðµ Ð²ÐµÑ€ÑŽ|Ð´Ð¾ÐºÐ°Ð¶Ð¸Ñ‚Ðµ",
}

_insights_cache: Dict[str, Tuple[float, object]] = {}
_CACHE_TTL = 300


def _wilson_score(successes: int, total: int, z: float = 1.96) -> float:
    if total == 0:
        return 0.0
    p = successes / total
    denominator = 1 + z * z / total
    centre = p + z * z / (2 * total)
    adjustment = z * math.sqrt((p * (1 - p) + z * z / (4 * total)) / total)
    return (centre - adjustment) / denominator


class FeedbackLoop:
    def __init__(self):
        self._init_db()

    def _init_db(self):
        if not DATABASE_URL:
            return

        try:
            with get_connection() as conn:
                with conn.cursor() as cur:
                    cur.execute("""
                        CREATE TABLE IF NOT EXISTS response_outcomes (
                            id SERIAL PRIMARY KEY,
                            user_id BIGINT NOT NULL,
                            message_text TEXT,
                            response_text TEXT,
                            response_variant VARCHAR(20),
                            funnel_stage VARCHAR(30),
                            propensity_score INT,
                            outcome_type VARCHAR(30) NULL,
                            outcome_at TIMESTAMP NULL,
                            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                        )
                    """)
                    cur.execute("CREATE INDEX IF NOT EXISTS idx_response_outcomes_user_id ON response_outcomes(user_id)")
                    cur.execute("CREATE INDEX IF NOT EXISTS idx_response_outcomes_outcome_type ON response_outcomes(outcome_type)")
                    cur.execute("CREATE INDEX IF NOT EXISTS idx_response_outcomes_created_at ON response_outcomes(created_at)")

                    cur.execute("""
                        CREATE TABLE IF NOT EXISTS response_tags (
                            id SERIAL PRIMARY KEY,
                            response_id INT NOT NULL REFERENCES response_outcomes(id) ON DELETE CASCADE,
                            tag_type VARCHAR(20) NOT NULL,
                            tag_value VARCHAR(50) NOT NULL,
                            confidence REAL DEFAULT 1.0,
                            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                        )
                    """)
                    cur.execute("CREATE INDEX IF NOT EXISTS idx_response_tags_response_id ON response_tags(response_id)")
                    cur.execute("CREATE INDEX IF NOT EXISTS idx_response_tags_type_value ON response_tags(tag_type, tag_value)")

                    cur.execute("""
                        CREATE TABLE IF NOT EXISTS niche_style_memory (
                            id SERIAL PRIMARY KEY,
                            niche VARCHAR(30) NOT NULL,
                            preferred_style VARCHAR(30),
                            preferred_techniques TEXT,
                            avoid_techniques TEXT,
                            custom_hint TEXT,
                            sample_size INT DEFAULT 0,
                            conversion_rate REAL DEFAULT 0.0,
                            updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                            UNIQUE(niche)
                        )
                    """)
            logger.info("Self-Learning Loop v2 tables initialized")
        except Exception as e:
            logger.error(f"Failed to init feedback tables: {e}")

    def log_response(self, user_id: int, message_text: str, response_text: str,
                     variant: Optional[str] = None, funnel_stage: Optional[str] = None,
                     propensity_score: Optional[int] = None) -> int:
        if not DATABASE_URL:
            return 0

        try:
            with get_connection() as conn:
                with conn.cursor() as cur:
                    cur.execute("""
                        INSERT INTO response_outcomes
                        (user_id, message_text, response_text, response_variant, funnel_stage, propensity_score)
                        VALUES (%s, %s, %s, %s, %s, %s)
                        RETURNING id
                    """, (user_id, message_text, response_text, variant, funnel_stage, propensity_score))
                    result = cur.fetchone()
                    response_id = result[0] if result else 0

            if response_id > 0:
                self._auto_tag_response(response_id, message_text or "", response_text or "")

            return response_id
        except Exception as e:
            logger.error(f"Failed to log response: {e}")
            return 0

    def _auto_tag_response(self, response_id: int, user_message: str, ai_response: str):
        try:
            combined = f"{user_message} {ai_response}".lower()
            tags: List[Tuple[str, str, float]] = []

            for tech_id, tech_info in CLOSING_TECHNIQUES.items():
                if re.search(tech_info["patterns"], ai_response, re.IGNORECASE):
                    tags.append(("technique", tech_id, 0.85))

            for niche_id, niche_info in NICHE_PATTERNS.items():
                if re.search(niche_info["patterns"], user_message, re.IGNORECASE):
                    tags.append(("niche", niche_id, 0.9))

            for style_id, pattern in STYLE_PATTERNS.items():
                if re.search(pattern, user_message, re.IGNORECASE):
                    tags.append(("style", style_id, 0.7))

            if not tags:
                return

            with get_connection() as conn:
                with conn.cursor() as cur:
                    for tag_type, tag_value, confidence in tags:
                        cur.execute("""
                            INSERT INTO response_tags (response_id, tag_type, tag_value, confidence)
                            VALUES (%s, %s, %s, %s)
                        """, (response_id, tag_type, tag_value, confidence))
        except Exception as e:
            logger.debug(f"Auto-tagging skipped: {e}")

    def record_outcome(self, user_id: int, outcome_type: str) -> bool:
        if not DATABASE_URL:
            return False

        try:
            with get_connection() as conn:
                with conn.cursor() as cur:
                    cur.execute("""
                        UPDATE response_outcomes
                        SET outcome_type = %s, outcome_at = NOW()
                        WHERE id = (
                            SELECT id FROM response_outcomes
                            WHERE user_id = %s AND outcome_type IS NULL
                            ORDER BY created_at DESC
                            LIMIT 1
                        )
                    """, (outcome_type, user_id))
                    return cur.rowcount > 0
        except Exception as e:
            logger.error(f"Failed to record outcome for user {user_id}: {e}")
            return False

    def record_outcome_by_id(self, response_id: int, outcome_type: str) -> bool:
        if not DATABASE_URL:
            return False

        try:
            with get_connection() as conn:
                with conn.cursor() as cur:
                    cur.execute("""
                        UPDATE response_outcomes
                        SET outcome_type = %s, outcome_at = NOW()
                        WHERE id = %s
                    """, (outcome_type, response_id))
                    return cur.rowcount > 0
        except Exception as e:
            logger.error(f"Failed to record outcome for response {response_id}: {e}")
            return False

    def get_best_techniques(self, niche: Optional[str] = None,
                            days: int = 30, min_samples: int = 10) -> List[Dict]:
        cache_key = f"best_techniques:{niche}:{days}"
        cached = _insights_cache.get(cache_key)
        if cached and (time.time() - cached[0]) < _CACHE_TTL:
            return cached[1]  # type: ignore

        if not DATABASE_URL:
            return []

        try:
            niche_filter = ""
            params: list = [days]
            if niche:
                niche_filter = """
                    AND ro.id IN (
                        SELECT rt2.response_id FROM response_tags rt2
                        WHERE rt2.tag_type = 'niche' AND rt2.tag_value = %s
                    )
                """
                params.append(niche)

            with get_connection() as conn:
                with conn.cursor() as cur:
                    cur.execute(f"""
                        SELECT
                            rt.tag_value AS technique,
                            COUNT(*) AS total,
                            COUNT(ro.outcome_type) AS converted,
                            ROUND(COUNT(ro.outcome_type)::numeric / NULLIF(COUNT(*), 0) * 100, 1) AS rate
                        FROM response_tags rt
                        JOIN response_outcomes ro ON rt.response_id = ro.id
                        WHERE rt.tag_type = 'technique'
                          AND ro.created_at >= NOW() - %s * INTERVAL '1 day'
                          {niche_filter}
                        GROUP BY rt.tag_value
                        HAVING COUNT(*) >= {min_samples}
                        ORDER BY rate DESC
                    """, params)

                    results = []
                    for row in cur.fetchall():
                        tech_id = row[0]
                        total = row[1]
                        converted = row[2]
                        raw_rate = float(row[3])
                        wilson = round(_wilson_score(converted, total) * 100, 1)
                        tech_info = CLOSING_TECHNIQUES.get(tech_id, {})
                        results.append({
                            "technique_id": tech_id,
                            "label": tech_info.get("label", tech_id),
                            "total": total,
                            "converted": converted,
                            "raw_rate": raw_rate,
                            "wilson_score": wilson,
                        })

                    results.sort(key=lambda x: x["wilson_score"], reverse=True)

            _insights_cache[cache_key] = (time.time(), results)
            return results
        except Exception as e:
            logger.error(f"Failed to get best techniques: {e}")
            return []

    def get_niche_insights(self, niche: str, days: int = 30,
                           min_samples: int = 5) -> Optional[Dict]:
        cache_key = f"niche_insights:{niche}:{days}"
        cached = _insights_cache.get(cache_key)
        if cached and (time.time() - cached[0]) < _CACHE_TTL:
            return cached[1]  # type: ignore

        if not DATABASE_URL:
            return None

        try:
            with get_connection() as conn:
                with conn.cursor() as cur:
                    cur.execute("""
                        SELECT
                            rt_style.tag_value AS style,
                            COUNT(*) AS total,
                            COUNT(ro.outcome_type) AS converted
                        FROM response_tags rt_niche
                        JOIN response_outcomes ro ON rt_niche.response_id = ro.id
                        JOIN response_tags rt_style ON rt_style.response_id = ro.id AND rt_style.tag_type = 'style'
                        WHERE rt_niche.tag_type = 'niche'
                          AND rt_niche.tag_value = %s
                          AND ro.created_at >= NOW() - %s * INTERVAL '1 day'
                        GROUP BY rt_style.tag_value
                        HAVING COUNT(*) >= %s
                        ORDER BY COUNT(ro.outcome_type)::numeric / NULLIF(COUNT(*), 0) DESC
                        LIMIT 3
                    """, (niche, days, min_samples))
                    style_rows = cur.fetchall()

                    cur.execute("""
                        SELECT
                            rt_tech.tag_value AS technique,
                            COUNT(*) AS total,
                            COUNT(ro.outcome_type) AS converted
                        FROM response_tags rt_niche
                        JOIN response_outcomes ro ON rt_niche.response_id = ro.id
                        JOIN response_tags rt_tech ON rt_tech.response_id = ro.id AND rt_tech.tag_type = 'technique'
                        WHERE rt_niche.tag_type = 'niche'
                          AND rt_niche.tag_value = %s
                          AND ro.created_at >= NOW() - %s * INTERVAL '1 day'
                        GROUP BY rt_tech.tag_value
                        HAVING COUNT(*) >= %s
                        ORDER BY COUNT(ro.outcome_type)::numeric / NULLIF(COUNT(*), 0) DESC
                        LIMIT 3
                    """, (niche, days, min_samples))
                    tech_rows = cur.fetchall()

                    cur.execute("""
                        SELECT
                            rt_tech.tag_value AS technique,
                            COUNT(*) AS total,
                            COUNT(ro.outcome_type) AS converted
                        FROM response_tags rt_niche
                        JOIN response_outcomes ro ON rt_niche.response_id = ro.id
                        JOIN response_tags rt_tech ON rt_tech.response_id = ro.id AND rt_tech.tag_type = 'technique'
                        WHERE rt_niche.tag_type = 'niche'
                          AND rt_niche.tag_value = %s
                          AND ro.created_at >= NOW() - %s * INTERVAL '1 day'
                        GROUP BY rt_tech.tag_value
                        HAVING COUNT(*) >= %s
                          AND COUNT(ro.outcome_type)::numeric / NULLIF(COUNT(*), 0) < 0.05
                        ORDER BY COUNT(ro.outcome_type)::numeric / NULLIF(COUNT(*), 0) ASC
                        LIMIT 2
                    """, (niche, days, min_samples))
                    avoid_rows = cur.fetchall()

            if not style_rows and not tech_rows:
                _insights_cache[cache_key] = (time.time(), None)
                return None

            niche_info = NICHE_PATTERNS.get(niche, {})
            result = {
                "niche": niche,
                "niche_label": niche_info.get("label", niche),
                "best_styles": [
                    {"style": r[0], "total": r[1], "converted": r[2],
                     "rate": round(r[2] / r[1] * 100, 1) if r[1] > 0 else 0}
                    for r in style_rows
                ],
                "best_techniques": [
                    {"technique": r[0],
                     "label": CLOSING_TECHNIQUES.get(r[0], {}).get("label", r[0]),
                     "total": r[1], "converted": r[2],
                     "rate": round(r[2] / r[1] * 100, 1) if r[1] > 0 else 0}
                    for r in tech_rows
                ],
                "avoid_techniques": [
                    {"technique": r[0],
                     "label": CLOSING_TECHNIQUES.get(r[0], {}).get("label", r[0]),
                     "total": r[1], "converted": r[2]}
                    for r in avoid_rows
                ],
            }

            _insights_cache[cache_key] = (time.time(), result)
            return result
        except Exception as e:
            logger.error(f"Failed to get niche insights: {e}")
            return None

    def get_adaptive_instructions(self, user_id: int,
                                  user_message: str,
                                  funnel_stage: Optional[str] = None) -> Optional[str]:
        cache_key = f"adaptive:{user_id}"
        cached = _insights_cache.get(cache_key)
        if cached and (time.time() - cached[0]) < 120:
            return cached[1]  # type: ignore

        parts: List[str] = []

        detected_niche = None
        msg_lower = user_message.lower()
        for niche_id, niche_info in NICHE_PATTERNS.items():
            if re.search(niche_info["patterns"], msg_lower):
                detected_niche = niche_id
                break

        if not detected_niche:
            detected_niche = self._get_user_niche(user_id)

        best_global = self.get_best_techniques(niche=None, days=30, min_samples=10)
        if best_global and len(best_global) >= 2:
            top2 = best_global[:2]
            labels = [f"{t['label']} ({t['wilson_score']}%)" for t in top2]
            parts.append(
                f"[Ð¡ÐÐœÐžÐžÐ‘Ð£Ð§Ð•ÐÐ˜Ð•] Ð›ÑƒÑ‡ÑˆÐ¸Ðµ Ñ‚ÐµÑ…Ð½Ð¸ÐºÐ¸ Ð·Ð°ÐºÑ€Ñ‹Ñ‚Ð¸Ñ (Ð¿Ð¾ Ð´Ð°Ð½Ð½Ñ‹Ð¼ Ð·Ð° 30 Ð´Ð½ÐµÐ¹): {', '.join(labels)}. "
                f"ÐŸÑ€Ð¸Ð¾Ñ€Ð¸Ñ‚Ð¸Ð·Ð¸Ñ€ÑƒÐ¹ Ð¸Ñ…, ÐºÐ¾Ð³Ð´Ð° ÑƒÐ¼ÐµÑÑ‚Ð½Ð¾."
            )

        if detected_niche:
            niche_data = self.get_niche_insights(detected_niche, days=30, min_samples=5)
            if niche_data:
                niche_parts = []
                if niche_data["best_techniques"]:
                    tech_labels = [t["label"] for t in niche_data["best_techniques"][:2]]
                    niche_parts.append(f"Ð»ÑƒÑ‡ÑˆÐ¸Ðµ Ñ‚ÐµÑ…Ð½Ð¸ÐºÐ¸: {', '.join(tech_labels)}")
                if niche_data["best_styles"]:
                    style = niche_data["best_styles"][0]["style"]
                    style_labels = {
                        "formal": "Ñ„Ð¾Ñ€Ð¼Ð°Ð»ÑŒÐ½Ñ‹Ð¹",
                        "casual": "Ð½ÐµÑ„Ð¾Ñ€Ð¼Ð°Ð»ÑŒÐ½Ñ‹Ð¹",
                        "analytical": "Ð°Ð½Ð°Ð»Ð¸Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¸Ð¹",
                        "emotional": "ÑÐ¼Ð¾Ñ†Ð¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ñ‹Ð¹",
                        "skeptical": "Ð¾ÑÑ‚Ð¾Ñ€Ð¾Ð¶Ð½Ñ‹Ð¹",
                    }
                    niche_parts.append(f"Ð¿Ñ€ÐµÐ´Ð¿Ð¾Ñ‡Ð¸Ñ‚Ð°ÐµÐ¼Ñ‹Ð¹ ÑÑ‚Ð¸Ð»ÑŒ: {style_labels.get(style, style)}")
                if niche_data["avoid_techniques"]:
                    avoid_labels = [t["label"] for t in niche_data["avoid_techniques"]]
                    niche_parts.append(f"Ð¸Ð·Ð±ÐµÐ³Ð°Ð¹: {', '.join(avoid_labels)}")

                if niche_parts:
                    niche_label = niche_data["niche_label"]
                    parts.append(
                        f"[ÐÐ˜Ð¨Ð: {niche_label}] ÐÐ°ÐºÐ¾Ð¿Ð»ÐµÐ½Ð½Ñ‹Ð¹ Ð¾Ð¿Ñ‹Ñ‚: {'; '.join(niche_parts)}."
                    )

        if not parts:
            _insights_cache[cache_key] = (time.time(), None)
            return None

        result = "\n".join(parts)
        _insights_cache[cache_key] = (time.time(), result)
        return result

    def _get_user_niche(self, user_id: int) -> Optional[str]:
        if not DATABASE_URL:
            return None

        try:
            with get_connection() as conn:
                with conn.cursor() as cur:
                    cur.execute("""
                        SELECT rt.tag_value, COUNT(*) as cnt
                        FROM response_tags rt
                        JOIN response_outcomes ro ON rt.response_id = ro.id
                        WHERE ro.user_id = %s AND rt.tag_type = 'niche'
                        GROUP BY rt.tag_value
                        ORDER BY cnt DESC
                        LIMIT 1
                    """, (user_id,))
                    row = cur.fetchone()
                    return row[0] if row else None
        except Exception:
            return None

    def refresh_niche_memory(self, days: int = 30, min_samples: int = 10):
        if not DATABASE_URL:
            return

        try:
            for niche_id in NICHE_PATTERNS:
                insights = self.get_niche_insights(niche_id, days, min_samples)
                if not insights:
                    continue

                best_techs = ",".join([t["technique"] for t in insights.get("best_techniques", [])])
                avoid_techs = ",".join([t["technique"] for t in insights.get("avoid_techniques", [])])
                best_style = insights["best_styles"][0]["style"] if insights.get("best_styles") else None
                total = sum(t["total"] for t in insights.get("best_techniques", []))
                converted = sum(t["converted"] for t in insights.get("best_techniques", []))
                rate = round(converted / total * 100, 1) if total > 0 else 0.0

                with get_connection() as conn:
                    with conn.cursor() as cur:
                        cur.execute("""
                            INSERT INTO niche_style_memory (niche, preferred_style, preferred_techniques,
                                avoid_techniques, sample_size, conversion_rate, updated_at)
                            VALUES (%s, %s, %s, %s, %s, %s, NOW())
                            ON CONFLICT (niche) DO UPDATE SET
                                preferred_style = EXCLUDED.preferred_style,
                                preferred_techniques = EXCLUDED.preferred_techniques,
                                avoid_techniques = EXCLUDED.avoid_techniques,
                                sample_size = EXCLUDED.sample_size,
                                conversion_rate = EXCLUDED.conversion_rate,
                                updated_at = NOW()
                        """, (niche_id, best_style, best_techs, avoid_techs, total, rate))

            logger.info("Niche memory refreshed")
        except Exception as e:
            logger.error(f"Failed to refresh niche memory: {e}")

    def get_successful_patterns(self, outcome_type: str = 'lead_created',
                                limit: int = 20) -> list:
        if not DATABASE_URL:
            return []

        try:
            with get_connection() as conn:
                with conn.cursor() as cur:
                    cur.execute("""
                        SELECT message_text, response_text, funnel_stage, response_variant
                        FROM response_outcomes
                        WHERE outcome_type = %s
                        ORDER BY created_at DESC LIMIT %s
                    """, (outcome_type, limit))
                    rows = cur.fetchall()
                    return [
                        {
                            "message_text": row[0],
                            "response_text": row[1],
                            "funnel_stage": row[2],
                            "response_variant": row[3],
                        }
                        for row in rows
                    ]
        except Exception as e:
            logger.error(f"Failed to get successful patterns: {e}")
            return []

    def get_conversion_rate(self, days: int = 30) -> dict:
        if not DATABASE_URL:
            return {}

        try:
            with get_connection() as conn:
                with conn.cursor() as cur:
                    cur.execute("""
                        SELECT
                            COUNT(*) AS total,
                            COUNT(outcome_type) AS with_outcome
                        FROM response_outcomes
                        WHERE created_at >= NOW() - %s * INTERVAL '1 day'
                    """, (days,))
                    row = cur.fetchone()
                    total_responses = row[0] if row else 0
                    with_outcome = row[1] if row else 0
                    conversion_rate = round((with_outcome / total_responses * 100), 2) if total_responses > 0 else 0.0

                    cur.execute("""
                        SELECT outcome_type, COUNT(*) AS cnt
                        FROM response_outcomes
                        WHERE outcome_type IS NOT NULL
                          AND created_at >= NOW() - %s * INTERVAL '1 day'
                        GROUP BY outcome_type
                    """, (days,))
                    by_outcome = {r[0]: r[1] for r in cur.fetchall()}

                    cur.execute("""
                        SELECT
                            funnel_stage,
                            COUNT(*) AS total,
                            COUNT(outcome_type) AS converted
                        FROM response_outcomes
                        WHERE funnel_stage IS NOT NULL
                          AND created_at >= NOW() - %s * INTERVAL '1 day'
                        GROUP BY funnel_stage
                    """, (days,))
                    by_stage = {r[0]: {"total": r[1], "converted": r[2]} for r in cur.fetchall()}

                    cur.execute("""
                        SELECT
                            response_variant,
                            COUNT(*) AS total,
                            COUNT(outcome_type) AS converted
                        FROM response_outcomes
                        WHERE response_variant IS NOT NULL
                          AND created_at >= NOW() - %s * INTERVAL '1 day'
                        GROUP BY response_variant
                    """, (days,))
                    by_variant = {r[0]: {"total": r[1], "converted": r[2]} for r in cur.fetchall()}

                    cur.execute("""
                        SELECT
                            rt.tag_value AS technique,
                            COUNT(*) AS total,
                            COUNT(ro.outcome_type) AS converted,
                            ROUND(COUNT(ro.outcome_type)::numeric / NULLIF(COUNT(*), 0) * 100, 1) AS rate
                        FROM response_tags rt
                        JOIN response_outcomes ro ON rt.response_id = ro.id
                        WHERE rt.tag_type = 'technique'
                          AND ro.created_at >= NOW() - %s * INTERVAL '1 day'
                        GROUP BY rt.tag_value
                        ORDER BY rate DESC
                    """, (days,))
                    by_technique = {}
                    for r in cur.fetchall():
                        tech_info = CLOSING_TECHNIQUES.get(r[0], {})
                        by_technique[tech_info.get("label", r[0])] = {
                            "total": r[1], "converted": r[2], "rate": float(r[3])
                        }

                    return {
                        "total_responses": total_responses,
                        "with_outcome": with_outcome,
                        "conversion_rate": conversion_rate,
                        "by_outcome": by_outcome,
                        "by_stage": by_stage,
                        "by_variant": by_variant,
                        "by_technique": by_technique,
                    }
        except Exception as e:
            logger.error(f"Failed to get conversion rate: {e}")
            return {}

    def get_learning_insights(self, limit: int = 10) -> str:
        if not DATABASE_URL:
            return "Database not configured"

        try:
            with get_connection() as conn:
                with conn.cursor() as cur:
                    cur.execute("""
                        SELECT funnel_stage,
                               COUNT(*) AS total,
                               COUNT(outcome_type) AS converted,
                               ROUND(COUNT(outcome_type)::numeric / NULLIF(COUNT(*), 0) * 100, 1) AS rate
                        FROM response_outcomes
                        WHERE funnel_stage IS NOT NULL
                        GROUP BY funnel_stage
                        ORDER BY rate DESC
                        LIMIT %s
                    """, (limit,))
                    stage_rows = cur.fetchall()

                    cur.execute("""
                        SELECT response_variant,
                               COUNT(*) AS total,
                               COUNT(outcome_type) AS converted,
                               ROUND(COUNT(outcome_type)::numeric / NULLIF(COUNT(*), 0) * 100, 1) AS rate
                        FROM response_outcomes
                        WHERE response_variant IS NOT NULL
                        GROUP BY response_variant
                        ORDER BY rate DESC
                        LIMIT %s
                    """, (limit,))
                    variant_rows = cur.fetchall()

                    cur.execute("""
                        SELECT
                            rt.tag_value AS technique,
                            COUNT(*) AS total,
                            COUNT(ro.outcome_type) AS converted,
                            ROUND(COUNT(ro.outcome_type)::numeric / NULLIF(COUNT(*), 0) * 100, 1) AS rate
                        FROM response_tags rt
                        JOIN response_outcomes ro ON rt.response_id = ro.id
                        WHERE rt.tag_type = 'technique'
                        GROUP BY rt.tag_value
                        HAVING COUNT(*) >= 5
                        ORDER BY rate DESC
                        LIMIT %s
                    """, (limit,))
                    technique_rows = cur.fetchall()

                    cur.execute("""
                        SELECT
                            rt.tag_value AS niche,
                            COUNT(*) AS total,
                            COUNT(ro.outcome_type) AS converted,
                            ROUND(COUNT(ro.outcome_type)::numeric / NULLIF(COUNT(*), 0) * 100, 1) AS rate
                        FROM response_tags rt
                        JOIN response_outcomes ro ON rt.response_id = ro.id
                        WHERE rt.tag_type = 'niche'
                        GROUP BY rt.tag_value
                        HAVING COUNT(*) >= 5
                        ORDER BY rate DESC
                        LIMIT %s
                    """, (limit,))
                    niche_rows = cur.fetchall()

            lines = ["ðŸ“Š Self-Learning Insights v2\n"]

            lines.append("ðŸ† Best converting funnel stages:")
            if stage_rows:
                for row in stage_rows:
                    lines.append(f"  â€¢ {row[0]}: {row[2]}/{row[1]} ({row[3]}%)")
            else:
                lines.append("  No data yet")

            lines.append("\nðŸ”¬ Best converting A/B variants:")
            if variant_rows:
                for row in variant_rows:
                    lines.append(f"  â€¢ Variant {row[0]}: {row[2]}/{row[1]} ({row[3]}%)")
            else:
                lines.append("  No data yet")

            lines.append("\nðŸŽ¯ Closing technique performance:")
            if technique_rows:
                for row in technique_rows:
                    tech_info = CLOSING_TECHNIQUES.get(row[0], {})
                    label = tech_info.get("label", row[0])
                    wilson = round(_wilson_score(row[2], row[1]) * 100, 1)
                    lines.append(f"  â€¢ {label}: {row[2]}/{row[1]} ({row[3]}%) [Wilson: {wilson}%]")
            else:
                lines.append("  Not enough data (need â‰¥5 samples per technique)")

            lines.append("\nðŸª Niche performance:")
            if niche_rows:
                for row in niche_rows:
                    niche_info = NICHE_PATTERNS.get(row[0], {})
                    label = niche_info.get("label", row[0])
                    lines.append(f"  â€¢ {label}: {row[2]}/{row[1]} ({row[3]}%)")
            else:
                lines.append("  Not enough data (need â‰¥5 samples per niche)")

            return "\n".join(lines)
        except Exception as e:
            logger.error(f"Failed to get learning insights: {e}")
            return "Error generating insights"

    def cleanup_old(self, days: int = 90) -> int:
        if not DATABASE_URL:
            return 0

        try:
            with get_connection() as conn:
                with conn.cursor() as cur:
                    cur.execute("""
                        DELETE FROM response_outcomes
                        WHERE created_at < NOW() - %s * INTERVAL '1 day'
                    """, (days,))
                    deleted = cur.rowcount
                    logger.info(f"Cleaned up {deleted} old response outcomes (older than {days} days)")
                    return deleted
        except Exception as e:
            logger.error(f"Failed to cleanup old response outcomes: {e}")
            return 0


feedback_loop = FeedbackLoop()
