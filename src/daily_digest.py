import logging
import time
from html import escape as html_escape
from typing import Optional, Dict, List, Tuple
from datetime import datetime

logger = logging.getLogger(__name__)

FUNNEL_STAGES = [
    ("start", "ğŸš€ Ğ¡Ñ‚Ğ°Ñ€Ñ‚"),
    ("menu_open", "ğŸ“± ĞœĞµĞ½Ñ"),
    ("calculator_open", "ğŸ§® ĞšĞ°Ğ»ÑŒĞºÑƒĞ»ÑÑ‚Ğ¾Ñ€"),
    ("lead_form_open", "ğŸ“ Ğ¤Ğ¾Ñ€Ğ¼Ğ°"),
    ("lead_submit", "âœ… Ğ—Ğ°ÑĞ²ĞºĞ°"),
    ("payment_view", "ğŸ’³ ĞĞ¿Ğ»Ğ°Ñ‚Ğ°"),
    ("payment_confirm", "ğŸ’° ĞĞ¿Ğ»Ğ°Ñ‡ĞµĞ½Ğ¾"),
]


async def generate_daily_digest(bot, admin_chat_id: int) -> None:
    try:
        parts: List[str] = []

        now = datetime.now()
        date_str = now.strftime("%d.%m.%Y")
        weekday = ["ĞŸĞ½", "Ğ’Ñ‚", "Ğ¡Ñ€", "Ğ§Ñ‚", "ĞŸÑ‚", "Ğ¡Ğ±", "Ğ’Ñ"][now.weekday()]

        parts.append(f"ğŸ“Š <b>Ğ•Ğ–Ğ•Ğ”ĞĞ•Ğ’ĞĞĞ¯ Ğ¡Ğ’ĞĞ”ĞšĞ</b>")
        parts.append(f"ğŸ“… {date_str} ({weekday})")
        parts.append("â”" * 28)

        overview = _build_overview_section()
        if overview:
            parts.append(overview)

        propensity = _build_propensity_section()
        if propensity:
            parts.append(propensity)

        funnel = _build_funnel_section()
        if funnel:
            parts.append(funnel)

        dropoff = _build_dropoff_section()
        if dropoff:
            parts.append(dropoff)

        hot_leads = _build_hot_leads_section()
        if hot_leads:
            parts.append(hot_leads)

        learning = _build_self_learning_section()
        if learning:
            parts.append(learning)

        revenue = _build_revenue_section()
        if revenue:
            parts.append(revenue)

        proactive = _build_proactive_section()
        if proactive:
            parts.append(proactive)

        followup = _build_followup_section()
        if followup:
            parts.append(followup)

        vision = _build_vision_section()
        if vision:
            parts.append(vision)

        ab_tests = _build_ab_tests_section()
        if ab_tests:
            parts.append(ab_tests)

        trends = _build_trends_section()
        if trends:
            parts.append(trends)

        parts.append("â”" * 28)
        parts.append(f"<i>ĞĞ²Ñ‚Ğ¾Ğ¾Ñ‚Ñ‡Ñ‘Ñ‚ â€¢ {date_str} 06:00</i>")

        full_text = "\n\n".join(parts)

        if len(full_text) > 4000:
            mid = len(parts) // 2
            msg1 = "\n\n".join(parts[:mid])
            msg2 = "\n\n".join(parts[mid:])
            await bot.send_message(chat_id=admin_chat_id, text=msg1, parse_mode="HTML")
            await bot.send_message(chat_id=admin_chat_id, text=msg2, parse_mode="HTML")
        else:
            await bot.send_message(chat_id=admin_chat_id, text=full_text, parse_mode="HTML")

        logger.info(f"Daily digest v2 sent to admin {admin_chat_id}")
    except Exception as e:
        logger.error(f"Failed to send daily digest: {e}")


def _build_overview_section() -> Optional[str]:
    parts = ["<b>ğŸ“ˆ ĞĞ‘Ğ—ĞĞ  Ğ—Ğ 24 Ğ§ĞĞ¡Ğ</b>"]

    try:
        from src.leads import lead_manager
        stats = lead_manager.get_stats()
        analytics = lead_manager.get_analytics_stats()

        users_today = analytics.get("today_users", 0)
        messages = analytics.get("total_messages", 0)
        voice = analytics.get("voice_messages", 0)

        parts.append(f"ğŸ‘¥ ĞŸĞ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹: {users_today}")
        parts.append(f"ğŸ’¬ Ğ¡Ğ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğ¹: {messages}")
        if voice:
            parts.append(f"ğŸ™ Ğ“Ğ¾Ğ»Ğ¾ÑĞ¾Ğ²Ñ‹Ñ…: {voice}")

        parts.append(f"\n<b>Ğ›Ğ¸Ğ´Ñ‹:</b>")
        parts.append(f"ğŸ†• ĞĞ¾Ğ²Ñ‹Ğµ: {stats.get('new', 0)}")
        parts.append(f"ğŸ“ Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ: {stats.get('contacted', 0)}")
        parts.append(f"âœ… ĞšĞ²Ğ°Ğ»Ğ¸Ñ„.: {stats.get('qualified', 0)}")
        parts.append(f"ğŸ’° ĞšĞ¾Ğ½Ğ²ĞµÑ€Ñ‚.: {stats.get('converted', 0)}")
        parts.append(f"ğŸ“Š Ğ’ÑĞµĞ³Ğ¾: {stats.get('total', 0)}")
    except Exception as e:
        logger.debug(f"Overview failed: {e}")
        return None

    try:
        from src.broadcast import broadcast_manager
        total_users = len(broadcast_manager.get_user_ids('all'))
        from src.leads import lead_manager as lm
        week_analytics = lm.get_analytics_stats()
        parts.append(f"\nğŸ‘¥ Ğ’ÑĞµĞ³Ğ¾: {total_users} | Ğ—Ğ° Ğ½ĞµĞ´ĞµĞ»Ñ: {week_analytics.get('week_users', 0)}")
    except Exception:
        pass

    return "\n".join(parts)


def _build_propensity_section() -> Optional[str]:
    try:
        from src.propensity import propensity_scorer
        dist = propensity_scorer.get_score_distribution()
        if not dist:
            return None

        hot = dist.get("hot_70_100", 0)
        warm = dist.get("warm_40_69", 0)
        cool = dist.get("cool_20_39", 0)
        cold = dist.get("cold_0_19", 0)
        total = hot + warm + cool + cold

        if total == 0:
            return None

        parts = ["<b>ğŸŒ¡ Ğ¢Ğ•ĞœĞŸĞ•Ğ ĞĞ¢Ğ£Ğ Ğ Ğ‘ĞĞ—Ğ«</b>"]
        bar_hot = "ğŸŸ¥" * min(hot, 15) if hot else ""
        bar_warm = "ğŸŸ§" * min(warm, 15) if warm else ""
        bar_cool = "ğŸŸ¦" * min(cool, 10) if cool else ""
        bar_cold = "â¬œ" * min(cold, 10) if cold else ""

        parts.append(f"ğŸ”¥ Ğ“Ğ¾Ñ€ÑÑ‡Ğ¸Ğµ (70+): <b>{hot}</b> {bar_hot}")
        parts.append(f"ğŸŒ¡ Ğ¢Ñ‘Ğ¿Ğ»Ñ‹Ğµ (40-69): <b>{warm}</b> {bar_warm}")
        parts.append(f"â„ï¸ ĞŸÑ€Ğ¾Ñ…Ğ»Ğ°Ğ´Ğ½Ñ‹Ğµ (20-39): {cool} {bar_cool}")
        parts.append(f"ğŸ§Š Ğ¥Ğ¾Ğ»Ğ¾Ğ´Ğ½Ñ‹Ğµ (0-19): {cold} {bar_cold}")

        if total > 0:
            hot_pct = round(hot / total * 100, 1)
            ready_pct = round((hot + warm) / total * 100, 1)
            parts.append(f"ğŸ“Š Ğ“Ğ¾Ñ‚Ğ¾Ğ²Ñ‹ Ğº Ğ¿Ğ¾ĞºÑƒĞ¿ĞºĞµ: {ready_pct}% ({hot + warm}/{total})")

        return "\n".join(parts)
    except Exception as e:
        logger.debug(f"Propensity section failed: {e}")
        return None


def _build_funnel_section() -> Optional[str]:
    try:
        from src.analytics import analytics
        stats = analytics.get_funnel_stats(1)
        if not stats:
            return None

        parts = ["<b>ğŸ“Š Ğ’ĞĞ ĞĞĞšĞ (24Ñ‡)</b>"]

        prev_count = None
        for event_name, label in FUNNEL_STAGES:
            count = stats.get(event_name, 0)
            if count == 0 and prev_count == 0:
                continue
            conv_str = ""
            if prev_count and prev_count > 0 and count > 0:
                conv = round(count / prev_count * 100, 1)
                conv_str = f" ({conv}%â†“)"
            parts.append(f"{label}: {count}{conv_str}")
            prev_count = count

        start = stats.get("start", 0)
        leads = stats.get("lead_submit", 0)
        if start > 0:
            total_conv = round(leads / start * 100, 1)
            parts.append(f"\nğŸ¯ Startâ†’Lead: <b>{total_conv}%</b>")

        payments = stats.get("payment_confirm", 0)
        if start > 0 and payments > 0:
            pay_conv = round(payments / start * 100, 1)
            parts.append(f"ğŸ’° Startâ†’Pay: <b>{pay_conv}%</b>")

        return "\n".join(parts)
    except Exception as e:
        logger.debug(f"Funnel section failed: {e}")
        return None


def _build_dropoff_section() -> Optional[str]:
    try:
        from src.advanced_analytics import advanced_analytics
        data = advanced_analytics.get_dropoff_analysis(1)
        if not data or not data.get("highest_dropoff"):
            return None

        parts = ["<b>ğŸš¨ ĞŸĞĞ¢Ğ•Ğ Ğ˜ ĞšĞ›Ğ˜Ğ•ĞĞ¢ĞĞ’</b>"]

        hd = data["highest_dropoff"]
        stage_labels = dict(FUNNEL_STAGES)
        from_label = stage_labels.get(hd["from_stage"], hd["from_stage"])
        to_label = stage_labels.get(hd["to_stage"], hd["to_stage"])
        parts.append(f"âš ï¸ ĞœĞ°ĞºÑ. Ğ¿Ğ¾Ñ‚ĞµÑ€Ñ: {from_label} â†’ {to_label}")
        parts.append(f"   Ğ£ÑˆĞ»Ğ¾: {hd['users_lost']} ({hd['dropoff_rate']}%)")

        last_stage = data.get("most_common_last_type", "")
        if last_stage and last_stage != "unknown":
            last_label = stage_labels.get(last_stage, last_stage)
            parts.append(f"ğŸ“ Ğ§Ğ°Ñ‰Ğµ Ğ²ÑĞµĞ³Ğ¾ ÑƒÑ…Ğ¾Ğ´ÑÑ‚ Ğ¿Ğ¾ÑĞ»Ğµ: {last_label}")

        avg_msg = data.get("avg_messages_before_dropoff", 0)
        if avg_msg > 0:
            parts.append(f"ğŸ’¬ Ğ¡Ñ€ĞµĞ´Ğ½ĞµĞµ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ¾ ÑƒÑ…Ğ¾Ğ´Ğ°: {avg_msg}")

        return "\n".join(parts)
    except Exception as e:
        logger.debug(f"Dropoff section failed: {e}")
        return None


def _build_hot_leads_section() -> Optional[str]:
    try:
        from src.database import DATABASE_URL, get_connection
        if not DATABASE_URL:
            return None

        with get_connection() as conn:
            with conn.cursor() as cur:
                cur.execute("""
                    SELECT l.user_id, l.first_name, l.username, l.score,
                           l.business_type, l.budget, l.phone,
                           im.last_score as propensity
                    FROM leads l
                    LEFT JOIN interaction_metrics im ON l.user_id = im.user_id
                    WHERE l.score >= 50
                       OR (im.last_score IS NOT NULL AND im.last_score >= 60)
                    ORDER BY COALESCE(im.last_score, 0) + COALESCE(l.score, 0) DESC
                    LIMIT 5
                """)
                rows = cur.fetchall()

        if not rows:
            return None

        parts = ["<b>ğŸ”¥ Ğ“ĞĞ Ğ¯Ğ§Ğ˜Ğ• Ğ›Ğ˜Ğ”Ğ« â€” Ğ¡Ğ’Ğ¯Ğ—ĞĞ¢Ğ¬Ğ¡Ğ¯ Ğ¡Ğ•Ğ“ĞĞ”ĞĞ¯</b>"]

        for i, row in enumerate(rows, 1):
            user_id, name, username, score, biz, budget, phone, propensity = row
            name_str = html_escape(name or "Ğ‘ĞµĞ· Ğ¸Ğ¼ĞµĞ½Ğ¸")
            if username:
                name_str += f" (@{html_escape(username)})"

            score_val = propensity or score or 0
            temp = "ğŸ”¥" if score_val >= 70 else "ğŸŒ¡" if score_val >= 40 else "â„ï¸"

            line = f"{i}. {temp} <b>{name_str}</b> [{score_val}/100]"
            details = []
            if biz:
                details.append(html_escape(str(biz)))
            if budget:
                details.append(f"Ğ±ÑĞ´Ğ¶ĞµÑ‚: {html_escape(str(budget))}")
            if phone:
                details.append(f"ğŸ“ {html_escape(str(phone))}")
            if details:
                line += f"\n   {' | '.join(details)}"

            parts.append(line)

        return "\n".join(parts)
    except Exception as e:
        logger.debug(f"Hot leads section failed: {e}")
        return None


def _build_self_learning_section() -> Optional[str]:
    try:
        from src.feedback_loop import feedback_loop
        conv_data = feedback_loop.get_conversion_rate(1)
        if not conv_data or conv_data.get("total_responses", 0) == 0:
            return None

        parts = ["<b>ğŸ§  AI Ğ¡ĞĞœĞĞĞ‘Ğ£Ğ§Ğ•ĞĞ˜Ğ• (24Ñ‡)</b>"]

        total = conv_data["total_responses"]
        converted = conv_data["with_outcome"]
        rate = conv_data["conversion_rate"]
        parts.append(f"ĞÑ‚Ğ²ĞµÑ‚Ğ¾Ğ²: {total} | ĞšĞ¾Ğ½Ğ²ĞµÑ€ÑĞ¸Ğ¹: {converted} ({rate}%)")

        by_technique = conv_data.get("by_technique", {})
        if by_technique:
            sorted_tech = sorted(by_technique.items(), key=lambda x: x[1]["rate"], reverse=True)
            parts.append("\n<b>Ğ›ÑƒÑ‡ÑˆĞ¸Ğµ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸:</b>")
            for tech_name, data in sorted_tech[:3]:
                parts.append(f"  âœ… {tech_name}: {data['rate']}% ({data['converted']}/{data['total']})")

            worst = [t for t in sorted_tech if t[1]["rate"] < 5 and t[1]["total"] >= 5]
            if worst:
                parts.append("<b>Ğ¡Ğ»Ğ°Ğ±Ñ‹Ğµ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸:</b>")
                for tech_name, data in worst[:2]:
                    parts.append(f"  âš ï¸ {tech_name}: {data['rate']}%")

        by_outcome = conv_data.get("by_outcome", {})
        if by_outcome:
            sorted_outcomes = sorted(by_outcome.items(), key=lambda x: x[1], reverse=True)
            top_3 = sorted_outcomes[:3]
            outcome_labels = {
                "consultation_booked": "ğŸ“… ĞšĞ¾Ğ½ÑÑƒĞ»ÑŒÑ‚Ğ°Ñ†Ğ¸Ğ¸",
                "lead_created": "ğŸ“ Ğ—Ğ°ÑĞ²ĞºĞ¸",
                "payment_started": "ğŸ’³ ĞĞ¿Ğ»Ğ°Ñ‚Ñ‹",
                "calculator_used": "ğŸ§® ĞšĞ°Ğ»ÑŒĞºÑƒĞ»ÑÑ‚Ğ¾Ñ€",
                "brief_generated": "ğŸ“‹ Ğ‘Ñ€Ğ¸Ñ„Ñ‹",
                "portfolio_viewed": "ğŸ“Š ĞŸĞ¾Ñ€Ñ‚Ñ„Ğ¾Ğ»Ğ¸Ğ¾",
                "callback_booking": "ğŸ“… Ğ‘Ñ€Ğ¾Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ",
                "callback_payment": "ğŸ’³ ĞŸĞ»Ğ°Ñ‚Ñ‘Ğ¶",
                "callback_brief": "ğŸ“‹ Ğ‘Ñ€Ğ¸Ñ„",
            }
            parts.append("\n<b>Ğ¢Ğ¾Ğ¿ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹:</b>")
            for outcome, count in top_3:
                label = outcome_labels.get(outcome, outcome)
                parts.append(f"  {label}: {count}")

        return "\n".join(parts)
    except Exception as e:
        logger.debug(f"Self-learning section failed: {e}")
        return None


def _build_revenue_section() -> Optional[str]:
    parts = []

    try:
        from src.database import DATABASE_URL, execute_one
        if not DATABASE_URL:
            return None

        result = execute_one(
            "SELECT COUNT(*) as cnt, COALESCE(SUM(amount), 0) as total FROM star_payments WHERE paid_at > NOW() - INTERVAL '24 hours'"
        )
        stars_today = result[0] if result and result[0] else 0
        stars_amount = result[1] if result and result[1] else 0

        if stars_today > 0:
            parts.append("<b>ğŸ’° REVENUE (24Ñ‡)</b>")
            parts.append(f"â­ Stars: {stars_today} Ğ¿Ğ»Ğ°Ñ‚ĞµĞ¶ĞµĞ¹ ({stars_amount} â­)")
    except Exception:
        pass

    try:
        from src.advanced_analytics import advanced_analytics
        ltv = advanced_analytics.get_ltv_analysis()
        if ltv and ltv.get("total_revenue", 0) > 0:
            if not parts:
                parts.append("<b>ğŸ’° REVENUE</b>")
            parts.append(f"ğŸ’µ ĞĞ±Ñ‰Ğ¸Ğ¹ Ğ´Ğ¾Ñ…Ğ¾Ğ´: {ltv['total_revenue']:,.0f}â‚½".replace(",", " "))
            parts.append(f"ğŸ‘¥ ĞŸĞ»Ğ°Ñ‚ÑÑ‰Ğ¸Ñ…: {ltv['total_paying_users']}")
            if ltv.get("arpu", 0) > 0:
                parts.append(f"ğŸ“Š ARPU: {ltv['arpu']:,.0f}â‚½".replace(",", " "))
    except Exception:
        pass

    return "\n".join(parts) if parts else None


def _build_proactive_section() -> Optional[str]:
    try:
        from src.proactive_engagement import proactive_engine
        stats = proactive_engine.get_trigger_stats()
        if not stats:
            return None

        total_today = sum(s.get("today", 0) for s in stats.values())
        total_responded = sum(s.get("responded", 0) for s in stats.values())
        total_all = sum(s.get("total", 0) for s in stats.values())

        if total_all == 0:
            return None

        overall_response = round(total_responded / total_all * 100, 1) if total_all > 0 else 0

        parts = ["<b>ğŸ¯ ĞŸĞ ĞĞĞšĞ¢Ğ˜Ğ’ĞĞ«Ğ• Ğ¡ĞĞĞ‘Ğ©Ğ•ĞĞ˜Ğ¯</b>"]
        parts.append(f"ğŸ“¤ Ğ¡ĞµĞ³Ğ¾Ğ´Ğ½Ñ: {total_today} | Ğ’ÑĞµĞ³Ğ¾: {total_all}")
        parts.append(f"ğŸ“¬ ĞÑ‚Ğ²ĞµÑ‚Ğ¸Ğ»Ğ¸: {total_responded} ({overall_response}%)")

        best_trigger = max(stats.items(), key=lambda x: x[1].get("response_rate", 0))
        if best_trigger[1]["response_rate"] > 0:
            trigger_labels = {
                "welcome_back": "ğŸ”„ Ğ’Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğµ",
                "calculator_abandon": "ğŸ§® ĞĞµĞ·Ğ°Ğ²ĞµÑ€ÑˆÑ‘Ğ½Ğ½Ñ‹Ğ¹ Ñ€Ğ°ÑÑ‡Ñ‘Ñ‚",
                "interest_no_action": "ğŸ’¡ Ğ˜Ğ½Ñ‚ĞµÑ€ĞµÑ Ğ±ĞµĞ· Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ",
                "high_engagement": "âš¡ Ğ’Ñ‹ÑĞ¾ĞºĞ°Ñ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ",
                "stale_lead": "ğŸ’¤ ĞÑÑ‚Ñ‹Ğ²ÑˆĞ¸Ğ¹ Ğ»Ğ¸Ğ´",
                "milestone": "ğŸ† Ğ”Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ",
                "seasonal": "ğŸ“… Ğ¡ĞµĞ·Ğ¾Ğ½Ğ½Ñ‹Ğ¹",
                "competitor_mention": "âš”ï¸ Ğ£Ğ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ½Ğ¸Ğµ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ°",
            }
            label = trigger_labels.get(best_trigger[0], best_trigger[0])
            parts.append(f"ğŸ† Ğ›ÑƒÑ‡ÑˆĞ¸Ğ¹ Ñ‚Ñ€Ğ¸Ğ³Ğ³ĞµÑ€: {label} ({best_trigger[1]['response_rate']}%)")

        return "\n".join(parts)
    except Exception as e:
        logger.debug(f"Proactive section failed: {e}")
        return None


def _build_followup_section() -> Optional[str]:
    try:
        from src.followup import follow_up_manager
        stats = follow_up_manager.get_stats()
        if not stats:
            return None

        sent_today = stats.get("sent_today", 0)
        scheduled = stats.get("scheduled", 0)
        responded = stats.get("responded", 0)
        total_sent = stats.get("sent", 0)

        if total_sent == 0 and scheduled == 0:
            return None

        response_rate = round(responded / total_sent * 100, 1) if total_sent > 0 else 0

        parts = ["<b>ğŸ“¨ FOLLOW-UP</b>"]
        parts.append(f"ğŸ“¤ Ğ¡ĞµĞ³Ğ¾Ğ´Ğ½Ñ: {sent_today} | Ğ’ Ğ¾Ñ‡ĞµÑ€ĞµĞ´Ğ¸: {scheduled}")
        parts.append(f"ğŸ“¬ ĞÑ‚Ğ²ĞµÑ‚Ğ¸Ğ»Ğ¸: {responded}/{total_sent} ({response_rate}%)")

        return "\n".join(parts)
    except Exception as e:
        logger.debug(f"Followup section failed: {e}")
        return None


def _build_vision_section() -> Optional[str]:
    try:
        from src.database import DATABASE_URL, get_connection
        if not DATABASE_URL:
            return None

        with get_connection() as conn:
            with conn.cursor() as cur:
                cur.execute("""
                    SELECT
                        COUNT(*) FILTER (WHERE event_type = 'photo_received' AND created_at > NOW() - INTERVAL '24 hours') as photos_today,
                        COUNT(*) FILTER (WHERE event_type = 'photo_received') as photos_total
                    FROM analytics
                """)
                row = cur.fetchone()
                photos_today = row[0] if row and row[0] else 0
                photos_total = row[1] if row and row[1] else 0

        if photos_today == 0 and photos_total == 0:
            return None

        parts = ["<b>ğŸ“¸ ĞĞĞĞ›Ğ˜Ğ— Ğ¤ĞĞ¢Ğ</b>"]
        parts.append(f"ğŸ“· Ğ¡ĞµĞ³Ğ¾Ğ´Ğ½Ñ: {photos_today} | Ğ’ÑĞµĞ³Ğ¾: {photos_total}")

        try:
            with get_connection() as conn:
                with conn.cursor() as cur:
                    cur.execute("""
                        SELECT data->>'image_type' as img_type, COUNT(*) as cnt
                        FROM analytics
                        WHERE event_type = 'photo_analyzed'
                          AND created_at > NOW() - INTERVAL '7 days'
                          AND data->>'image_type' IS NOT NULL
                        GROUP BY data->>'image_type'
                        ORDER BY cnt DESC
                        LIMIT 3
                    """)
                    type_rows = cur.fetchall()
                    if type_rows:
                        type_labels = {
                            "design_mockup": "ğŸ¨ ĞœĞ°ĞºĞµÑ‚Ñ‹",
                            "document_tz": "ğŸ“„ Ğ¢Ğ—",
                            "app_screenshot": "ğŸ“± Ğ¡ĞºÑ€Ğ¸Ğ½ÑˆĞ¾Ñ‚Ñ‹",
                            "website_screenshot": "ğŸŒ Ğ¡Ğ°Ğ¹Ñ‚Ñ‹",
                            "competitor_app": "âš”ï¸ ĞšĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ñ‹",
                            "business_photo": "ğŸ¢ Ğ‘Ğ¸Ğ·Ğ½ĞµÑ",
                            "product_photo": "ğŸ“¦ Ğ¢Ğ¾Ğ²Ğ°Ñ€Ñ‹",
                            "menu_catalog": "ğŸ½ ĞœĞµĞ½Ñ",
                        }
                        for img_type, cnt in type_rows:
                            label = type_labels.get(img_type, img_type)
                            parts.append(f"  {label}: {cnt}")
        except Exception:
            pass

        return "\n".join(parts)
    except Exception as e:
        logger.debug(f"Vision section failed: {e}")
        return None


def _build_ab_tests_section() -> Optional[str]:
    try:
        from src.ab_testing import ab_testing
        summary = ab_testing.format_all_tests_summary()
        if not summary or "Ğ½ĞµÑ‚ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ…" in summary.lower() or len(summary) < 20:
            return None

        lines = summary.strip().split("\n")
        if len(lines) > 8:
            lines = lines[:8]
            lines.append("...")

        return "<b>ğŸ”¬ A/B Ğ¢Ğ•Ğ¡Ğ¢Ğ«</b>\n" + "\n".join(lines)
    except Exception as e:
        logger.debug(f"AB tests section failed: {e}")
        return None


def _build_trends_section() -> Optional[str]:
    try:
        from src.database import DATABASE_URL, get_connection
        if not DATABASE_URL:
            return None

        with get_connection() as conn:
            with conn.cursor() as cur:
                cur.execute("""
                    SELECT
                        COUNT(DISTINCT user_id) FILTER (WHERE created_at >= CURRENT_DATE - INTERVAL '1 day' AND created_at < CURRENT_DATE) as users_yesterday,
                        COUNT(DISTINCT user_id) FILTER (WHERE created_at >= CURRENT_DATE) as users_today,
                        COUNT(*) FILTER (WHERE created_at >= CURRENT_DATE - INTERVAL '1 day' AND created_at < CURRENT_DATE) as msgs_yesterday,
                        COUNT(*) FILTER (WHERE created_at >= CURRENT_DATE) as msgs_today
                    FROM analytics
                """)
                row = cur.fetchone()
                if not row:
                    return None

                users_y, users_t, msgs_y, msgs_t = row[0] or 0, row[1] or 0, row[2] or 0, row[3] or 0

                cur.execute("""
                    SELECT
                        COUNT(*) FILTER (WHERE created_at >= CURRENT_DATE - INTERVAL '1 day' AND created_at < CURRENT_DATE) as leads_yesterday,
                        COUNT(*) FILTER (WHERE created_at >= CURRENT_DATE) as leads_today
                    FROM leads
                """)
                lead_row = cur.fetchone()
                leads_y = lead_row[0] if lead_row and lead_row[0] else 0
                leads_t = lead_row[1] if lead_row and lead_row[1] else 0

        def trend_arrow(today: int, yesterday: int) -> str:
            if yesterday == 0:
                if today > 0:
                    return f"ğŸ†• +{today}"
                return "â€”"
            delta = today - yesterday
            pct = round(delta / yesterday * 100)
            if delta > 0:
                return f"ğŸ“ˆ +{pct}%"
            elif delta < 0:
                return f"ğŸ“‰ {pct}%"
            else:
                return "â¡ï¸ 0%"

        parts = ["<b>ğŸ“Š Ğ¢Ğ Ğ•ĞĞ” vs Ğ’Ğ§Ğ•Ğ Ğ</b>"]
        parts.append(f"ğŸ‘¥ ĞŸĞ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸: {users_t} ({trend_arrow(users_t, users_y)})")
        parts.append(f"ğŸ’¬ Ğ¡Ğ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ: {msgs_t} ({trend_arrow(msgs_t, msgs_y)})")
        parts.append(f"ğŸ“ Ğ›Ğ¸Ğ´Ñ‹: {leads_t} ({trend_arrow(leads_t, leads_y)})")

        try:
            from src.database import execute_one
            stars_result = execute_one("""
                SELECT
                    COALESCE(SUM(amount) FILTER (WHERE paid_at >= CURRENT_DATE - INTERVAL '1 day' AND paid_at < CURRENT_DATE), 0),
                    COALESCE(SUM(amount) FILTER (WHERE paid_at >= CURRENT_DATE), 0)
                FROM star_payments
            """)
            if stars_result:
                stars_y = stars_result[0] or 0
                stars_t = stars_result[1] or 0
                if stars_y > 0 or stars_t > 0:
                    parts.append(f"â­ Stars: {stars_t} ({trend_arrow(int(stars_t), int(stars_y))})")
        except Exception:
            pass

        return "\n".join(parts)
    except Exception as e:
        logger.debug(f"Trends section failed: {e}")
        return None


def format_digest_preview() -> str:
    sections = []

    sections.append("=" * 60)
    sections.append("  DEMO: Ğ•Ğ¶ĞµĞ´Ğ½ĞµĞ²Ğ½Ğ°Ñ ÑĞ²Ğ¾Ğ´ĞºĞ° v2 (Daily Digest)")
    sections.append("=" * 60)

    section_list = [
        ("ğŸ“ˆ ĞĞ‘Ğ—ĞĞ  Ğ—Ğ 24 Ğ§ĞĞ¡Ğ", "ĞŸĞ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸, ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ, Ğ»Ğ¸Ğ´Ñ‹ Ğ¿Ğ¾ ÑÑ‚Ğ°Ñ‚ÑƒÑĞ°Ğ¼"),
        ("ğŸŒ¡ Ğ¢Ğ•ĞœĞŸĞ•Ğ ĞĞ¢Ğ£Ğ Ğ Ğ‘ĞĞ—Ğ«", "Propensity Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ: ğŸ”¥Ğ³Ğ¾Ñ€ÑÑ‡Ğ¸Ğµ/ğŸŒ¡Ñ‚Ñ‘Ğ¿Ğ»Ñ‹Ğµ/â„ï¸Ğ¿Ñ€Ğ¾Ñ…Ğ»Ğ°Ğ´Ğ½Ñ‹Ğµ/ğŸ§ŠÑ…Ğ¾Ğ»Ğ¾Ğ´Ğ½Ñ‹Ğµ"),
        ("ğŸ“Š Ğ’ĞĞ ĞĞĞšĞ (24Ñ‡)", "Startâ†’Menuâ†’Calcâ†’Leadâ†’Pay Ñ % ĞºĞ¾Ğ½Ğ²ĞµÑ€ÑĞ¸Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ ÑÑ‚Ğ°Ğ¿Ğ°Ğ¼Ğ¸"),
        ("ğŸš¨ ĞŸĞĞ¢Ğ•Ğ Ğ˜ ĞšĞ›Ğ˜Ğ•ĞĞ¢ĞĞ’", "ĞœĞ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ drop-off, Ğ³Ğ´Ğµ ÑƒÑ…Ğ¾Ğ´ÑÑ‚, ÑÑ€ĞµĞ´Ğ½ĞµĞµ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ¾ ÑƒÑ…Ğ¾Ğ´Ğ°"),
        ("ğŸ”¥ Ğ“ĞĞ Ğ¯Ğ§Ğ˜Ğ• Ğ›Ğ˜Ğ”Ğ«", "Ğ¢Ğ¾Ğ¿-5 ĞºĞ»Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ ĞºĞ¾Ğ½Ñ‚Ğ°ĞºÑ‚Ğ° ÑĞµĞ³Ğ¾Ğ´Ğ½Ñ Ñ Ğ´ĞµÑ‚Ğ°Ğ»ÑĞ¼Ğ¸"),
        ("ğŸ§  AI Ğ¡ĞĞœĞĞĞ‘Ğ£Ğ§Ğ•ĞĞ˜Ğ•", "Ğ›ÑƒÑ‡ÑˆĞ¸Ğµ/ÑĞ»Ğ°Ğ±Ñ‹Ğµ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚Ğ¸Ñ, ĞºĞ¾Ğ½Ğ²ĞµÑ€ÑĞ¸Ñ, Ñ‚Ğ¾Ğ¿ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¹"),
        ("ğŸ’° REVENUE", "Stars Ğ¾Ğ¿Ğ»Ğ°Ñ‚Ñ‹, Ğ¾Ğ±Ñ‰Ğ¸Ğ¹ Ğ´Ğ¾Ñ…Ğ¾Ğ´, ARPU, Ğ¿Ğ»Ğ°Ñ‚ÑÑ‰Ğ¸Ğµ"),
        ("ğŸ¯ ĞŸĞ ĞĞĞšĞ¢Ğ˜Ğ’ĞĞ«Ğ•", "Ğ¢Ñ€Ğ¸Ğ³Ğ³ĞµÑ€Ğ½Ñ‹Ğµ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ: Ğ¾Ñ‚Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¾, Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¸Ğ»Ğ¸, Ğ»ÑƒÑ‡ÑˆĞ¸Ğ¹ Ñ‚Ñ€Ğ¸Ğ³Ğ³ĞµÑ€"),
        ("ğŸ“¨ FOLLOW-UP", "ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ĞºĞ°ÑĞ°Ğ½Ğ¸Ñ: Ğ¾Ñ‚Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¾, Ğ² Ğ¾Ñ‡ĞµÑ€ĞµĞ´Ğ¸, response rate"),
        ("ğŸ“¸ ĞĞĞĞ›Ğ˜Ğ— Ğ¤ĞĞ¢Ğ", "Ğ¤Ğ¾Ñ‚Ğ¾ Ğ·Ğ° ÑÑƒÑ‚ĞºĞ¸, Ñ‚Ğ¸Ğ¿Ñ‹ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ (Ğ¼Ğ°ĞºĞµÑ‚Ñ‹, Ğ¢Ğ—, ÑĞºÑ€Ğ¸Ğ½ÑˆĞ¾Ñ‚Ñ‹)"),
        ("ğŸ”¬ A/B Ğ¢Ğ•Ğ¡Ğ¢Ğ«", "ĞĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¸ Ğ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹"),
        ("ğŸ“Š Ğ¢Ğ Ğ•ĞĞ” vs Ğ’Ğ§Ğ•Ğ Ğ", "Ğ”ĞµĞ»ÑŒÑ‚Ñ‹: Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸ â†‘â†“%, ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ â†‘â†“%, Ğ»Ğ¸Ğ´Ñ‹ â†‘â†“%, Stars â†‘â†“%"),
    ]

    sections.append(f"\n  Ğ¡ĞµĞºÑ†Ğ¸Ğ¹ Ğ² ÑĞ²Ğ¾Ğ´ĞºĞµ: {len(section_list)}")
    sections.append(f"  Graceful degradation: ĞºĞ°Ğ¶Ğ´Ğ°Ñ ÑĞµĞºÑ†Ğ¸Ñ Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ°")
    sections.append(f"  ĞĞ²Ñ‚Ğ¾-ÑĞ¿Ğ»Ğ¸Ñ‚: Ğ¿Ñ€Ğ¸ >4000 ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¾Ğ² Ñ€Ğ°Ğ·Ğ±Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ½Ğ° 2 ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ")
    sections.append("")

    for i, (title, desc) in enumerate(section_list, 1):
        sections.append(f"  {i:2d}. {title}")
        sections.append(f"      {desc}")

    sections.append("")

    sections.append("  ĞŸĞ Ğ˜ĞœĞ•Ğ  Ğ¤ĞĞ ĞœĞĞ¢Ğ:")
    sections.append("  " + "-" * 50)

    example = """  ğŸ“Š Ğ•Ğ–Ğ•Ğ”ĞĞ•Ğ’ĞĞĞ¯ Ğ¡Ğ’ĞĞ”ĞšĞ
  ğŸ“… 18.02.2026 (Ğ’Ñ‚)
  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

  ğŸ“ˆ ĞĞ‘Ğ—ĞĞ  Ğ—Ğ 24 Ğ§ĞĞ¡Ğ
  ğŸ‘¥ ĞŸĞ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹: 47
  ğŸ’¬ Ğ¡Ğ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğ¹: 312
  ğŸ™ Ğ“Ğ¾Ğ»Ğ¾ÑĞ¾Ğ²Ñ‹Ñ…: 8
  Ğ›Ğ¸Ğ´Ñ‹:
  ğŸ†• ĞĞ¾Ğ²Ñ‹Ğµ: 12 | ğŸ“ Ğ’ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ: 5
  âœ… ĞšĞ²Ğ°Ğ»Ğ¸Ñ„.: 3 | ğŸ’° ĞšĞ¾Ğ½Ğ²ĞµÑ€Ñ‚.: 1

  ğŸŒ¡ Ğ¢Ğ•ĞœĞŸĞ•Ğ ĞĞ¢Ğ£Ğ Ğ Ğ‘ĞĞ—Ğ«
  ğŸ”¥ Ğ“Ğ¾Ñ€ÑÑ‡Ğ¸Ğµ (70+): 8 ğŸŸ¥ğŸŸ¥ğŸŸ¥ğŸŸ¥ğŸŸ¥ğŸŸ¥ğŸŸ¥ğŸŸ¥
  ğŸŒ¡ Ğ¢Ñ‘Ğ¿Ğ»Ñ‹Ğµ (40-69): 23 ğŸŸ§ğŸŸ§ğŸŸ§ğŸŸ§ğŸŸ§
  â„ï¸ ĞŸÑ€Ğ¾Ñ…Ğ»Ğ°Ğ´Ğ½Ñ‹Ğµ (20-39): 45
  ğŸ§Š Ğ¥Ğ¾Ğ»Ğ¾Ğ´Ğ½Ñ‹Ğµ (0-19): 124
  ğŸ“Š Ğ“Ğ¾Ñ‚Ğ¾Ğ²Ñ‹ Ğº Ğ¿Ğ¾ĞºÑƒĞ¿ĞºĞµ: 15.5% (31/200)

  ğŸ“Š Ğ’ĞĞ ĞĞĞšĞ (24Ñ‡)
  ğŸš€ Ğ¡Ñ‚Ğ°Ñ€Ñ‚: 47
  ğŸ“± ĞœĞµĞ½Ñ: 38 (80.9%â†“)
  ğŸ§® ĞšĞ°Ğ»ÑŒĞºÑƒĞ»ÑÑ‚Ğ¾Ñ€: 15 (39.5%â†“)
  âœ… Ğ—Ğ°ÑĞ²ĞºĞ°: 3 (20%â†“)
  ğŸ¯ Startâ†’Lead: 6.4%

  ğŸš¨ ĞŸĞĞ¢Ğ•Ğ Ğ˜ ĞšĞ›Ğ˜Ğ•ĞĞ¢ĞĞ’
  âš ï¸ ĞœĞ°ĞºÑ. Ğ¿Ğ¾Ñ‚ĞµÑ€Ñ: ğŸ“± ĞœĞµĞ½Ñ â†’ ğŸ§® ĞšĞ°Ğ»ÑŒĞºÑƒĞ»ÑÑ‚Ğ¾Ñ€
     Ğ£ÑˆĞ»Ğ¾: 23 (60.5%)
  ğŸ’¬ Ğ¡Ñ€ĞµĞ´Ğ½ĞµĞµ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğ¹ Ğ´Ğ¾ ÑƒÑ…Ğ¾Ğ´Ğ°: 3.2

  ğŸ”¥ Ğ“ĞĞ Ğ¯Ğ§Ğ˜Ğ• Ğ›Ğ˜Ğ”Ğ« â€” Ğ¡Ğ’Ğ¯Ğ—ĞĞ¢Ğ¬Ğ¡Ğ¯ Ğ¡Ğ•Ğ“ĞĞ”ĞĞ¯
  1. ğŸ”¥ ĞĞ»ĞµĞºÑĞµĞ¹ (@alex_biz) [85/100]
     Ğ ĞµÑÑ‚Ğ¾Ñ€Ğ°Ğ½ | Ğ±ÑĞ´Ğ¶ĞµÑ‚: 300Ğº | ğŸ“ +7...
  2. ğŸŒ¡ ĞœĞ°Ñ€Ğ¸Ñ (@masha_shop) [62/100]
     Ğ˜Ğ½Ñ‚ĞµÑ€Ğ½ĞµÑ‚-Ğ¼Ğ°Ğ³Ğ°Ğ·Ğ¸Ğ½ | Ğ±ÑĞ´Ğ¶ĞµÑ‚: 150Ğº

  ğŸ§  AI Ğ¡ĞĞœĞĞĞ‘Ğ£Ğ§Ğ•ĞĞ˜Ğ• (24Ñ‡)
  ĞÑ‚Ğ²ĞµÑ‚Ğ¾Ğ²: 312 | ĞšĞ¾Ğ½Ğ²ĞµÑ€ÑĞ¸Ğ¹: 47 (15.1%)
  Ğ›ÑƒÑ‡ÑˆĞ¸Ğµ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸:
    âœ… Future pacing: 28.5% (8/28)
    âœ… Assumptive close: 22.1% (6/27)
  Ğ¡Ğ»Ğ°Ğ±Ñ‹Ğµ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸:
    âš ï¸ Sharp angle: 2.1%

  ğŸ“Š Ğ¢Ğ Ğ•ĞĞ” vs Ğ’Ğ§Ğ•Ğ Ğ
  ğŸ‘¥ ĞŸĞ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğ¸: 47 (ğŸ“ˆ +18%)
  ğŸ’¬ Ğ¡Ğ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ: 312 (ğŸ“ˆ +5%)
  ğŸ“ Ğ›Ğ¸Ğ´Ñ‹: 12 (ğŸ“‰ -8%)
  â­ Stars: 2400 (ğŸ“ˆ +50%)"""

    sections.append(example)
    sections.append("  " + "-" * 50)

    old_vs_new = """
  Ğ‘Ğ«Ğ›Ğ (ÑÑ‚Ğ°Ñ€Ğ°Ñ ÑĞ²Ğ¾Ğ´ĞºĞ°):          Ğ¡Ğ¢ĞĞ›Ğ (v2):
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€          â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  6 Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº                       12 ÑĞµĞºÑ†Ğ¸Ğ¹
  Ğ›Ğ¸Ğ´Ñ‹ Ğ¿Ğ¾ ÑÑ‚Ğ°Ñ‚ÑƒÑĞ°Ğ¼               + Propensity Pipeline
  Stars Ğ·Ğ° 24Ñ‡                   + Ğ’Ğ¾Ñ€Ğ¾Ğ½ĞºĞ° Ñ ĞºĞ¾Ğ½Ğ²ĞµÑ€ÑĞ¸ÑĞ¼Ğ¸
  Follow-up (ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾)         + Drop-off Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·
  Ğ’ÑĞµĞ³Ğ¾ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹            + Ğ“Ğ¾Ñ€ÑÑ‡Ğ¸Ğµ Ğ»Ğ¸Ğ´Ñ‹ (Ğ¢ĞĞŸ-5)
                                 + AI Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸ + ĞºĞ¾Ğ½Ğ²ĞµÑ€ÑĞ¸Ñ
                                 + Revenue/LTV/ARPU
                                 + Proactive engagement
                                 + Vision Sales Ğ°Ğ½Ğ°Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ°
                                 + A/B Ñ‚ĞµÑÑ‚Ñ‹ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹
                                 + Ğ¢Ñ€ĞµĞ½Ğ´ vs Ğ²Ñ‡ĞµÑ€Ğ° (Ğ´ĞµĞ»ÑŒÑ‚Ñ‹)"""
    sections.append(old_vs_new)

    sections.append("-" * 60)
    sections.append("  DAILY DIGEST v2: ALL SYSTEMS OPERATIONAL")
    sections.append("-" * 60)

    return "\n".join(sections)
