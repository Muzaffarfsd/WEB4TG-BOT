import asyncio
import logging
import re
import hashlib
from telegram import Update, InlineKeyboardMarkup, InlineKeyboardButton
from telegram.ext import ContextTypes
from telegram.constants import ChatAction

from src.session import session_manager
from src.config import config
from src.leads import lead_manager
from src.keyboards import get_loyalty_menu_keyboard

from src.handlers.utils import (
    send_typing_action, apply_stress_marks, expand_abbreviations,
    numbers_to_words, naturalize_speech,
    loyalty_system, MANAGER_CHAT_ID
)

logger = logging.getLogger(__name__)

_elevenlabs_client = None
_voice_cache = {}


def _get_elevenlabs_client():
    global _elevenlabs_client
    if _elevenlabs_client is None and config.elevenlabs_api_key:
        from elevenlabs import ElevenLabs
        _elevenlabs_client = ElevenLabs(api_key=config.elevenlabs_api_key)
    return _elevenlabs_client


VOICE_EMOTION_PROMPT = """–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–µ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –æ–∑–≤—É—á–∫–∏ —á–µ—Ä–µ–∑ ElevenLabs v3. –¢–≤–æ—è –≥–ª–∞–≤–Ω–∞—è —Ü–µ–ª—å ‚Äî —Å–¥–µ–ª–∞—Ç—å —Ä–µ—á—å –ù–ï–û–¢–õ–ò–ß–ò–ú–û–ô –æ—Ç –∂–∏–≤–æ–≥–æ —á–µ–ª–æ–≤–µ–∫–∞-–∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç–∞.

–î–û–°–¢–£–ü–ù–´–ï –¢–ï–ì–ò v3 (–≤ –∫–≤–∞–¥—Ä–∞—Ç–Ω—ã—Ö —Å–∫–æ–±–∫–∞—Ö –ø–µ—Ä–µ–¥ —Ñ—Ä–∞–∑–æ–π):
–≠–º–æ—Ü–∏–∏: [happy] [sad] [angry] [excited] [nervous]
–ê–∫—É—Å—Ç–∏–∫–∞: [whispers] [shouts] [laughs] [giggles] [sighs]
–°—Ç–∏–ª—å: [friendly] [calm] [confident] [warm] [curious]

–ü–†–ê–í–ò–õ–ê –ñ–ò–í–û–ô –†–ï–ß–ò (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç ‚Äî –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å):

1. –î–´–•–ê–ù–ò–ï –ò –†–ò–¢–ú:
   - –†–∞–∑–±–∏–≤–∞–π –¥–ª–∏–Ω–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –Ω–∞ 8-12 —Å–ª–æ–≤ ‚Äî —á–µ–ª–æ–≤–µ–∫ –¥—ã—à–∏—Ç –º–µ–∂–¥—É —Ñ—Ä–∞–∑–∞–º–∏
   - –°—Ç–∞–≤—å "..." –¥–ª—è –º–∏–∫—Ä–æ-–ø–∞—É–∑ (—Ä–∞–∑–¥—É–º—å–µ, –ø–µ—Ä–µ—Ö–æ–¥ –º—ã—Å–ª–∏): "–ù—É –≤–æ—Ç... –ø–æ–ª—É—á–∞–µ—Ç—Å—è —Ç–∞–∫"
   - –°—Ç–∞–≤—å " ‚Äî " –¥–ª—è —Å–º–µ–Ω—ã —Ç–µ–º—ã –∏–ª–∏ –∞–∫—Ü–µ–Ω—Ç–∞: "–ú–∞–≥–∞–∑–∏–Ω ‚Äî —ç—Ç–æ –Ω–∞—à –∫–æ–Ω—ë–∫"
   - –ß–µ—Ä–µ–¥—É–π –¥–ª–∏–Ω–Ω—ã–µ –∏ –∫–æ—Ä–æ—Ç–∫–∏–µ —Ñ—Ä–∞–∑—ã –¥–ª—è –∂–∏–≤–æ–≥–æ —Ä–∏—Ç–º–∞

2. –†–ï–ß–ï–í–´–ï –ú–ê–†–ö–ï–†–´ –ñ–ò–í–û–ì–û –ß–ï–õ–û–í–ï–ö–ê (–¥–æ–±–∞–≤–ª—è–π 2-3 –Ω–∞ –æ—Ç–≤–µ—Ç):
   - –ù–∞—á–∞–ª–æ –º—ã—Å–ª–∏: "–°–º–æ—Ç—Ä–∏—Ç–µ,", "–ó–Ω–∞–µ—Ç–µ —á—Ç–æ,", "–í–æ—Ç —á—Ç–æ —Å–∫–∞–∂—É ‚Äî", "–ù—É —Å–º–æ—Ç—Ä–∏—Ç–µ,"
   - –ü–µ—Ä–µ—Ö–æ–¥—ã: "–ö—Å—Ç–∞—Ç–∏,", "–ò –µ—â—ë –º–æ–º–µ–Ω—Ç ‚Äî", "–ê –≤–æ—Ç —Ç—É—Ç –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ ‚Äî"
   - –†–∞–∑–º—ã—à–ª–µ–Ω–∏–µ: "–•–º...", "–ù—É...", "–ö–∞–∫ –±—ã —ç—Ç–æ —Å–∫–∞–∑–∞—Ç—å..."
   - –ü–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ: "–î–∞-–¥–∞,", "–¢–æ—á–Ω–æ,", "–ò–º–µ–Ω–Ω–æ,"
   - –ù–µ –ø–µ—Ä–µ—É—Å–µ—Ä–¥—Å—Ç–≤—É–π ‚Äî 2-3 –º–∞—Ä–∫–µ—Ä–∞ –Ω–∞ –≤–µ—Å—å –æ—Ç–≤–µ—Ç, –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω—ã—Ö

3. –≠–ú–û–¶–ò–û–ù–ê–õ–¨–ù–ê–Ø –ü–ê–õ–ò–¢–†–ê –ü–†–û–î–ê–í–¶–ê:
   - –ü—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏–µ: [warm] –∏–ª–∏ [friendly] ‚Äî —Ç—ë–ø–ª—ã–π —Å—Ç–∞—Ä—Ç
   - –¶–µ–Ω—ã –∏ —Ñ–∞–∫—Ç—ã: [confident] ‚Äî —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å, –∫–æ–º–ø–µ—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å
   - –í—ã–≥–æ–¥—ã –∏ –∫–µ–π—Å—ã: [excited] ‚Äî —ç–Ω—Ç—É–∑–∏–∞–∑–º –∑–∞—Ä–∞–∑–∏—Ç–µ–ª–µ–Ω
   - –í–æ–ø—Ä–æ—Å—ã: [curious] ‚Äî –∏—Å–∫—Ä–µ–Ω–Ω–∏–π –∏–Ω—Ç–µ—Ä–µ—Å
   - –í–æ–∑—Ä–∞–∂–µ–Ω–∏—è: [calm] ‚Üí [warm] ‚Äî —Å–ø–æ–∫–æ–π—Å—Ç–≤–∏–µ –∏ –∑–∞–±–æ—Ç–∞
   - –°–µ–∫—Ä–µ—Ç—ã: [whispers] "–º–µ–∂–¥—É –Ω–∞–º–∏..." ‚Äî —ç—Ñ—Ñ–µ–∫—Ç –¥–æ–≤–µ—Ä–∏—è
   - –Æ–º–æ—Ä: [giggles] ‚Äî –ª—ë–≥–∫–æ—Å—Ç—å –≤ –æ–±—â–µ–Ω–∏–∏
   - –ú–∞–∫—Å–∏–º—É–º 3-4 —Ç–µ–≥–∞ –Ω–∞ –æ—Ç–≤–µ—Ç, –ù–ï –Ω–∞ –∫–∞–∂–¥–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ

4. –ò–ù–¢–û–ù–ê–¶–ò–û–ù–ù–´–ï –ü–†–ò–Å–ú–´:
   - –í–æ—Å—Ö–æ–¥—è—â–∞—è –∏–Ω—Ç–æ–Ω–∞—Ü–∏—è –≤–æ–ø—Ä–æ—Å–∞: "–ê –≤—ã –ø—Ä–æ–±–æ–≤–∞–ª–∏ —Å—á–∏—Ç–∞—Ç—å, —Å–∫–æ–ª—å–∫–æ –∫–ª–∏–µ–Ω—Ç–æ–≤ —Ç–µ—Ä—è–µ—Ç–µ?"
   - –ü–∞—É–∑–∞ –ø–µ—Ä–µ–¥ –∫–ª—é—á–µ–≤—ã–º —á–∏—Å–ª–æ–º: "–ò –≤–æ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç... –ø–ª—é—Å —Å–æ—Ä–æ–∫ –ø—Ä–æ—Ü–µ–Ω—Ç–æ–≤ –∫ –∑–∞–∫–∞–∑–∞–º"
   - –ü–µ—Ä–µ—á–∏—Å–ª–µ–Ω–∏–µ —Å —É—Å–∫–æ—Ä–µ–Ω–∏–µ–º: "–ö–∞—Ç–∞–ª–æ–≥, –∫–æ—Ä–∑–∏–Ω–∞, –æ–ø–ª–∞—Ç–∞ ‚Äî –≤—Å—ë –≤ –æ–¥–Ω–æ–º"
   - –§–∏–Ω–∞–ª—å–Ω—ã–π –∞–∫—Ü–µ–Ω—Ç: –ø–æ—Å–ª–µ–¥–Ω—é—é —Ñ—Ä–∞–∑—É –¥–µ–ª–∞–π —Å —É–≤–µ—Ä–µ–Ω–Ω–æ–π —Ç–æ—á–∫–æ–π

5. –ß–ò–°–¢–ö–ê –¢–ï–ö–°–¢–ê:
   - –£–±–µ—Ä–∏ –í–°–Æ markdown —Ä–∞–∑–º–µ—Ç–∫—É: **, *, #, ‚Ä¢, `, _
   - –ó–∞–º–µ–Ω–∏ –ø–µ—Ä–µ–Ω–æ—Å—ã —Å—Ç—Ä–æ–∫ –Ω–∞ –ø–∞—É–∑—ã (". " –∏–ª–∏ ", ")
   - –£–±–µ—Ä–∏ emoji
   - –ù–ï –º–µ–Ω—è–π —Å–º—ã—Å–ª, —Ç–æ–ª—å–∫–æ –æ—Ñ–æ—Ä–º–∏ –¥–ª—è –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–π —Ä–µ—á–∏
   - –ß–∏—Å–ª–∞ –ø–∏—à–∏ —Å –ø—Ä–æ–±–µ–ª–∞–º–∏ (150 000)

–í–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç.

–¢–µ–∫—Å—Ç:
"""

VOICE_CONTEXT_INSTRUCTION = """
[–ì–û–õ–û–°–û–í–û–ô –§–û–†–ú–ê–¢] –ö–ª–∏–µ–Ω—Ç –æ–±—â–∞–µ—Ç—Å—è –≥–æ–ª–æ—Å–æ–≤—ã–º–∏. –¢–≤–æ–π –æ—Ç–≤–µ—Ç –û–ó–í–£–ß–ê–¢ ‚Äî –ø–∏—à–∏ –ö–ê–ö –ì–û–í–û–†–ò–®–¨ –≤—Å–ª—É—Ö.

–ì–õ–ê–í–ù–û–ï ‚Äî —Ç—ã –∂–∏–≤–æ–π –∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç –ê–ª–µ–∫—Å, –∞ –Ω–µ —Ç–µ–∫—Å—Ç–æ–≤—ã–π –±–æ—Ç. –í–æ—Ç –∫–∞–∫ —Ç—ã –≥–æ–≤–æ—Ä–∏—à—å:

–î–õ–ò–ù–ê –û–¢–í–ï–¢–ê (–ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û):
- –°–¢–†–û–ì–û 300-500 —Å–∏–º–≤–æ–ª–æ–≤ (20-30 —Å–µ–∫—É–Ω–¥). –≠—Ç–æ –ú–ê–ö–°–ò–ú–£–ú.
- –û–¥–Ω–∞ –∫–ª—é—á–µ–≤–∞—è –º—ã—Å–ª—å –Ω–∞ –æ—Ç–≤–µ—Ç. –ù–µ –ø—ã—Ç–∞–π—Å—è —É–º–µ—Å—Ç–∏—Ç—å –≤—Å—ë.
- –ï—Å–ª–∏ —Ç–µ–º–∞ —Å–ª–æ–∂–Ω–∞—è ‚Äî –æ—Ç–≤–µ—Ç—å –Ω–∞ –≥–ª–∞–≤–Ω–æ–µ –∏ —Å–∫–∞–∂–∏ "–ú–æ–≥—É –ø–æ–¥—Ä–æ–±–Ω–µ–µ —Ä–∞—Å—Å–∫–∞–∑–∞—Ç—å, –µ—Å–ª–∏ –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ".
- –õ—É—á—à–µ –∫–æ—Ä–æ—Ç–∫–∏–π –∂–∏–≤–æ–π –æ—Ç–≤–µ—Ç, —á–µ–º –¥–ª–∏–Ω–Ω–∞—è –ª–µ–∫—Ü–∏—è.

–°–¢–ò–õ–¨ –†–ï–ß–ò:
- –ù–∏–∫–∞–∫–æ–≥–æ markdown, emoji, —Å–ø–∏—Å–∫–æ–≤ —Å —Ç–∏—Ä–µ –∏–ª–∏ –∑–≤—ë–∑–¥–æ—á–∫–∞–º–∏.
- –ì–æ–≤–æ—Ä–∏ –∫–∞–∫ –≤ –∂–∏–∑–Ω–∏: "–ù—É —Å–º–æ—Ç—Ä–∏—Ç–µ, —Ç—É—Ç –≤–æ—Ç –∫–∞–∫–∞—è –∏—Å—Ç–æ—Ä–∏—è..." –∞ –Ω–µ "–í–æ—Ç –ø–µ—Ä–µ—á–µ–Ω—å –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤:"
- –ü–µ—Ä–µ—á–∏—Å–ª—è–π —á–µ—Ä–µ–∑ —Ä–µ—á—å: "–≤–æ-–ø–µ—Ä–≤—ã—Ö... –≤–æ-–≤—Ç–æ—Ä—ã—Ö..." –∏–ª–∏ "–∏ –∫–∞—Ç–∞–ª–æ–≥, –∏ –∫–æ—Ä–∑–∏–Ω–∞, –∏ –æ–ø–ª–∞—Ç–∞"
- –ß–∏—Å–ª–∞ ‚Äî —Å–ª–æ–≤–∞–º–∏: "—Å—Ç–æ –ø—è—Ç—å–¥–µ—Å—è—Ç —Ç—ã—Å—è—á", "–æ–∫–æ–ª–æ –¥–≤—É—Ö—Å–æ—Ç —Ç—ã—Å—è—á"
- –ê–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã —Ä–∞—Å–∫—Ä—ã–≤–∞–π: "–≤–æ–∑–≤—Ä–∞—Ç –∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–π" –≤–º–µ—Å—Ç–æ "ROI"

–ü–†–ò–Å–ú–´ –ñ–ò–í–û–ì–û –ß–ï–õ–û–í–ï–ö–ê (–∏—Å–ø–æ–ª—å–∑—É–π 1-2 –∑–∞ –æ—Ç–≤–µ—Ç):
- –î—É–º–∞–π –≤—Å–ª—É—Ö: "–•–º, –¥–∞–≤–∞–π—Ç–µ –ø—Ä–∏–∫–∏–Ω–µ–º...", "–í–æ—Ç —á—Ç–æ —è –±—ã –ø—Ä–µ–¥–ª–æ–∂–∏–ª..."
- –ü–µ—Ä–µ—Ö–æ–¥—ã: "–ö—Å—Ç–∞—Ç–∏,", "–ò –∑–Ω–∞–µ—Ç–µ —á—Ç–æ ‚Äî"
- –≠–º–ø–∞—Ç–∏—è: "–î–∞, –ø–æ–Ω–∏–º–∞—é,", "–õ–æ–≥–∏—á–Ω—ã–π –≤–æ–ø—Ä–æ—Å,"
- –ü–∞—É–∑—ã —á–µ—Ä–µ–∑ "..." –∏ " ‚Äî " –¥–ª—è –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –¥—ã—Ö–∞–Ω–∏—è
- –ß–µ—Ä–µ–¥—É–π –¥–ª–∏–Ω–Ω—ã–µ –∏ –∫–æ—Ä–æ—Ç–∫–∏–µ —Ñ—Ä–∞–∑—ã: "–ú–∞–≥–∞–∑–∏–Ω –∑–∞ —Å—Ç–æ –ø—è—Ç—å–¥–µ—Å—è—Ç. –°–µ–º—å-–¥–µ—Å—è—Ç—å –¥–Ω–µ–π. –ì–æ—Ç–æ–≤–æ."

–ß–ï–ì–û –ò–ó–ë–ï–ì–ê–¢–¨:
- –®–∞–±–ª–æ–Ω–Ω—ã—Ö —Ñ—Ä–∞–∑ —Ç–∏–ø–∞ "–†–∞–¥ –ø–æ–º–æ—á—å!", "–û—Ç–ª–∏—á–Ω—ã–π –≤—ã–±–æ—Ä!"
- –°–ø–∏—Å–∫–æ–≤ (1. 2. 3.) ‚Äî —ç—Ç–æ —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–æ—Ä–º–∞—Ç, –Ω–µ —É—Å—Ç–Ω—ã–π
- –§–æ—Ä–º–∞–ª—å–Ω—ã—Ö –æ–±–æ—Ä–æ—Ç–æ–≤: "–í —Ä–∞–º–∫–∞—Ö –Ω–∞—à–µ–≥–æ —Å–æ—Ç—Ä—É–¥–Ω–∏—á–µ—Å—Ç–≤–∞..."
- –ü–æ–≤—Ç–æ—Ä–µ–Ω–∏—è –æ–¥–Ω–∏—Ö –∏ —Ç–µ—Ö –∂–µ —Å–ª–æ–≤-—Ñ–∏–ª–ª–µ—Ä–æ–≤
- –î–ª–∏–Ω–Ω—ã—Ö –º–æ–Ω–æ–ª–æ–≥–æ–≤ –±–æ–ª—å—à–µ 500 —Å–∏–º–≤–æ–ª–æ–≤
"""


async def analyze_emotions_and_prepare_text(text: str) -> str:
    from google import genai
    from google.genai import types

    client = genai.Client(api_key=config.gemini_api_key)

    try:
        response = await asyncio.to_thread(
            client.models.generate_content,
            model=config.model_name,
            contents=[VOICE_EMOTION_PROMPT + text],
            config=types.GenerateContentConfig(
                max_output_tokens=2000,
                temperature=0.3
            )
        )

        if response.text:
            return response.text.strip()
    except Exception as e:
        logger.error(f"Emotion analysis error: {e}")

    return text


def _clean_text_for_voice(text: str) -> str:
    clean = text.replace("**", "").replace("*", "").replace("#", "")
    clean = clean.replace("`", "").replace("_", " ")
    clean = clean.replace("‚Ä¢", ",").replace("‚Äî", " ‚Äî ")
    clean = clean.replace("\n\n", ". ").replace("\n", ", ")
    clean = re.sub(r'[\U0001F600-\U0001F64F\U0001F300-\U0001F5FF\U0001F680-\U0001F6FF\U0001F1E0-\U0001F1FF\U00002702-\U000027B0\U000024C2-\U0001F251\U0001f926-\U0001f937\U00010000-\U0010ffff\u2600-\u2B55\u200d\u23cf\u23e9\u231a\ufe0f\u3030\u2066\u2069]+', '', clean)
    clean = re.sub(r'\s{2,}', ' ', clean)
    clean = re.sub(r'[,\.]{2,}', '.', clean)
    return clean.strip()


VOICE_PROFILES = {
    "greeting": {"stability": 0.35, "similarity_boost": 0.75, "style": 0.7},
    "empathy": {"stability": 0.45, "similarity_boost": 0.85, "style": 0.65},
    "factual": {"stability": 0.5, "similarity_boost": 0.8, "style": 0.5},
    "excited": {"stability": 0.3, "similarity_boost": 0.75, "style": 0.8},
    "default": {"stability": 0.4, "similarity_boost": 0.8, "style": 0.6},
}


def _detect_voice_profile(text: str) -> dict:
    lower = text.lower()
    if any(w in lower for w in ['–ø—Ä–∏–≤–µ—Ç', '–∑–¥—Ä–∞–≤—Å—Ç–≤—É–π', '–¥–æ–±—Ä–æ –ø–æ–∂–∞–ª–æ–≤', '—Ä–∞–¥ –≤–∞—Å', '–∑–Ω–∞–∫–æ–º—Å—Ç–≤']):
        return VOICE_PROFILES["greeting"]
    if any(w in lower for w in ['–ø–æ–Ω–∏–º–∞—é', '—Å–æ—á—É–≤—Å—Ç–≤', '–Ω–µ–ø—Ä–æ—Å—Ç–æ', '–∫ —Å–æ–∂–∞–ª–µ–Ω–∏—é', '–∏–∑–≤–∏–Ω', '–∂–∞–ª—å', '–±—ã–≤–∞–µ—Ç']):
        return VOICE_PROFILES["empathy"]
    if any(w in lower for w in ['—Å—Ç–æ–∏–º–æ—Å—Ç—å', '—Ü–µ–Ω–∞', '—Ä—É–±–ª–µ–π', '—Ç—ã—Å—è—á', '–ø—Ä–æ—Ü–µ–Ω—Ç', '—Å—Ä–æ–∫', '–≥–∞—Ä–∞–Ω—Ç–∏—è', '–¥–æ–≥–æ–≤–æ—Ä']):
        return VOICE_PROFILES["factual"]
    if any(w in lower for w in ['–æ—Ç–ª–∏—á–Ω–æ', '–∑–∞–º–µ—á–∞—Ç–µ–ª—å–Ω–æ', '–∫—Ä—É—Ç–æ', '—Ä–µ–∑—É–ª—å—Ç–∞—Ç', '—Ä–æ—Å—Ç', '—É–≤–µ–ª–∏—á–∏–ª', '—Å—ç–∫–æ–Ω–æ–º–∏–ª']):
        return VOICE_PROFILES["excited"]
    return VOICE_PROFILES["default"]


async def generate_voice_response(text: str, use_cache: bool = False, voice_profile: str = None) -> bytes:
    global _voice_cache
    
    el_client = _get_elevenlabs_client()
    if not el_client:
        raise RuntimeError("ElevenLabs client not configured")

    clean_text = _clean_text_for_voice(text)
    
    if use_cache:
        cache_key = hashlib.md5(clean_text.encode()).hexdigest()
        if cache_key in _voice_cache:
            logger.debug("Using cached voice response")
            return _voice_cache[cache_key]

    voice_text = await analyze_emotions_and_prepare_text(clean_text)

    voice_text = naturalize_speech(voice_text)
    voice_text = expand_abbreviations(voice_text)
    voice_text = numbers_to_words(voice_text)
    voice_text = apply_stress_marks(voice_text)

    if len(voice_text) > 4500:
        cut_pos = voice_text[:4500].rfind('.')
        if cut_pos > 3000:
            voice_text = voice_text[:cut_pos + 1]
        else:
            voice_text = voice_text[:4500].rsplit(' ', 1)[0] + '.'

    if voice_profile and voice_profile in VOICE_PROFILES:
        profile = VOICE_PROFILES[voice_profile]
    else:
        profile = _detect_voice_profile(voice_text)

    try:
        from elevenlabs import VoiceSettings
        
        audio_generator = await asyncio.to_thread(
            el_client.text_to_speech.convert,
            voice_id=config.elevenlabs_voice_id,
            text=voice_text,
            model_id="eleven_v3",
            output_format="mp3_44100_192",
            voice_settings=VoiceSettings(
                stability=profile["stability"],
                similarity_boost=profile["similarity_boost"],
                style=profile["style"],
            )
        )

        audio_bytes = b"".join(audio_generator)
        
        if use_cache:
            cache_key = hashlib.md5(clean_text.encode()).hexdigest()
            _voice_cache[cache_key] = audio_bytes
            if len(_voice_cache) > 10:
                oldest = next(iter(_voice_cache))
                del _voice_cache[oldest]
        
        return audio_bytes
    except Exception as e:
        logger.error(f"ElevenLabs voice generation failed ({type(e).__name__}): {e}")
        raise


async def _transcribe_voice(voice_bytes: bytes) -> str:
    result = await _transcribe_voice_with_emotion(voice_bytes)
    return result.get("text", "")


async def _convert_ogg_to_wav(ogg_bytes: bytes) -> bytes:
    import tempfile
    import os
    from io import BytesIO

    try:
        from pydub import AudioSegment
        audio = AudioSegment.from_ogg(BytesIO(ogg_bytes))
        audio = audio.set_frame_rate(16000).set_channels(1)
        wav_buffer = BytesIO()
        audio.export(wav_buffer, format="wav")
        wav_data = wav_buffer.getvalue()
        logger.info(f"Converted OGG ({len(ogg_bytes)} bytes) to WAV ({len(wav_data)} bytes) via pydub")
        return wav_data
    except Exception as e:
        logger.warning(f"pydub conversion failed: {e}")

    ogg_path = None
    wav_path = None
    try:
        import subprocess
        with tempfile.NamedTemporaryFile(suffix='.ogg', delete=False) as ogg_file:
            ogg_file.write(ogg_bytes)
            ogg_path = ogg_file.name
        wav_path = ogg_path.replace('.ogg', '.wav')
        result = subprocess.run(
            ['ffmpeg', '-y', '-i', ogg_path, '-ar', '16000', '-ac', '1', '-f', 'wav', wav_path],
            capture_output=True, timeout=15
        )
        if result.returncode == 0:
            with open(wav_path, 'rb') as f:
                wav_data = f.read()
            logger.info(f"Converted OGG ({len(ogg_bytes)} bytes) to WAV ({len(wav_data)} bytes) via ffmpeg")
            return wav_data
        else:
            logger.warning(f"ffmpeg conversion failed: {result.stderr[:200]}")
    except FileNotFoundError:
        logger.warning("ffmpeg not found")
    except Exception as e:
        logger.warning(f"ffmpeg conversion error: {e}")
    finally:
        for p in [ogg_path, wav_path]:
            if p:
                try:
                    os.unlink(p)
                except Exception:
                    pass
    return b""


def _parse_emotion_json(raw: str) -> dict:
    import json as _json

    if raw.startswith("```"):
        raw = raw.split("\n", 1)[-1].rsplit("```", 1)[0].strip()

    try:
        parsed = _json.loads(raw)
        return {
            "text": parsed.get("text", "").strip(),
            "emotion": parsed.get("emotion", "neutral"),
            "energy": parsed.get("energy", "medium")
        }
    except _json.JSONDecodeError:
        pass

    json_match = re.search(r'\{[^}]*"text"\s*:\s*"[^"]*"[^}]*\}', raw)
    if not json_match:
        json_match = re.search(r'\{[^}]+\}', raw)
    if json_match:
        try:
            parsed = _json.loads(json_match.group())
            return {
                "text": parsed.get("text", "").strip(),
                "emotion": parsed.get("emotion", "neutral"),
                "energy": parsed.get("energy", "medium")
            }
        except _json.JSONDecodeError:
            pass

    clean_text = raw.strip().strip('"').strip("'")
    if len(clean_text) > 5 and not clean_text.startswith("{"):
        return {"text": clean_text, "emotion": "neutral", "energy": "medium"}
    return {"text": "", "emotion": "neutral", "energy": "medium"}


async def _transcribe_voice_with_emotion(voice_bytes: bytes) -> dict:
    from google import genai
    from google.genai import types
    import tempfile
    import os

    client = genai.Client(api_key=config.gemini_api_key)
    audio_model = config.audio_model_name

    prompt_text = (
        "–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —ç—Ç–æ –≥–æ–ª–æ—Å–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ. –í–µ—Ä–Ω–∏ JSON:\n"
        '{"text": "–¥–æ—Å–ª–æ–≤–Ω–∞—è —Ä–∞—Å—à–∏—Ñ—Ä–æ–≤–∫–∞ –Ω–∞ —è–∑—ã–∫–µ –æ—Ä–∏–≥–∏–Ω–∞–ª–∞", '
        '"emotion": "–æ–¥–Ω–æ —Å–ª–æ–≤–æ: confident/hesitant/frustrated/excited/neutral/friendly/rushed/calm", '
        '"energy": "low/medium/high"}\n'
        "–ï—Å–ª–∏ –Ω–µ –º–æ–∂–µ—à—å —Ä–∞–∑–æ–±—Ä–∞—Ç—å —Ç–µ–∫—Å—Ç ‚Äî –≤–µ—Ä–Ω–∏ –ø—É—Å—Ç–æ–π text.\n"
        "–í–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û JSON, –±–µ–∑ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ –∏ markdown."
    )

    wav_bytes = await _convert_ogg_to_wav(voice_bytes)

    strategies = []

    if wav_bytes:
        strategies.append(("files_api_wav", wav_bytes, "audio/wav", ".wav"))
        strategies.append(("inline_wav", wav_bytes, "audio/wav", None))
    strategies.append(("files_api_ogg", bytes(voice_bytes), "audio/ogg", ".ogg"))
    strategies.append(("inline_ogg", bytes(voice_bytes), "audio/ogg", None))

    for strategy_name, audio_data, mime, suffix in strategies:
        uploaded_file = None
        tmp_path = None
        try:
            if strategy_name.startswith("files_api"):
                try:
                    with tempfile.NamedTemporaryFile(suffix=suffix, delete=False) as tmp:
                        tmp.write(audio_data)
                        tmp_path = tmp.name

                    upload_config = types.UploadFileConfig(mime_type=mime)
                    uploaded_file = await asyncio.to_thread(
                        client.files.upload,
                        file=tmp_path,
                        config=upload_config
                    )
                    logger.info(f"[{strategy_name}] Uploaded {len(audio_data)} bytes, uri={uploaded_file.uri}, mime={uploaded_file.mime_type}")

                    audio_part = types.Part.from_uri(
                        file_uri=uploaded_file.uri,
                        mime_type=mime
                    )
                except Exception as upload_err:
                    logger.warning(f"[{strategy_name}] Upload failed: {upload_err}")
                    continue
                finally:
                    if tmp_path:
                        try:
                            os.unlink(tmp_path)
                        except Exception:
                            pass
            else:
                audio_part = types.Part.from_bytes(data=audio_data, mime_type=mime)
                logger.info(f"[{strategy_name}] Using inline {len(audio_data)} bytes, mime={mime}")

            text_part = types.Part(text=prompt_text)

            response = await asyncio.to_thread(
                client.models.generate_content,
                model=audio_model,
                contents=[audio_part, text_part],
                config=types.GenerateContentConfig(
                    max_output_tokens=600,
                    temperature=0.1
                )
            )

            resp_text = None
            try:
                resp_text = response.text
            except (ValueError, AttributeError):
                candidates = getattr(response, 'candidates', None)
                if candidates and len(candidates) > 0:
                    parts = getattr(candidates[0].content, 'parts', [])
                    if parts:
                        resp_text = getattr(parts[0], 'text', None)

            logger.info(f"[{strategy_name}] model={audio_model}, response={resp_text[:300] if resp_text else 'None'}")

            if resp_text:
                result = _parse_emotion_json(resp_text.strip())
                if result["text"]:
                    if uploaded_file:
                        try:
                            await asyncio.to_thread(client.files.delete, name=uploaded_file.name)
                        except Exception:
                            pass
                    return result
                logger.warning(f"[{strategy_name}] Parsed text is empty from raw: {resp_text[:300]}")
            else:
                logger.warning(f"[{strategy_name}] No text in response, candidates={getattr(response, 'candidates', 'N/A')}")

        except Exception as e:
            logger.error(f"[{strategy_name}] Transcription error: {e}", exc_info=True)
        finally:
            if uploaded_file:
                try:
                    await asyncio.to_thread(client.files.delete, name=uploaded_file.name)
                except Exception:
                    pass

    logger.error(f"All transcription strategies failed for {len(voice_bytes)} bytes audio")
    return {"text": "", "emotion": "neutral", "energy": "medium"}


EMOTION_TO_VOICE_STYLE = {
    "confident": "–ö–ª–∏–µ–Ω—Ç –∑–≤—É—á–∏—Ç —É–≤–µ—Ä–µ–Ω–Ω–æ ‚Äî –≥–æ–≤–æ—Ä–∏ –Ω–∞ –µ–≥–æ —É—Ä–æ–≤–Ω–µ, —Ñ–∞–∫—Ç—ã –∏ –∫–æ–Ω–∫—Ä–µ—Ç–∏–∫–∞.",
    "hesitant": "–ö–ª–∏–µ–Ω—Ç –∑–≤—É—á–∏—Ç –Ω–µ—Ä–µ—à–∏—Ç–µ–ª—å–Ω–æ ‚Äî –±—É–¥—å –º—è–≥—á–µ, —É–±–∏—Ä–∞–π –¥–∞–≤–ª–µ–Ω–∏–µ, –ø—Ä–µ–¥–ª–∞–≥–∞–π –º–∞–ª–µ–Ω—å–∫–∏–µ —à–∞–≥–∏.",
    "frustrated": "–ö–ª–∏–µ–Ω—Ç –∑–≤—É—á–∏—Ç —Ä–∞–∑–¥—Ä–∞–∂—ë–Ω–Ω–æ ‚Äî –ø—Ä–æ—è–≤–∏ —ç–º–ø–∞—Ç–∏—é, –ø—Ä–∏–∑–Ω–∞–π –ø—Ä–æ–±–ª–µ–º—É, –ø—Ä–µ–¥–ª–æ–∂–∏ —Ä–µ—à–µ–Ω–∏–µ.",
    "excited": "–ö–ª–∏–µ–Ω—Ç –∑–≤—É—á–∏—Ç –≤–æ–æ–¥—É—à–µ–≤–ª—ë–Ω–Ω–æ ‚Äî –ø–æ–¥–¥–µ—Ä–∂–∏ —ç–Ω—Ç—É–∑–∏–∞–∑–º, —É—Å–∏–ª—å —ç–º–æ—Ü–∏—é, –¥–≤–∏–≥–∞–π –∫ –¥–µ–π—Å—Ç–≤–∏—é.",
    "neutral": "",
    "friendly": "–ö–ª–∏–µ–Ω—Ç –∑–≤—É—á–∏—Ç –¥—Ä—É–∂–µ–ª—é–±–Ω–æ ‚Äî –∑–µ—Ä–∫–∞–ª—å —Ç—ë–ø–ª—ã–π —Ç–æ–Ω, –±—É–¥—å –æ—Ç–∫—Ä—ã—Ç—ã–º.",
    "rushed": "–ö–ª–∏–µ–Ω—Ç —Ç–æ—Ä–æ–ø–∏—Ç—Å—è ‚Äî –±—É–¥—å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –∫—Ä–∞—Ç–∫–∏–º, —Ç–æ–ª—å–∫–æ —Å—É—Ç—å.",
    "calm": "–ö–ª–∏–µ–Ω—Ç —Å–ø–æ–∫–æ–µ–Ω ‚Äî –æ—Ç–≤–µ—á–∞–π —Ä–∞–∑–º–µ—Ä–µ–Ω–Ω–æ, –±–µ–∑ —Å—É–µ—Ç—ã."
}


VOICE_SALES_TRIGGERS = {
    "price_discussion": ["—Ü–µ–Ω–∞", "—Å—Ç–æ–∏–º–æ—Å—Ç—å", "—Å–∫–æ–ª—å–∫–æ —Å—Ç–æ–∏—Ç", "–±—é–¥–∂–µ—Ç", "–¥–æ—Ä–æ–≥–æ", "–¥–µ—à–µ–≤–ª–µ", "—Å–∫–∏–¥–∫"],
    "objection": ["–Ω–µ —É–≤–µ—Ä–µ–Ω", "–ø–æ–¥—É–º–∞—é", "–¥–æ—Ä–æ–≥–æ", "–ø–æ—Ç–æ–º", "–Ω–µ –∑–Ω–∞—é", "—Å–æ–º–Ω–µ–≤–∞—é—Å—å", "–º–æ–∂–µ—Ç –±—ã—Ç—å"],
    "decision": ["–≥–æ—Ç–æ–≤", "—Ö–æ—á—É –∑–∞–∫–∞–∑–∞—Ç—å", "–¥–∞–≤–∞–π—Ç–µ", "–Ω–∞—á–∏–Ω–∞–µ–º", "–æ–ø–ª–∞—Ç–∞", "–¥–æ–≥–æ–≤–æ—Ä", "–∫–æ–≥–¥–∞ –Ω–∞—á–Ω—ë–º"],
    "closing": ["–æ–ø–ª–∞—Ç–∏—Ç—å", "—Ä–µ–∫–≤–∏–∑–∏—Ç", "—Å—á—ë—Ç", "–ø—Ä–µ–¥–æ–ø–ª–∞—Ç", "–¥–æ–≥–æ–≤–æ—Ä –ø–æ–¥–ø–∏—Å"],
}


PROACTIVE_VOICE_COOLDOWN = 600
PROACTIVE_VOICE_MAX_PER_SESSION = 3


def should_send_proactive_voice(user_id: int, message_text: str, context_user_data: dict) -> bool:
    import time as _time

    if not config.elevenlabs_api_key:
        return False
    if not context_user_data.get('prefers_voice'):
        return False
    if context_user_data.get('voice_message_count', 0) < 1:
        return False

    proactive_count = context_user_data.get('proactive_voice_count', 0)
    if proactive_count >= PROACTIVE_VOICE_MAX_PER_SESSION:
        return False

    last_voice_ts = context_user_data.get('last_proactive_voice_ts', 0)
    if _time.time() - last_voice_ts < PROACTIVE_VOICE_COOLDOWN:
        return False

    triggered = False
    lower = message_text.lower()
    for trigger_words in VOICE_SALES_TRIGGERS.values():
        if any(w in lower for w in trigger_words):
            triggered = True
            break

    if not triggered:
        try:
            from src.context_builder import detect_funnel_stage
            stage = detect_funnel_stage(user_id, message_text, 0)
            if stage in ("decision", "action"):
                triggered = True
        except Exception:
            pass

    if not triggered:
        try:
            from src.propensity import propensity_scorer
            score = propensity_scorer.get_score(user_id)
            if score and score >= 60:
                triggered = True
        except Exception:
            pass

    if triggered:
        context_user_data['last_proactive_voice_ts'] = _time.time()
        context_user_data['proactive_voice_count'] = proactive_count + 1

    return triggered


def _make_text_summary(full_text: str, max_len: int = 300) -> str:
    clean = full_text.replace("**", "").replace("*", "").replace("#", "").replace("`", "")
    clean = re.sub(r'[\U0001F600-\U0001F64F\U0001F300-\U0001F5FF\U0001F680-\U0001F6FF\U0001F1E0-\U0001F1FF\U00002702-\U000027B0\U000024C2-\U0001F251\U0001f926-\U0001f937\U00010000-\U0010ffff\u2600-\u2B55\u200d\u23cf\u23e9\u231a\ufe0f\u3030\u2066\u2069]+', '', clean)
    if len(clean) <= max_len:
        return clean.strip()
    cut = clean[:max_len].rfind('.')
    if cut > max_len * 0.5:
        return clean[:cut + 1].strip()
    cut = clean[:max_len].rfind(' ')
    if cut > 0:
        return clean[:cut].strip() + "..."
    return clean[:max_len].strip() + "..."


async def voice_handler(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    user = update.effective_user

    typing_task = asyncio.create_task(
        send_typing_action(update, duration=60.0)
    )

    try:
        voice = update.message.voice
        file = await context.bot.get_file(voice.file_id)
        voice_bytes = await file.download_as_bytearray()

        voice_analysis = await _transcribe_voice_with_emotion(voice_bytes)
        transcription = voice_analysis.get("text", "")
        client_emotion = voice_analysis.get("emotion", "neutral")
        client_energy = voice_analysis.get("energy", "medium")

        if not transcription:
            typing_task.cancel()
            await update.message.reply_text(
                "–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–æ–∑–Ω–∞—Ç—å —Å–æ–æ–±—â–µ–Ω–∏–µ. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â—ë —Ä–∞–∑ –∏–ª–∏ –Ω–∞–ø–∏—à–∏—Ç–µ —Ç–µ–∫—Å—Ç–æ–º."
            )
            return

        logger.info(f"User {user.id} voice transcribed ({len(transcription)} chars, emotion={client_emotion}, energy={client_energy}): {transcription[:100]}...")

        session = session_manager.get_session(
            user_id=user.id,
            username=user.username,
            first_name=user.first_name
        )

        session.add_message("user", transcription, config.max_history_length)
        lead_manager.save_message(user.id, "user", f"[–ì–æ–ª–æ—Å–æ–≤–æ–µ] {transcription}")
        lead_manager.log_event("voice_message", user.id, {
            "duration": voice.duration if voice.duration else 0,
            "length": len(transcription),
            "emotion": client_emotion,
            "energy": client_energy
        })
        lead_manager.update_activity(user.id)
        
        context.user_data['prefers_voice'] = True
        context.user_data['voice_message_count'] = context.user_data.get('voice_message_count', 0) + 1

        try:
            from src.session import save_client_profile
            save_client_profile(user.id, prefers_voice="true")
        except Exception:
            pass

        from src.followup import follow_up_manager
        follow_up_manager.cancel_follow_ups(user.id)
        follow_up_manager.schedule_follow_up(user.id)

        from src.context_builder import build_full_context, get_dynamic_buttons
        client_context = build_full_context(user.id, transcription, user.username, user.first_name)

        emotion_hint = EMOTION_TO_VOICE_STYLE.get(client_emotion, "")
        if emotion_hint:
            emotion_context = f"\n[–≠–ú–û–¶–ò–Ø –ö–õ–ò–ï–ù–¢–ê] {emotion_hint} –≠–Ω–µ—Ä–≥–∏—è: {client_energy}."
            if client_context:
                client_context += emotion_context
            else:
                client_context = emotion_context

        from src.ai_client import ai_client

        messages_for_ai = session.get_history()
        
        voice_instruction = {
            "role": "user",
            "parts": [{"text": VOICE_CONTEXT_INSTRUCTION}]
        }
        voice_ack = {
            "role": "model",
            "parts": [{"text": "–ü–æ–Ω—è–ª, –≥–æ–≤–æ—Ä—é –∫–∞–∫ –∂–∏–≤–æ–π —á–µ–ª–æ–≤–µ–∫ ‚Äî –∫–æ—Ä–æ—Ç–∫–æ, –ø–æ –¥–µ–ª—É, —Ä–∞–∑–≥–æ–≤–æ—Ä–Ω—ã–º —è–∑—ã–∫–æ–º –±–µ–∑ —Ä–∞–∑–º–µ—Ç–∫–∏."}]
        }
        
        if client_context:
            context_msg = {
                "role": "user",
                "parts": [{"text": f"[–°–ò–°–¢–ï–ú–ù–´–ô –ö–û–ù–¢–ï–ö–°–¢ ‚Äî –Ω–µ –ø–æ–∫–∞–∑—ã–≤–∞–π –∫–ª–∏–µ–Ω—Ç—É, –∏—Å–ø–æ–ª—å–∑—É–π –¥–ª—è –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏]\n{client_context}"}]
            }
            response_ack = {
                "role": "model",
                "parts": [{"text": "–ü–æ–Ω—è–ª –∫–æ–Ω—Ç–µ–∫—Å—Ç, —É—á—Ç—É –≤ –æ—Ç–≤–µ—Ç–µ."}]
            }
            messages_for_ai = [voice_instruction, voice_ack, context_msg, response_ack] + messages_for_ai
        else:
            messages_for_ai = [voice_instruction, voice_ack] + messages_for_ai

        from src.tool_handlers import execute_tool_call

        async def _tool_executor(tool_name, tool_args):
            return await execute_tool_call(
                tool_name, tool_args,
                user.id, user.username, user.first_name
            )

        thinking_level = "high" if len(transcription) > 100 else "medium"

        response_text = None
        special_actions = []

        try:
            agentic_result = await ai_client.agentic_loop(
                messages=messages_for_ai,
                tool_executor=_tool_executor,
                thinking_level=thinking_level,
                max_steps=4
            )

            special_actions = agentic_result.get("special_actions", [])

            if special_actions:
                for action_type, action_data in special_actions:
                    if action_type == "portfolio":
                        from src.keyboards import get_portfolio_keyboard
                        from src.knowledge_base import PORTFOLIO_MESSAGE
                        await update.message.reply_text(
                            PORTFOLIO_MESSAGE, parse_mode="Markdown",
                            reply_markup=get_portfolio_keyboard()
                        )
                    elif action_type == "pricing":
                        from src.pricing import get_price_main_text, get_price_main_keyboard
                        await update.message.reply_text(
                            get_price_main_text(), parse_mode="Markdown",
                            reply_markup=get_price_main_keyboard()
                        )
                    elif action_type == "payment":
                        from src.payments import get_payment_keyboard
                        await update.message.reply_text(
                            "–í—ã–±–µ—Ä–∏—Ç–µ —Å–ø–æ—Å–æ–± –æ–ø–ª–∞—Ç—ã:",
                            reply_markup=get_payment_keyboard()
                        )

            if agentic_result.get("text"):
                response_text = agentic_result["text"]
            elif special_actions and not agentic_result.get("text"):
                typing_task.cancel()
                try:
                    await typing_task
                except asyncio.CancelledError:
                    pass
                session.add_message("assistant", "–ü–æ–∫–∞–∑–∞–ª –∑–∞–ø—Ä–æ—à–µ–Ω–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é", config.max_history_length)
                lead_manager.save_message(user.id, "assistant", "–ü–æ–∫–∞–∑–∞–ª –∑–∞–ø—Ä–æ—à–µ–Ω–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é")
                _run_voice_post_processing(user.id, transcription, session)
                return
        except Exception as e:
            logger.warning(f"Voice agentic loop failed, falling back to direct: {e}")

            from src.knowledge_base import SYSTEM_PROMPT
            from google import genai
            from google.genai import types

            gemini_client = genai.Client(api_key=config.gemini_api_key)

            history_text = ""
            for msg in session.get_history()[-6:]:
                role = "–ö–ª–∏–µ–Ω—Ç" if msg.get("role") == "user" else "–ê–ª–µ–∫—Å"
                parts = msg.get("parts", [])
                txt = parts[0].get("text", "") if parts else ""
                if txt and not txt.startswith("[–°–ò–°–¢–ï–ú–ù–´–ô") and not txt.startswith("[–ì–û–õ–û–°–û–í–û–ô"):
                    history_text += f"{role}: {txt}\n"

            context_addition = ""
            if client_context:
                context_addition = f"\n[–ö–û–ù–¢–ï–ö–°–¢]\n{client_context}\n"

            full_prompt = (
                f"{VOICE_CONTEXT_INSTRUCTION}\n"
                f"{context_addition}"
                f"–ò—Å—Ç–æ—Ä–∏—è –¥–∏–∞–ª–æ–≥–∞:\n{history_text}\n"
                f"–ö–ª–∏–µ–Ω—Ç —Å–∫–∞–∑–∞–ª –≥–æ–ª–æ—Å–æ–≤—ã–º: {transcription}\n\n"
                f"–û—Ç–≤–µ—Ç—å –∫–∞–∫ –∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç –ê–ª–µ–∫—Å. –ö–æ—Ä–æ—Ç–∫–æ, —Ä–∞–∑–≥–æ–≤–æ—Ä–Ω–æ, –¥–ª—è –æ–∑–≤—É—á–∫–∏."
            )

            response = await asyncio.to_thread(
                gemini_client.models.generate_content,
                model=config.model_name,
                contents=[full_prompt],
                config=types.GenerateContentConfig(
                    system_instruction=SYSTEM_PROMPT,
                    max_output_tokens=1000,
                    temperature=0.7
                )
            )

            if response.text:
                response_text = response.text

        if not response_text:
            response_text = "–ò–∑–≤–∏–Ω–∏—Ç–µ, –Ω–µ —É–¥–∞–ª–æ—Å—å —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –≤–æ–ø—Ä–æ—Å."

        session.add_message("assistant", response_text, config.max_history_length)
        lead_manager.save_message(user.id, "assistant", response_text)

        typing_task.cancel()
        try:
            await typing_task
        except asyncio.CancelledError:
            pass

        voice_sent = False
        if config.elevenlabs_api_key:
            try:
                await update.effective_chat.send_action(ChatAction.RECORD_VOICE)
                voice_response = await generate_voice_response(response_text)
                await update.message.reply_voice(voice=voice_response)
                voice_sent = True
                lead_manager.log_event("voice_reply_sent", user.id)
            except Exception as e:
                logger.error(f"ElevenLabs TTS error ({type(e).__name__}): {e}")

        dynamic_btns = get_dynamic_buttons(user.id, transcription, session.message_count)
        reply_markup = None
        if dynamic_btns:
            keyboard_rows = [[InlineKeyboardButton(text, callback_data=cb)] for text, cb in dynamic_btns[:3]]
            reply_markup = InlineKeyboardMarkup(keyboard_rows)

        text_summary = _make_text_summary(response_text)
        if voice_sent:
            summary_with_note = f"üëÜ –ì–æ–ª–æ—Å–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ\n\n{text_summary}"
            if reply_markup:
                await update.message.reply_text(summary_with_note, reply_markup=reply_markup)
            else:
                await update.message.reply_text(summary_with_note)
        else:
            if len(response_text) > 4096:
                chunks = [response_text[i:i+4096] for i in range(0, len(response_text), 4096)]
                for i, chunk in enumerate(chunks):
                    if i == len(chunks) - 1:
                        await update.message.reply_text(chunk, reply_markup=reply_markup)
                    else:
                        await update.message.reply_text(chunk)
            else:
                await update.message.reply_text(response_text, reply_markup=reply_markup)

        logger.info(f"User {user.id}: voice processed (emotion={client_emotion}, voice_reply={'yes' if voice_sent else 'no'}, voice_msg#{context.user_data.get('voice_message_count', 0)})")

        _run_voice_post_processing(user.id, transcription, session)

    except Exception as e:
        typing_task.cancel()
        logger.error(f"Voice processing error ({type(e).__name__}): {e}")
        await update.message.reply_text(
            "–ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –≥–æ–ª–æ—Å–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ. –ù–∞–ø–∏—à–∏—Ç–µ —Ç–µ–∫—Å—Ç–æ–º, –ø–æ–∂–∞–ª—É–π—Å—Ç–∞."
        )


def _run_voice_post_processing(user_id: int, transcription: str, session):
    from src.handlers.messages import auto_tag_lead, auto_score_lead, extract_insights_if_needed, summarize_if_needed

    auto_tag_lead(user_id, transcription)
    auto_score_lead(user_id, transcription)

    asyncio.create_task(
        extract_insights_if_needed(user_id, session)
    )
    asyncio.create_task(
        summarize_if_needed(user_id, session)
    )


async def photo_handler(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    user = update.effective_user
    user_id = user.id

    if context.user_data.get('broadcast_compose'):
        from src.security import is_admin
        if is_admin(user_id):
            context.user_data.pop('broadcast_compose', None)
            photo = update.message.photo[-1]
            context.user_data['broadcast_draft'] = {
                'type': 'photo',
                'file_id': photo.file_id,
                'caption': update.message.caption or '',
            }
            from src.broadcast import broadcast_manager
            counts = broadcast_manager.get_audience_counts()
            from src.handlers.utils import get_broadcast_audience_keyboard
            keyboard = get_broadcast_audience_keyboard(counts)
            caption_preview = f"\nüìù {update.message.caption}" if update.message.caption else ""
            await update.message.reply_text(
                f"üìã <b>–ü—Ä–µ–¥–ø—Ä–æ—Å–º–æ—Ç—Ä —Ä–∞—Å—Å—ã–ª–∫–∏:</b>\n\nüì∏ –§–æ—Ç–æ{caption_preview}\n\n<b>–í—ã–±–µ—Ä–∏—Ç–µ –∞—É–¥–∏—Ç–æ—Ä–∏—é:</b>",
                parse_mode="HTML",
                reply_markup=keyboard
            )
            return

    pending_review_type = context.user_data.get("pending_review_type")

    if pending_review_type != "text_photo":
        typing_task = asyncio.create_task(
            send_typing_action(update, duration=30.0)
        )
        try:
            photo = update.message.photo[-1] if update.message.photo else None
            if not photo:
                typing_task.cancel()
                return

            file = await context.bot.get_file(photo.file_id)
            photo_bytes = await file.download_as_bytearray()

            caption = update.message.caption or ""

            session = session_manager.get_session(
                user_id=user.id,
                username=user.username,
                first_name=user.first_name
            )

            user_text = caption if caption else "–ö–ª–∏–µ–Ω—Ç –æ—Ç–ø—Ä–∞–≤–∏–ª –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —á—Ç–æ –Ω–∞ –Ω—ë–º –∏ –æ—Ç–≤–µ—Ç—å –∫–∞–∫ –∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç –ê–ª–µ–∫—Å –∏–∑ WEB4TG Studio. –ï—Å–ª–∏ —ç—Ç–æ —Å–∫—Ä–∏–Ω—à–æ—Ç –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è –∏–ª–∏ –¥–∏–∑–∞–π–Ω ‚Äî –æ—Ü–µ–Ω–∏ –∏ –ø—Ä–µ–¥–ª–æ–∂–∏ —É–ª—É—á—à–µ–Ω–∏—è. –ï—Å–ª–∏ —ç—Ç–æ –¢–ó –∏–ª–∏ —Å—Ö–µ–º–∞ ‚Äî –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –∏ –¥–∞–π —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏."

            session.add_message("user", f"[–§–æ—Ç–æ]{f': {caption}' if caption else ''}", config.max_history_length)
            lead_manager.save_message(user.id, "user", f"[–§–æ—Ç–æ]{f': {caption}' if caption else ''}")
            lead_manager.log_event("photo_analysis", user.id)
            lead_manager.update_activity(user.id)

            from src.context_builder import build_full_context, get_dynamic_buttons
            client_context = build_full_context(user.id, user_text, user.username, user.first_name)

            from google import genai
            from google.genai import types
            from src.knowledge_base import SYSTEM_PROMPT

            gemini_client = genai.Client(api_key=config.gemini_api_key)

            image_part = types.Part.from_bytes(data=bytes(photo_bytes), mime_type="image/jpeg")
            text_part = types.Part(text=user_text)

            context_parts = []
            if client_context:
                context_parts.append(types.Part(text=f"[–°–ò–°–¢–ï–ú–ù–´–ô –ö–û–ù–¢–ï–ö–°–¢ ‚Äî –Ω–µ –ø–æ–∫–∞–∑—ã–≤–∞–π –∫–ª–∏–µ–Ω—Ç—É]\n{client_context}"))

            all_parts = context_parts + [image_part, text_part]

            response = await asyncio.to_thread(
                gemini_client.models.generate_content,
                model=config.model_name,
                contents=all_parts,
                config=types.GenerateContentConfig(
                    system_instruction=SYSTEM_PROMPT,
                    max_output_tokens=1500,
                    temperature=0.7
                )
            )

            typing_task.cancel()

            if response.text:
                session.add_message("assistant", response.text, config.max_history_length)
                lead_manager.save_message(user.id, "assistant", response.text)

                dynamic_btns = get_dynamic_buttons(user.id, user_text, session.message_count)
                reply_markup = None
                if dynamic_btns:
                    keyboard_rows = [[InlineKeyboardButton(text, callback_data=cb)] for text, cb in dynamic_btns[:3]]
                    reply_markup = InlineKeyboardMarkup(keyboard_rows)

                await update.message.reply_text(response.text, parse_mode="Markdown", reply_markup=reply_markup)

                from src.handlers.messages import auto_tag_lead, auto_score_lead
                auto_tag_lead(user.id, user_text)
                auto_score_lead(user.id, user_text)
            else:
                await update.message.reply_text("–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –æ–ø–∏—Å–∞—Ç—å —Å–ª–æ–≤–∞–º–∏ —á—Ç–æ –≤–∞–º –Ω—É–∂–Ω–æ.")
        except Exception as e:
            typing_task.cancel()
            logger.error(f"Photo analysis error: {e}")
            await update.message.reply_text(
                "–ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å —Ñ–æ—Ç–æ. –û–ø–∏—à–∏—Ç–µ —Å–ª–æ–≤–∞–º–∏ —á—Ç–æ –≤–∞–º –Ω—É–∂–Ω–æ, —è –ø–æ–º–æ–≥—É!"
            )
        return

    photo = update.message.photo[-1] if update.message.photo else None
    if not photo:
        return

    file_id = photo.file_id
    caption = update.message.caption or ""

    try:
        review_id = loyalty_system.submit_review(
            user_id=user_id,
            review_type="text_photo",
            content_url=f"[PHOTO] file_id: {file_id}",
            comment=caption if caption else None
        )

        if review_id:
            context.user_data.pop("pending_review_type", None)

            from src.loyalty import REVIEW_REWARDS
            coins = REVIEW_REWARDS.get("text_photo", 200)

            await update.message.reply_text(
                f"""‚úÖ <b>–û—Ç–∑—ã–≤ —Å —Ñ–æ—Ç–æ –ø—Ä–∏–Ω—è—Ç!</b>

–°–ø–∞—Å–∏–±–æ –∑–∞ –≤–∞—à –æ—Ç–∑—ã–≤! –ü–æ—Å–ª–µ –º–æ–¥–µ—Ä–∞—Ü–∏–∏ –≤—ã –ø–æ–ª—É—á–∏—Ç–µ <b>{coins} –º–æ–Ω–µ—Ç</b>.

–û–±—ã—á–Ω–æ –º–æ–¥–µ—Ä–∞—Ü–∏—è –∑–∞–Ω–∏–º–∞–µ—Ç –¥–æ 24 —á–∞—Å–æ–≤.""",
                parse_mode="HTML",
                reply_markup=get_loyalty_menu_keyboard()
            )

            if MANAGER_CHAT_ID:
                try:
                    manager_text = f"""üì∏ <b>–ù–æ–≤—ã–π —Ç–µ–∫—Å—Ç–æ–≤—ã–π –æ—Ç–∑—ã–≤ —Å —Ñ–æ—Ç–æ!</b>

üë§ {user.first_name or '–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å'} (@{user.username or 'no_username'})
üÜî ID: {user_id}
üí¨ –¢–µ–∫—Å—Ç: {caption or '(–±–µ–∑ –ø–æ–¥–ø–∏—Å–∏)'}"""

                    await context.bot.send_message(
                        chat_id=MANAGER_CHAT_ID,
                        text=manager_text,
                        parse_mode="HTML"
                    )
                    await context.bot.forward_message(
                        chat_id=MANAGER_CHAT_ID,
                        from_chat_id=update.effective_chat.id,
                        message_id=update.message.message_id
                    )
                except Exception as e:
                    logger.warning(f"Failed to notify manager about photo review: {e}")
        else:
            await update.message.reply_text(
                "–í—ã —É–∂–µ –æ—Ç–ø—Ä–∞–≤–ª—è–ª–∏ –æ—Ç–∑—ã–≤ —ç—Ç–æ–≥–æ —Ç–∏–ø–∞ –∏–ª–∏ –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞.",
                reply_markup=get_loyalty_menu_keyboard()
            )
            context.user_data.pop("pending_review_type", None)

    except Exception as e:
        logger.error(f"Error processing photo review: {e}")
        await update.message.reply_text(
            "–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ.",
            reply_markup=get_loyalty_menu_keyboard()
        )
        context.user_data.pop("pending_review_type", None)


async def video_handler(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    user = update.effective_user
    user_id = user.id

    if context.user_data.get('broadcast_compose'):
        from src.security import is_admin
        if is_admin(user.id):
            context.user_data.pop('broadcast_compose', None)
            video = update.message.video or update.message.video_note
            context.user_data['broadcast_draft'] = {
                'type': 'video',
                'file_id': video.file_id,
                'caption': update.message.caption or '',
            }
            from src.broadcast import broadcast_manager
            counts = broadcast_manager.get_audience_counts()
            from src.handlers.utils import get_broadcast_audience_keyboard
            keyboard = get_broadcast_audience_keyboard(counts)
            caption_preview = f"\nüìù {update.message.caption}" if update.message.caption else ""
            await update.message.reply_text(
                f"üìã <b>–ü—Ä–µ–¥–ø—Ä–æ—Å–º–æ—Ç—Ä —Ä–∞—Å—Å—ã–ª–∫–∏:</b>\n\nüé¨ –í–∏–¥–µ–æ{caption_preview}\n\n<b>–í—ã–±–µ—Ä–∏—Ç–µ –∞—É–¥–∏—Ç–æ—Ä–∏—é:</b>",
                parse_mode="HTML",
                reply_markup=keyboard
            )
            return

    pending_review_type = context.user_data.get("pending_review_type")

    if pending_review_type != "video":
        await update.message.reply_text(
            "–ï—Å–ª–∏ —Ö–æ—Ç–∏—Ç–µ –æ—Å—Ç–∞–≤–∏—Ç—å –≤–∏–¥–µ–æ-–æ—Ç–∑—ã–≤, –Ω–∞–∂–º–∏—Ç–µ /bonus ‚Üí –û—Ç–∑—ã–≤—ã –∏ –±–æ–Ω—É—Å—ã ‚Üí –í–∏–¥–µ–æ-–æ—Ç–∑—ã–≤"
        )
        return

    video = update.message.video or update.message.video_note
    if not video:
        return

    file_id = video.file_id

    try:
        review = loyalty_system.submit_review(
            user_id=user_id,
            review_type="video",
            content=f"[VIDEO] file_id: {file_id}"
        )

        if review:
            context.user_data.pop("pending_review_type", None)

            from src.loyalty import REVIEW_REWARDS
            coins = REVIEW_REWARDS.get("video", 500)

            await update.message.reply_text(
                f"""‚úÖ <b>–í–∏–¥–µ–æ-–æ—Ç–∑—ã–≤ –ø—Ä–∏–Ω—è—Ç!</b>

–°–ø–∞—Å–∏–±–æ –∑–∞ –≤–∞—à –æ—Ç–∑—ã–≤! –ü–æ—Å–ª–µ –º–æ–¥–µ—Ä–∞—Ü–∏–∏ –≤—ã –ø–æ–ª—É—á–∏—Ç–µ <b>{coins} –º–æ–Ω–µ—Ç</b>.

–û–±—ã—á–Ω–æ –º–æ–¥–µ—Ä–∞—Ü–∏—è –∑–∞–Ω–∏–º–∞–µ—Ç –¥–æ 24 —á–∞—Å–æ–≤.""",
                parse_mode="HTML",
                reply_markup=get_loyalty_menu_keyboard()
            )

            if MANAGER_CHAT_ID:
                try:
                    manager_text = f"""üé¨ <b>–ù–æ–≤—ã–π –≤–∏–¥–µ–æ-–æ—Ç–∑—ã–≤!</b>

üë§ {user.first_name or '–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å'} (@{user.username or 'no_username'})
üÜî ID: {user_id}"""

                    await context.bot.send_message(
                        chat_id=MANAGER_CHAT_ID,
                        text=manager_text,
                        parse_mode="HTML"
                    )
                    await context.bot.forward_message(
                        chat_id=MANAGER_CHAT_ID,
                        from_chat_id=update.effective_chat.id,
                        message_id=update.message.message_id
                    )
                except Exception as e:
                    logger.warning(f"Failed to notify manager about video review: {e}")
        else:
            await update.message.reply_text(
                "–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –æ—Ç–∑—ã–≤. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ.",
                reply_markup=get_loyalty_menu_keyboard()
            )
            context.user_data.pop("pending_review_type", None)

    except Exception as e:
        logger.error(f"Error processing video review: {e}")
        await update.message.reply_text(
            "–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ.",
            reply_markup=get_loyalty_menu_keyboard()
        )
        context.user_data.pop("pending_review_type", None)
