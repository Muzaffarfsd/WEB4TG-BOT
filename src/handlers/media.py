import asyncio
import logging
import re
import hashlib
import time
from telegram import Update, InlineKeyboardMarkup, InlineKeyboardButton
from telegram.ext import ContextTypes
from telegram.constants import ChatAction

from src.session import session_manager
from src.config import config
from src.leads import lead_manager
from src.keyboards import get_loyalty_menu_keyboard

from src.handlers.utils import (
    send_typing_action, apply_stress_marks, expand_abbreviations,
    numbers_to_words, naturalize_speech,
    loyalty_system, MANAGER_CHAT_ID
)

logger = logging.getLogger(__name__)

_elevenlabs_client = None
_elevenlabs_async_client = None
_voice_cache = {}

STREAMING_LATENCY_OPTIMIZATION = 3
SHORT_TEXT_THRESHOLD = 200
SHORT_TEXT_FORMAT = "mp3_22050_32"
LONG_TEXT_FORMAT = "mp3_44100_128"


def _get_elevenlabs_client():
    global _elevenlabs_client
    if _elevenlabs_client is None and config.elevenlabs_api_key:
        from elevenlabs import ElevenLabs
        _elevenlabs_client = ElevenLabs(api_key=config.elevenlabs_api_key)
    return _elevenlabs_client


def _get_async_elevenlabs_client():
    global _elevenlabs_async_client
    if _elevenlabs_async_client is None and config.elevenlabs_api_key:
        try:
            from elevenlabs import AsyncElevenLabs
            _elevenlabs_async_client = AsyncElevenLabs(api_key=config.elevenlabs_api_key)
        except ImportError:
            logger.warning("AsyncElevenLabs not available, will use sync client with threading")
            return None
    return _elevenlabs_async_client


VOICE_ENHANCE_PROMPT = """–¢—ã ‚Äî Enhance-–¥–≤–∏–∂–æ–∫ –¥–ª—è ElevenLabs v3. –¢–≤–æ—è –∑–∞–¥–∞—á–∞: –ø–æ–ª—É—á–∏—Ç—å —á–∏—Å—Ç—ã–π —Ç–µ–∫—Å—Ç –∏ –¢–û–ß–ï–ß–ù–û —Ä–∞—Å—Å—Ç–∞–≤–∏—Ç—å —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ —Ç–µ–≥–∏ —Ç–∞–º, –≥–¥–µ –æ–Ω–∏ —É—Å–∏–ª—è—Ç —Ä–µ—á—å.

–ü–†–ò–ù–¶–ò–ü: –ö–∞–∫ –∫–Ω–æ–ø–∫–∞ Enhance –≤ ElevenLabs ‚Äî –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—à—å —Å–º—ã—Å–ª –∫–∞–∂–¥–æ–π —Ñ—Ä–∞–∑—ã –∏ –¥–æ–±–∞–≤–ª—è–µ—à—å –¢–û–õ–¨–ö–û –ø–æ–¥—Ö–æ–¥—è—â–∏–π —Ç–µ–≥. –ù–µ –≤—Å–µ —Ç–µ–≥–∏ –Ω—É–∂–Ω—ã –≤ –∫–∞–∂–¥–æ–º —Ç–µ–∫—Å—Ç–µ. –ò—Å–ø–æ–ª—å–∑—É–π —Ç–æ–ª—å–∫–æ —Ç–æ, —á—Ç–æ —Ä–µ–∞–ª—å–Ω–æ —É–ª—É—á—à–∏—Ç –∑–≤—É—á–∞–Ω–∏–µ.

–®–ê–ì 1 ‚Äî –ê–ù–ê–õ–ò–ó: –ü—Ä–æ—á–∏—Ç–∞–π —Ç–µ–∫—Å—Ç. –û–ø—Ä–µ–¥–µ–ª–∏ —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ —Ç–æ—á–∫–∏:
- –ì–¥–µ —Ä–∞–¥–æ—Å—Ç—å? –ì–¥–µ –≥—Ä—É—Å—Ç—å? –ì–¥–µ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å? –ì–¥–µ –∏–Ω—Ç—Ä–∏–≥–∞?
- –ö–∞–∫–æ–π –æ–±—â–∏–π —Ç–æ–Ω: –¥–µ–ª–æ–≤–æ–π, –¥—Ä—É–∂–µ—Å–∫–∏–π, —Ç—ë–ø–ª—ã–π, —Å–µ—Ä—å—ë–∑–Ω—ã–π?
- –ï—Å—Ç—å –ª–∏ —Å–º–µ–Ω–∞ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏—è –≤ —Ç–µ–∫—Å—Ç–µ?

–®–ê–ì 2 ‚Äî –†–ê–°–°–¢–ê–ù–û–í–ö–ê –¢–ï–ì–û–í (—Ç–æ–ª—å–∫–æ –Ω—É–∂–Ω—ã–µ, –Ω–µ –≤—Å–µ –ø–æ–¥—Ä—è–¥):
–¢–µ–≥–∏ –≤ [–∫–≤–∞–¥—Ä–∞—Ç–Ω—ã—Ö —Å–∫–æ–±–∫–∞—Ö] –ø–µ—Ä–µ–¥ —Ñ—Ä–∞–∑–æ–π. –ù–ï –ø—Ä–æ–∏–∑–Ω–æ—Å—è—Ç—Å—è. –£–ø—Ä–∞–≤–ª—è—é—Ç —Ç–æ–Ω–æ–º –≥–æ–ª–æ—Å–∞.

–î–û–°–¢–£–ü–ù–´–ï –¢–ï–ì–ò:
–≠–º–æ—Ü–∏–∏: [happy] [excited] [sad] [nervous] [frustrated] [sorrowful] [curious] [mischievous]
–†–µ–∞–∫—Ü–∏–∏: [laughs] [giggles] [sighs] [gasps] [clears throat]
–°—Ç–∏–ª–∏: [whispers] [cheerfully] [flatly] [deadpan] [playfully] [hesitant] [resigned tone]
–ù–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ: [warm] [friendly] [calm] [confident]

–ö–û–ì–î–ê –ö–ê–ö–û–ô –¢–ï–ì (–≤—ã–±–∏—Ä–∞–π –ø–æ —Å–º—ã—Å–ª—É, –∞ –Ω–µ –ø–æ —Å–ø–∏—Å–∫—É):
- –ü—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏–µ, —Ç—ë–ø–ª—ã–µ —Å–ª–æ–≤–∞ ‚Üí [warm] –∏–ª–∏ [friendly]
- –≠–∫—Å–ø–µ—Ä—Ç–Ω–æ–µ –º–Ω–µ–Ω–∏–µ, —Ñ–∞–∫—Ç—ã ‚Üí [confident]
- –•–æ—Ä–æ—à–∞—è –Ω–æ–≤–æ—Å—Ç—å, —Ä–µ–∑—É–ª—å—Ç–∞—Ç ‚Üí [excited] –∏–ª–∏ [happy]
- –í–ø–µ—á–∞—Ç–ª—è—é—â–∞—è —Ü–∏—Ñ—Ä–∞ ‚Üí [gasps] –ø–µ—Ä–µ–¥ –Ω–µ–π
- –í–æ–ø—Ä–æ—Å —Å –∏–Ω—Ç–µ—Ä–µ—Å–æ–º ‚Üí [curious]
- –°–æ—á—É–≤—Å—Ç–≤–∏–µ, –ø—Ä–æ–±–ª–µ–º–∞ ‚Üí [calm] –∏–ª–∏ [sighs]
- –°–µ–∫—Ä–µ—Ç, —Å–∫–∏–¥–∫–∞, –±–æ–Ω—É—Å ‚Üí [whispers] –∏–ª–∏ [mischievous]
- –õ—ë–≥–∫–∏–π —é–º–æ—Ä ‚Üí [giggles] –∏–ª–∏ [playfully]
- –ü—Ä–∏–∑—ã–≤ –∫ –¥–µ–π—Å—Ç–≤–∏—é ‚Üí [cheerfully]
- –†–∞–∑–º—ã—à–ª–µ–Ω–∏–µ, —Å–æ–º–Ω–µ–Ω–∏–µ ‚Üí [hesitant]
- –ö–æ–Ω—Ç—Ä–∞—Å—Ç (–±—ã–ª–æ –ø–ª–æ—Ö–æ ‚Üí —Å—Ç–∞–ª–æ —Ö–æ—Ä–æ—à–æ) ‚Üí [flatly] –ø–æ—Ç–æ–º [excited]
- –°–µ—Ä—å—ë–∑–Ω—ã–µ —Ü–∏—Ñ—Ä—ã –±–µ–∑ —ç–º–æ—Ü–∏–π ‚Üí [deadpan]

–ü–†–ê–í–ò–õ–ê:
1. –ú–∞–∫—Å–∏–º—É–º 2-4 —Ç–µ–≥–∞ –Ω–∞ –æ—Ç–≤–µ—Ç –¥–æ 300 —Å–∏–º–≤–æ–ª–æ–≤, 3-5 –Ω–∞ –¥–ª–∏–Ω–Ω—ã–π
2. –ù–ï —Å—Ç–∞–≤—å —Ç–µ–≥ –Ω–∞ –∫–∞–∂–¥–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ ‚Äî —Ç–æ–ª—å–∫–æ –Ω–∞ —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ —Ç–æ—á–∫–∏
3. –ü–µ—Ä–≤—ã–π —Ç–µ–≥ ‚Äî –∑–∞–¥–∞—ë—Ç –æ–±—â–∏–π —Ç–æ–Ω (–æ–±—ã—á–Ω–æ [warm], [friendly] –∏–ª–∏ [confident])
4. –ú–µ–∂–¥—É —Ç–µ–≥–∞–º–∏ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å 1-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –±–µ–∑ —Ç–µ–≥–æ–≤ ‚Äî —ç—Ç–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ
5. –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç –Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π/—Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏–π ‚Äî —Ö–≤–∞—Ç–∏—Ç 1-2 —Ç–µ–≥–æ–≤
6. –ù–ï –ú–ï–ù–Ø–ô —Å–∞–º —Ç–µ–∫—Å—Ç ‚Äî —Ç–æ–ª—å–∫–æ –¥–æ–±–∞–≤–ª—è–π —Ç–µ–≥–∏ –ø–µ—Ä–µ–¥ —Ñ—Ä–∞–∑–∞–º–∏
7. –°–æ—Ö—Ä–∞–Ω—è–π "..." –ø–∞—É–∑—ã –∏ " ‚Äî " —Ç–∏—Ä–µ ‚Äî –æ–Ω–∏ –≤–∞–∂–Ω—ã –¥–ª—è —Ä–∏—Ç–º–∞
8. –£–±–µ—Ä–∏ markdown (**, *, #, ‚Ä¢, `) –∏ emoji, –∑–∞–º–µ–Ω–∏ –ø–µ—Ä–µ–Ω–æ—Å—ã –Ω–∞ ". "
9. –ß–∏—Å–ª–∞ –æ—Å—Ç–∞–≤—å –∫–∞–∫ –µ—Å—Ç—å

–í–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û —Ç–µ–∫—Å—Ç —Å —Ä–∞—Å—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–º–∏ —Ç–µ–≥–∞–º–∏. –ë–µ–∑ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤.

–¢–µ–∫—Å—Ç:
"""

VOICE_CONTEXT_INSTRUCTION = """
[–ì–û–õ–û–°–û–í–û–ô –§–û–†–ú–ê–¢] –ö–ª–∏–µ–Ω—Ç –æ–±—â–∞–µ—Ç—Å—è –≥–æ–ª–æ—Å–æ–≤—ã–º–∏. –¢–≤–æ–π –æ—Ç–≤–µ—Ç –û–ó–í–£–ß–ê–¢ ‚Äî –ø–∏—à–∏ –ö–ê–ö –ì–û–í–û–†–ò–®–¨ –≤—Å–ª—É—Ö.
–≠–º–æ—Ü–∏–∏ –∏ –∏–Ω—Ç–æ–Ω–∞—Ü–∏–∏ –±—É–¥—É—Ç –¥–æ–±–∞–≤–ª–µ–Ω—ã –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ ‚Äî —Ç–µ–±–µ –ù–ï –Ω—É–∂–Ω–æ —Å—Ç–∞–≤–∏—Ç—å —Ç–µ–≥–∏ –≤—Ä–æ–¥–µ [warm] –∏–ª–∏ [excited]. –ü—Ä–æ—Å—Ç–æ –ø–∏—à–∏ –∂–∏–≤–æ–π —Ç–µ–∫—Å—Ç, –æ—Å—Ç–∞–ª—å–Ω–æ–µ —Å–¥–µ–ª–∞–µ—Ç —Å–∏—Å—Ç–µ–º–∞.

–ì–õ–ê–í–ù–û–ï ‚Äî —Ç—ã –∂–∏–≤–æ–π –∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç –ê–ª–µ–∫—Å, –∞ –Ω–µ —Ç–µ–∫—Å—Ç–æ–≤—ã–π –±–æ—Ç.

–î–õ–ò–ù–ê –û–¢–í–ï–¢–ê (–ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û):
- –°–¢–†–û–ì–û 300-500 —Å–∏–º–≤–æ–ª–æ–≤ (20-30 —Å–µ–∫—É–Ω–¥). –≠—Ç–æ –ú–ê–ö–°–ò–ú–£–ú.
- –û–¥–Ω–∞ –∫–ª—é—á–µ–≤–∞—è –º—ã—Å–ª—å –Ω–∞ –æ—Ç–≤–µ—Ç. –ù–µ –ø—ã—Ç–∞–π—Å—è —É–º–µ—Å—Ç–∏—Ç—å –≤—Å—ë.
- –ï—Å–ª–∏ —Ç–µ–º–∞ —Å–ª–æ–∂–Ω–∞—è ‚Äî —Å–∫–∞–∂–∏ "–ú–æ–≥—É –ø–æ–¥—Ä–æ–±–Ω–µ–µ —Ä–∞—Å—Å–∫–∞–∑–∞—Ç—å, –µ—Å–ª–∏ –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ".

–°–¢–ò–õ–¨ –†–ï–ß–ò:
- –ù–∏–∫–∞–∫–æ–≥–æ markdown, emoji, —Å–ø–∏—Å–∫–æ–≤ —Å —Ç–∏—Ä–µ –∏–ª–∏ –∑–≤—ë–∑–¥–æ—á–∫–∞–º–∏
- –ì–æ–≤–æ—Ä–∏ –∫–∞–∫ –≤ –∂–∏–∑–Ω–∏: "–ù—É —Å–º–æ—Ç—Ä–∏—Ç–µ, —Ç—É—Ç –≤–æ—Ç –∫–∞–∫–∞—è –∏—Å—Ç–æ—Ä–∏—è..."
- –ß–∏—Å–ª–∞ ‚Äî —Å–ª–æ–≤–∞–º–∏: "—Å—Ç–æ –ø—è—Ç—å–¥–µ—Å—è—Ç —Ç—ã—Å—è—á"
- –ê–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã —Ä–∞—Å–∫—Ä—ã–≤–∞–π: "–≤–æ–∑–≤—Ä–∞—Ç –∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–π" –≤–º–µ—Å—Ç–æ "ROI"

–ü–†–ò–Å–ú–´ –ñ–ò–í–û–ì–û –ß–ï–õ–û–í–ï–ö–ê (1-2 –∑–∞ –æ—Ç–≤–µ—Ç):
- –î—É–º–∞–π –≤—Å–ª—É—Ö: "–•–º, –¥–∞–≤–∞–π—Ç–µ –ø—Ä–∏–∫–∏–Ω–µ–º..."
- –ü–µ—Ä–µ—Ö–æ–¥—ã: "–ö—Å—Ç–∞—Ç–∏,", "–ò –∑–Ω–∞–µ—Ç–µ —á—Ç–æ ‚Äî"
- –≠–º–ø–∞—Ç–∏—è: "–î–∞, –ø–æ–Ω–∏–º–∞—é,", "–õ–æ–≥–∏—á–Ω—ã–π –≤–æ–ø—Ä–æ—Å,"
- –ü–∞—É–∑—ã —á–µ—Ä–µ–∑ "..." –∏ " ‚Äî " –¥–ª—è –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –¥—ã—Ö–∞–Ω–∏—è
- –ß–µ—Ä–µ–¥—É–π –¥–ª–∏–Ω–Ω—ã–µ –∏ –∫–æ—Ä–æ—Ç–∫–∏–µ —Ñ—Ä–∞–∑—ã: "–ú–∞–≥–∞–∑–∏–Ω –∑–∞ —Å—Ç–æ –ø—è—Ç—å–¥–µ—Å—è—Ç. –°–µ–º—å-–¥–µ—Å—è—Ç—å –¥–Ω–µ–π. –ì–æ—Ç–æ–≤–æ."

–ß–ï–ì–û –ò–ó–ë–ï–ì–ê–¢–¨:
- –®–∞–±–ª–æ–Ω–Ω—ã—Ö —Ñ—Ä–∞–∑: "–†–∞–¥ –ø–æ–º–æ—á—å!", "–û—Ç–ª–∏—á–Ω—ã–π –≤—ã–±–æ—Ä!"
- –°–ø–∏—Å–∫–æ–≤ (1. 2. 3.) ‚Äî —ç—Ç–æ —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–æ—Ä–º–∞—Ç
- –§–æ—Ä–º–∞–ª—å–Ω—ã—Ö –æ–±–æ—Ä–æ—Ç–æ–≤: "–í —Ä–∞–º–∫–∞—Ö –Ω–∞—à–µ–≥–æ —Å–æ—Ç—Ä—É–¥–Ω–∏—á–µ—Å—Ç–≤–∞..."
"""


async def enhance_voice_text(text: str) -> str:
    existing_tags = re.findall(r'\[\w[\w\s]*?\]', text)
    if len(existing_tags) >= 2:
        logger.debug(f"Text already has {len(existing_tags)} tags, skipping enhance")
        return text

    if len(text) < 50:
        return _auto_enhance_short(text)

    from google.genai import types
    from src.config import get_gemini_client

    client = get_gemini_client()

    try:
        response = await asyncio.to_thread(
            client.models.generate_content,
            model=config.model_name,
            contents=[VOICE_ENHANCE_PROMPT + text],
            config=types.GenerateContentConfig(
                max_output_tokens=2000,
                temperature=0.2
            )
        )

        if response.text:
            enhanced = response.text.strip()
            enhanced = enhanced.strip('"').strip("'")
            enhanced = re.sub(r'\*+', '', enhanced)
            enhanced = re.sub(r'#+\s*', '', enhanced)

            enhanced_tags = re.findall(r'\[\w[\w\s]*?\]', enhanced)
            valid_tags = {
                '[happy]', '[excited]', '[sad]', '[angry]', '[nervous]',
                '[frustrated]', '[sorrowful]', '[curious]', '[mischievous]',
                '[laughs]', '[giggles]', '[sighs]', '[gasps]', '[gulps]',
                '[clears throat]', '[whispers]', '[shouts]', '[cheerfully]',
                '[flatly]', '[deadpan]', '[playfully]', '[sarcastically]',
                '[hesitant]', '[resigned tone]', '[warm]', '[friendly]',
                '[calm]', '[confident]'
            }
            bad_tags = [t for t in enhanced_tags if t not in valid_tags]
            for bt in bad_tags:
                enhanced = enhanced.replace(bt, '')

            tag_count = len(re.findall(r'\[\w[\w\s]*?\]', enhanced))
            text_len = len(re.sub(r'\[\w[\w\s]*?\]\s*', '', enhanced))
            max_tags = 3 if text_len < 300 else 5
            if tag_count > max_tags:
                logger.debug(f"Enhance produced {tag_count} tags, capping at {max_tags}")
                found = 0
                result = []
                i = 0
                while i < len(enhanced):
                    match = re.match(r'\[(\w[\w\s]*?)\]', enhanced[i:])
                    if match:
                        found += 1
                        if found <= max_tags:
                            result.append(match.group(0))
                        i += len(match.group(0))
                    else:
                        result.append(enhanced[i])
                        i += 1
                enhanced = ''.join(result)

            enhanced = re.sub(r'\s{2,}', ' ', enhanced).strip()
            final_tag_count = len(re.findall(r'\[\w[\w\s]*?\]', enhanced))
            logger.info(f"Enhance: {len(existing_tags)} -> {final_tag_count} tags")
            return enhanced
    except Exception as e:
        logger.error(f"Voice enhance error: {e}")

    return _auto_enhance_short(text)


def _auto_enhance_short(text: str) -> str:
    lower = text.lower()

    if any(w in lower for w in ['–ø—Ä–∏–≤–µ—Ç', '–∑–¥—Ä–∞–≤—Å—Ç–≤—É–π', '–¥–æ–±—Ä—ã–π', '–¥–æ–±—Ä–æ–µ', '–¥–æ–±—Ä–æ–π']):
        if not text.startswith('['):
            return f'[warm] {text}'

    if any(w in lower for w in ['–æ—Ç–ª–∏—á–Ω–æ', '–∑–∞–º–µ—á–∞—Ç–µ–ª—å–Ω–æ', '–∫—Ä—É—Ç–æ', '—Å—É–ø–µ—Ä', '—É—Ä–∞', '–∫–ª–∞—Å—Å']):
        if not text.startswith('['):
            return f'[excited] {text}'

    if any(w in lower for w in ['–ø–æ–Ω–∏–º–∞—é', '—Å–æ—á—É–≤—Å—Ç–≤', '–∫ —Å–æ–∂–∞–ª–µ–Ω–∏—é', '–∂–∞–ª—å']):
        if not text.startswith('['):
            return f'[calm] {text}'

    if any(w in lower for w in ['—Å—Ç–æ–∏–º–æ—Å—Ç—å', '—Ü–µ–Ω–∞', '—Ä—É–±–ª–µ–π', '—Ç—ã—Å—è—á', '–≥–∞—Ä–∞–Ω—Ç–∏—è']):
        if not text.startswith('['):
            return f'[confident] {text}'

    if not text.startswith('['):
        return f'[friendly] {text}'

    return text


def _clean_text_for_voice(text: str) -> str:
    clean = re.sub(r'\.{3,}', '...', text)
    _ellipsis_placeholder = '\x00ELLIPSIS\x00'
    clean = clean.replace('...', _ellipsis_placeholder)
    clean = clean.replace("**", "").replace("*", "").replace("#", "")
    clean = clean.replace("`", "").replace("_", " ")
    clean = clean.replace("‚Ä¢", ",").replace("‚Äî", " ‚Äî ")
    clean = clean.replace("\n\n", ". ").replace("\n", ", ")
    clean = re.sub(r'[\U0001F600-\U0001F64F\U0001F300-\U0001F5FF\U0001F680-\U0001F6FF\U0001F1E0-\U0001F1FF\U00002702-\U000027B0\U000024C2-\U0001F251\U0001f926-\U0001f937\U00010000-\U0010ffff\u2600-\u2B55\u200d\u23cf\u23e9\u231a\ufe0f\u3030\u2066\u2069]+', '', clean)
    clean = re.sub(r'\s{2,}', ' ', clean)
    clean = re.sub(r'[,\.]{2,}', '.', clean)
    clean = clean.replace(_ellipsis_placeholder, '...')
    return clean.strip()


VOICE_PROFILES = {
    "greeting": {"stability": 0.35, "similarity_boost": 0.8, "style": 1.0},
    "empathy": {"stability": 0.5, "similarity_boost": 0.85, "style": 1.0},
    "factual": {"stability": 1.0, "similarity_boost": 0.8, "style": 0.5},
    "excited": {"stability": 0.15, "similarity_boost": 0.75, "style": 1.0},
    "whisper": {"stability": 0.6, "similarity_boost": 0.9, "style": 0.8},
    "playful": {"stability": 0.25, "similarity_boost": 0.75, "style": 1.0},
    "default": {"stability": 0.5, "similarity_boost": 0.8, "style": 1.0},
}


def _detect_voice_profile(text: str) -> dict:
    lower = text.lower()
    tags = re.findall(r'\[(\w[\w\s]*?)\]', text)
    tag_set = {t.lower() for t in tags}

    if tag_set & {'whispers', 'mischievous'}:
        return VOICE_PROFILES["whisper"]
    if tag_set & {'excited', 'happy', 'gasps', 'cheerfully'}:
        return VOICE_PROFILES["excited"]
    if tag_set & {'warm', 'friendly'} and any(w in lower for w in ['–ø—Ä–∏–≤–µ—Ç', '–∑–¥—Ä–∞–≤—Å—Ç–≤—É–π', '–¥–æ–±—Ä–æ –ø–æ–∂–∞–ª–æ–≤', '—Ä–∞–¥ –≤–∞—Å', '–∑–Ω–∞–∫–æ–º—Å—Ç–≤']):
        return VOICE_PROFILES["greeting"]
    if tag_set & {'playfully', 'giggles', 'laughs'}:
        return VOICE_PROFILES["playful"]
    if tag_set & {'calm', 'sorrowful', 'sighs', 'sad'}:
        return VOICE_PROFILES["empathy"]
    if tag_set & {'confident', 'deadpan', 'flatly'}:
        return VOICE_PROFILES["factual"]

    if any(w in lower for w in ['–ø—Ä–∏–≤–µ—Ç', '–∑–¥—Ä–∞–≤—Å—Ç–≤—É–π', '–¥–æ–±—Ä–æ –ø–æ–∂–∞–ª–æ–≤', '—Ä–∞–¥ –≤–∞—Å', '–∑–Ω–∞–∫–æ–º—Å—Ç–≤']):
        return VOICE_PROFILES["greeting"]
    if any(w in lower for w in ['–ø–æ–Ω–∏–º–∞—é', '—Å–æ—á—É–≤—Å—Ç–≤', '–Ω–µ–ø—Ä–æ—Å—Ç–æ', '–∫ —Å–æ–∂–∞–ª–µ–Ω–∏—é', '–∏–∑–≤–∏–Ω', '–∂–∞–ª—å', '–±—ã–≤–∞–µ—Ç']):
        return VOICE_PROFILES["empathy"]
    if any(w in lower for w in ['—Å—Ç–æ–∏–º–æ—Å—Ç—å', '—Ü–µ–Ω–∞', '—Ä—É–±–ª–µ–π', '—Ç—ã—Å—è—á', '–ø—Ä–æ—Ü–µ–Ω—Ç', '—Å—Ä–æ–∫', '–≥–∞—Ä–∞–Ω—Ç–∏—è', '–¥–æ–≥–æ–≤–æ—Ä']):
        return VOICE_PROFILES["factual"]
    if any(w in lower for w in ['–æ—Ç–ª–∏—á–Ω–æ', '–∑–∞–º–µ—á–∞—Ç–µ–ª—å–Ω–æ', '–∫—Ä—É—Ç–æ', '—Ä–µ–∑—É–ª—å—Ç–∞—Ç', '—Ä–æ—Å—Ç', '—É–≤–µ–ª–∏—á–∏–ª', '—Å—ç–∫–æ–Ω–æ–º–∏–ª']):
        return VOICE_PROFILES["excited"]
    return VOICE_PROFILES["default"]


def _select_output_format(text_length: int) -> str:
    if text_length <= SHORT_TEXT_THRESHOLD:
        return SHORT_TEXT_FORMAT
    return LONG_TEXT_FORMAT


async def _generate_voice_streaming_async(voice_text: str, profile: dict, output_format: str) -> bytes:
    async_client = _get_async_elevenlabs_client()
    if not async_client:
        return await _generate_voice_sync_fallback(voice_text, profile, output_format)

    from elevenlabs import VoiceSettings

    start_time = time.monotonic()

    try:
        stream_kwargs = {
            "voice_id": config.elevenlabs_voice_id,
            "text": voice_text,
            "model_id": "eleven_v3",
            "output_format": output_format,
            "voice_settings": VoiceSettings(
                stability=profile["stability"],
                similarity_boost=profile["similarity_boost"],
                style=profile["style"],
            ),
        }

        stream_call = async_client.text_to_speech.stream(**stream_kwargs)
        if asyncio.iscoroutine(stream_call):
            audio_stream = await stream_call
        else:
            audio_stream = stream_call

        chunks = []
        first_chunk_time = None
        async for chunk in audio_stream:
            if chunk:
                if first_chunk_time is None:
                    first_chunk_time = time.monotonic()
                chunks.append(chunk)

        audio_bytes = b"".join(chunks)
        total_time = time.monotonic() - start_time
        ttfb = (first_chunk_time - start_time) if first_chunk_time else total_time

        logger.info(
            f"Streaming TTS: {len(voice_text)} chars ‚Üí {len(audio_bytes)} bytes, "
            f"TTFB={ttfb:.2f}s, total={total_time:.2f}s, "
            f"format={output_format}, chunks={len(chunks)}"
        )
        return audio_bytes

    except Exception as e:
        logger.warning(f"Async streaming TTS failed ({type(e).__name__}): {e}, falling back to sync")
        return await _generate_voice_sync_fallback(voice_text, profile, output_format)


async def _generate_voice_sync_fallback(voice_text: str, profile: dict, output_format: str) -> bytes:
    el_client = _get_elevenlabs_client()
    if not el_client:
        raise RuntimeError("ElevenLabs client not configured")

    from elevenlabs import VoiceSettings

    start_time = time.monotonic()

    audio_generator = await asyncio.to_thread(
        el_client.text_to_speech.convert,
        voice_id=config.elevenlabs_voice_id,
        text=voice_text,
        model_id="eleven_v3",
        output_format=output_format,
        voice_settings=VoiceSettings(
            stability=profile["stability"],
            similarity_boost=profile["similarity_boost"],
            style=profile["style"],
        )
    )

    audio_bytes = b"".join(audio_generator)
    total_time = time.monotonic() - start_time
    logger.info(
        f"Sync TTS fallback: {len(voice_text)} chars ‚Üí {len(audio_bytes)} bytes, "
        f"total={total_time:.2f}s, format={output_format}"
    )
    return audio_bytes


async def generate_voice_response(text: str, use_cache: bool = False, voice_profile: str = None) -> bytes:
    global _voice_cache
    
    if not config.elevenlabs_api_key:
        raise RuntimeError("ElevenLabs client not configured")

    clean_text = _clean_text_for_voice(text)
    
    if use_cache:
        cache_key = hashlib.md5(clean_text.encode()).hexdigest()
        if cache_key in _voice_cache:
            logger.debug("Using cached voice response")
            return _voice_cache[cache_key]

    voice_text = await enhance_voice_text(clean_text)

    voice_text = naturalize_speech(voice_text)
    voice_text = expand_abbreviations(voice_text)
    voice_text = numbers_to_words(voice_text)
    voice_text = apply_stress_marks(voice_text)

    if len(voice_text) > 4500:
        cut_pos = voice_text[:4500].rfind('.')
        if cut_pos > 3000:
            voice_text = voice_text[:cut_pos + 1]
        else:
            voice_text = voice_text[:4500].rsplit(' ', 1)[0] + '.'

    if voice_profile and voice_profile in VOICE_PROFILES:
        profile = VOICE_PROFILES[voice_profile]
    else:
        profile = _detect_voice_profile(voice_text)

    output_format = _select_output_format(len(voice_text))

    try:
        audio_bytes = await _generate_voice_streaming_async(voice_text, profile, output_format)

        if not audio_bytes:
            raise RuntimeError("Empty audio response from TTS")

        if use_cache:
            cache_key = hashlib.md5(clean_text.encode()).hexdigest()
            _voice_cache[cache_key] = audio_bytes
            if len(_voice_cache) > 10:
                oldest = next(iter(_voice_cache))
                del _voice_cache[oldest]
        
        return audio_bytes
    except Exception as e:
        logger.error(f"ElevenLabs voice generation failed ({type(e).__name__}): {e}")
        raise


async def _transcribe_voice(voice_bytes: bytes) -> str:
    result = await _transcribe_voice_with_emotion(voice_bytes)
    return result.get("text", "")


async def _convert_ogg_to_wav(ogg_bytes: bytes) -> bytes:
    import tempfile
    import os
    from io import BytesIO

    try:
        from pydub import AudioSegment
        audio = AudioSegment.from_ogg(BytesIO(ogg_bytes))
        audio = audio.set_frame_rate(16000).set_channels(1)
        wav_buffer = BytesIO()
        audio.export(wav_buffer, format="wav")
        wav_data = wav_buffer.getvalue()
        logger.info(f"Converted OGG ({len(ogg_bytes)} bytes) to WAV ({len(wav_data)} bytes) via pydub")
        return wav_data
    except Exception as e:
        logger.warning(f"pydub conversion failed: {e}")

    ogg_path = None
    wav_path = None
    try:
        import subprocess
        with tempfile.NamedTemporaryFile(suffix='.ogg', delete=False) as ogg_file:
            ogg_file.write(ogg_bytes)
            ogg_path = ogg_file.name
        wav_path = ogg_path.replace('.ogg', '.wav')
        result = subprocess.run(
            ['ffmpeg', '-y', '-i', ogg_path, '-ar', '16000', '-ac', '1', '-f', 'wav', wav_path],
            capture_output=True, timeout=15
        )
        if result.returncode == 0:
            with open(wav_path, 'rb') as f:
                wav_data = f.read()
            logger.info(f"Converted OGG ({len(ogg_bytes)} bytes) to WAV ({len(wav_data)} bytes) via ffmpeg")
            return wav_data
        else:
            logger.warning(f"ffmpeg conversion failed: {result.stderr[:200]}")
    except FileNotFoundError:
        logger.warning("ffmpeg not found")
    except Exception as e:
        logger.warning(f"ffmpeg conversion error: {e}")
    finally:
        for p in [ogg_path, wav_path]:
            if p:
                try:
                    os.unlink(p)
                except Exception:
                    pass
    return b""


def _parse_emotion_json(raw: str) -> dict:
    import json as _json

    if raw.startswith("```"):
        raw = raw.split("\n", 1)[-1].rsplit("```", 1)[0].strip()

    try:
        parsed = _json.loads(raw)
        return {
            "text": parsed.get("text", "").strip(),
            "emotion": parsed.get("emotion", "neutral"),
            "energy": parsed.get("energy", "medium")
        }
    except _json.JSONDecodeError:
        pass

    json_match = re.search(r'\{.*?"text"\s*:\s*".*?".*?\}', raw, re.DOTALL)
    if not json_match:
        json_match = re.search(r'\{.+?\}', raw, re.DOTALL)
    if json_match:
        try:
            parsed = _json.loads(json_match.group())
            return {
                "text": parsed.get("text", "").strip(),
                "emotion": parsed.get("emotion", "neutral"),
                "energy": parsed.get("energy", "medium")
            }
        except _json.JSONDecodeError:
            pass

    text_match = re.search(r'"text"\s*:\s*"([^"]*)"', raw)
    if text_match and text_match.group(1).strip():
        emotion_match = re.search(r'"emotion"\s*:\s*"([^"]*)"', raw)
        energy_match = re.search(r'"energy"\s*:\s*"([^"]*)"', raw)
        return {
            "text": text_match.group(1).strip(),
            "emotion": emotion_match.group(1) if emotion_match else "neutral",
            "energy": energy_match.group(1) if energy_match else "medium"
        }

    clean_text = raw.strip().strip('"').strip("'")
    if len(clean_text) > 5 and not clean_text.startswith("{"):
        return {"text": clean_text, "emotion": "neutral", "energy": "medium"}
    return {"text": "", "emotion": "neutral", "energy": "medium"}


async def _transcribe_voice_with_emotion(voice_bytes: bytes) -> dict:
    from google import genai
    from google.genai import types
    import tempfile
    import os

    from src.config import get_gemini_client
    client = get_gemini_client()
    audio_model = config.audio_model_name

    prompt_text = (
        "–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —ç—Ç–æ –≥–æ–ª–æ—Å–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ. –í–µ—Ä–Ω–∏ JSON:\n"
        '{"text": "–¥–æ—Å–ª–æ–≤–Ω–∞—è —Ä–∞—Å—à–∏—Ñ—Ä–æ–≤–∫–∞ –Ω–∞ —è–∑—ã–∫–µ –æ—Ä–∏–≥–∏–Ω–∞–ª–∞", '
        '"emotion": "–æ–¥–Ω–æ —Å–ª–æ–≤–æ: confident/hesitant/frustrated/excited/neutral/friendly/rushed/calm", '
        '"energy": "low/medium/high"}\n'
        "–ï—Å–ª–∏ –Ω–µ –º–æ–∂–µ—à—å —Ä–∞–∑–æ–±—Ä–∞—Ç—å —Ç–µ–∫—Å—Ç ‚Äî –≤–µ—Ä–Ω–∏ –ø—É—Å—Ç–æ–π text.\n"
        "–í–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û JSON, –±–µ–∑ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ –∏ markdown."
    )

    wav_bytes = await _convert_ogg_to_wav(voice_bytes)

    strategies = []

    if wav_bytes:
        strategies.append(("files_api_wav", wav_bytes, "audio/wav", ".wav"))
        strategies.append(("inline_wav", wav_bytes, "audio/wav", None))
    strategies.append(("files_api_ogg", bytes(voice_bytes), "audio/ogg", ".ogg"))
    strategies.append(("inline_ogg", bytes(voice_bytes), "audio/ogg", None))

    for strategy_name, audio_data, mime, suffix in strategies:
        uploaded_file = None
        tmp_path = None
        try:
            if strategy_name.startswith("files_api"):
                try:
                    with tempfile.NamedTemporaryFile(suffix=suffix, delete=False) as tmp:
                        tmp.write(audio_data)
                        tmp_path = tmp.name

                    upload_config = types.UploadFileConfig(mime_type=mime)
                    uploaded_file = await asyncio.to_thread(
                        client.files.upload,
                        file=tmp_path,
                        config=upload_config
                    )
                    logger.info(f"[{strategy_name}] Uploaded {len(audio_data)} bytes, uri={uploaded_file.uri}, mime={uploaded_file.mime_type}")

                    audio_part = types.Part.from_uri(
                        file_uri=uploaded_file.uri,
                        mime_type=mime
                    )
                except Exception as upload_err:
                    logger.warning(f"[{strategy_name}] Upload failed: {upload_err}")
                    continue
                finally:
                    if tmp_path:
                        try:
                            os.unlink(tmp_path)
                        except Exception:
                            pass
            else:
                audio_part = types.Part.from_bytes(data=audio_data, mime_type=mime)
                logger.info(f"[{strategy_name}] Using inline {len(audio_data)} bytes, mime={mime}")

            text_part = types.Part(text=prompt_text)

            response = await asyncio.to_thread(
                client.models.generate_content,
                model=audio_model,
                contents=[audio_part, text_part],
                config=types.GenerateContentConfig(
                    max_output_tokens=600,
                    temperature=0.1
                )
            )

            resp_text = None
            try:
                resp_text = response.text
            except (ValueError, AttributeError):
                candidates = getattr(response, 'candidates', None)
                if candidates and len(candidates) > 0:
                    parts = getattr(candidates[0].content, 'parts', [])
                    if parts:
                        resp_text = getattr(parts[0], 'text', None)

            logger.info(f"[{strategy_name}] model={audio_model}, response={resp_text[:300] if resp_text else 'None'}")

            if resp_text:
                result = _parse_emotion_json(resp_text.strip())
                if result["text"]:
                    if uploaded_file:
                        try:
                            await asyncio.to_thread(client.files.delete, name=uploaded_file.name)
                        except Exception:
                            pass
                    return result
                logger.warning(f"[{strategy_name}] Parsed text is empty from raw: {resp_text[:300]}")
            else:
                logger.warning(f"[{strategy_name}] No text in response, candidates={getattr(response, 'candidates', 'N/A')}")

        except Exception as e:
            logger.error(f"[{strategy_name}] Transcription error: {e}", exc_info=True)
        finally:
            if uploaded_file:
                try:
                    await asyncio.to_thread(client.files.delete, name=uploaded_file.name)
                except Exception:
                    pass

    logger.error(f"All transcription strategies failed for {len(voice_bytes)} bytes audio")
    return {"text": "", "emotion": "neutral", "energy": "medium"}


EMOTION_TO_VOICE_STYLE = {
    "confident": "–ö–ª–∏–µ–Ω—Ç –∑–≤—É—á–∏—Ç —É–≤–µ—Ä–µ–Ω–Ω–æ ‚Äî –≥–æ–≤–æ—Ä–∏ –Ω–∞ –µ–≥–æ —É—Ä–æ–≤–Ω–µ, —Ñ–∞–∫—Ç—ã –∏ –∫–æ–Ω–∫—Ä–µ—Ç–∏–∫–∞.",
    "hesitant": "–ö–ª–∏–µ–Ω—Ç –∑–≤—É—á–∏—Ç –Ω–µ—Ä–µ—à–∏—Ç–µ–ª—å–Ω–æ ‚Äî –±—É–¥—å –º—è–≥—á–µ, —É–±–∏—Ä–∞–π –¥–∞–≤–ª–µ–Ω–∏–µ, –ø—Ä–µ–¥–ª–∞–≥–∞–π –º–∞–ª–µ–Ω—å–∫–∏–µ —à–∞–≥–∏.",
    "frustrated": "–ö–ª–∏–µ–Ω—Ç –∑–≤—É—á–∏—Ç —Ä–∞–∑–¥—Ä–∞–∂—ë–Ω–Ω–æ ‚Äî –ø—Ä–æ—è–≤–∏ —ç–º–ø–∞—Ç–∏—é, –ø—Ä–∏–∑–Ω–∞–π –ø—Ä–æ–±–ª–µ–º—É, –ø—Ä–µ–¥–ª–æ–∂–∏ —Ä–µ—à–µ–Ω–∏–µ.",
    "excited": "–ö–ª–∏–µ–Ω—Ç –∑–≤—É—á–∏—Ç –≤–æ–æ–¥—É—à–µ–≤–ª—ë–Ω–Ω–æ ‚Äî –ø–æ–¥–¥–µ—Ä–∂–∏ —ç–Ω—Ç—É–∑–∏–∞–∑–º, —É—Å–∏–ª—å —ç–º–æ—Ü–∏—é, –¥–≤–∏–≥–∞–π –∫ –¥–µ–π—Å—Ç–≤–∏—é.",
    "neutral": "",
    "friendly": "–ö–ª–∏–µ–Ω—Ç –∑–≤—É—á–∏—Ç –¥—Ä—É–∂–µ–ª—é–±–Ω–æ ‚Äî –∑–µ—Ä–∫–∞–ª—å —Ç—ë–ø–ª—ã–π —Ç–æ–Ω, –±—É–¥—å –æ—Ç–∫—Ä—ã—Ç—ã–º.",
    "rushed": "–ö–ª–∏–µ–Ω—Ç —Ç–æ—Ä–æ–ø–∏—Ç—Å—è ‚Äî –±—É–¥—å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –∫—Ä–∞—Ç–∫–∏–º, —Ç–æ–ª—å–∫–æ —Å—É—Ç—å.",
    "calm": "–ö–ª–∏–µ–Ω—Ç —Å–ø–æ–∫–æ–µ–Ω ‚Äî –æ—Ç–≤–µ—á–∞–π —Ä–∞–∑–º–µ—Ä–µ–Ω–Ω–æ, –±–µ–∑ —Å—É–µ—Ç—ã."
}


VOICE_SALES_TRIGGERS = {
    "price_discussion": ["—Ü–µ–Ω–∞", "—Å—Ç–æ–∏–º–æ—Å—Ç—å", "—Å–∫–æ–ª—å–∫–æ —Å—Ç–æ–∏—Ç", "–±—é–¥–∂–µ—Ç", "–¥–æ—Ä–æ–≥–æ", "–¥–µ—à–µ–≤–ª–µ", "—Å–∫–∏–¥–∫"],
    "objection": ["–Ω–µ —É–≤–µ—Ä–µ–Ω", "–ø–æ–¥—É–º–∞—é", "–¥–æ—Ä–æ–≥–æ", "–ø–æ—Ç–æ–º", "–Ω–µ –∑–Ω–∞—é", "—Å–æ–º–Ω–µ–≤–∞—é—Å—å", "–º–æ–∂–µ—Ç –±—ã—Ç—å"],
    "decision": ["–≥–æ—Ç–æ–≤", "—Ö–æ—á—É –∑–∞–∫–∞–∑–∞—Ç—å", "–¥–∞–≤–∞–π—Ç–µ", "–Ω–∞—á–∏–Ω–∞–µ–º", "–æ–ø–ª–∞—Ç–∞", "–¥–æ–≥–æ–≤–æ—Ä", "–∫–æ–≥–¥–∞ –Ω–∞—á–Ω—ë–º"],
    "closing": ["–æ–ø–ª–∞—Ç–∏—Ç—å", "—Ä–µ–∫–≤–∏–∑–∏—Ç", "—Å—á—ë—Ç", "–ø—Ä–µ–¥–æ–ø–ª–∞—Ç", "–¥–æ–≥–æ–≤–æ—Ä –ø–æ–¥–ø–∏—Å"],
}


PROACTIVE_VOICE_COOLDOWN = 600
PROACTIVE_VOICE_MAX_PER_SESSION = 3


def should_send_proactive_voice(user_id: int, message_text: str, context_user_data: dict) -> bool:
    import time as _time

    if not config.elevenlabs_api_key:
        return False
    if not context_user_data.get('prefers_voice'):
        return False
    if context_user_data.get('voice_message_count', 0) < 1:
        return False

    proactive_count = context_user_data.get('proactive_voice_count', 0)
    if proactive_count >= PROACTIVE_VOICE_MAX_PER_SESSION:
        return False

    last_voice_ts = context_user_data.get('last_proactive_voice_ts', 0)
    if _time.time() - last_voice_ts < PROACTIVE_VOICE_COOLDOWN:
        return False

    triggered = False
    lower = message_text.lower()
    for trigger_words in VOICE_SALES_TRIGGERS.values():
        if any(w in lower for w in trigger_words):
            triggered = True
            break

    if not triggered:
        try:
            from src.context_builder import detect_funnel_stage
            stage = detect_funnel_stage(user_id, message_text, 0)
            if stage in ("decision", "action"):
                triggered = True
        except Exception:
            pass

    if not triggered:
        try:
            from src.propensity import propensity_scorer
            score = propensity_scorer.get_score(user_id)
            if score and score >= 60:
                triggered = True
        except Exception:
            pass

    if triggered:
        context_user_data['last_proactive_voice_ts'] = _time.time()
        context_user_data['proactive_voice_count'] = proactive_count + 1

    return triggered


def _make_text_summary(full_text: str, max_len: int = 300) -> str:
    clean = full_text.replace("**", "").replace("*", "").replace("#", "").replace("`", "")
    clean = re.sub(r'[\U0001F600-\U0001F64F\U0001F300-\U0001F5FF\U0001F680-\U0001F6FF\U0001F1E0-\U0001F1FF\U00002702-\U000027B0\U000024C2-\U0001F251\U0001f926-\U0001f937\U00010000-\U0010ffff\u2600-\u2B55\u200d\u23cf\u23e9\u231a\ufe0f\u3030\u2066\u2069]+', '', clean)
    if len(clean) <= max_len:
        return clean.strip()
    cut = clean[:max_len].rfind('.')
    if cut > max_len * 0.5:
        return clean[:cut + 1].strip()
    cut = clean[:max_len].rfind(' ')
    if cut > 0:
        return clean[:cut].strip() + "..."
    return clean[:max_len].strip() + "..."


async def voice_handler(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    user = update.effective_user

    typing_task = asyncio.create_task(
        send_typing_action(update, duration=60.0)
    )

    try:
        voice = update.message.voice
        file = await context.bot.get_file(voice.file_id)
        voice_bytes = await file.download_as_bytearray()

        voice_analysis = await _transcribe_voice_with_emotion(voice_bytes)
        transcription = voice_analysis.get("text", "")
        client_emotion = voice_analysis.get("emotion", "neutral")
        client_energy = voice_analysis.get("energy", "medium")

        if not transcription:
            typing_task.cancel()
            await update.message.reply_text(
                "–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–æ–∑–Ω–∞—Ç—å —Å–æ–æ–±—â–µ–Ω–∏–µ. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â—ë —Ä–∞–∑ –∏–ª–∏ –Ω–∞–ø–∏—à–∏—Ç–µ —Ç–µ–∫—Å—Ç–æ–º."
            )
            return

        logger.info(f"User {user.id} voice transcribed ({len(transcription)} chars, emotion={client_emotion}, energy={client_energy}): {transcription[:100]}...")

        session = session_manager.get_session(
            user_id=user.id,
            username=user.username,
            first_name=user.first_name
        )

        session.add_message("user", transcription, config.max_history_length)
        lead_manager.save_message(user.id, "user", f"[–ì–æ–ª–æ—Å–æ–≤–æ–µ] {transcription}")
        lead_manager.log_event("voice_message", user.id, {
            "duration": voice.duration if voice.duration else 0,
            "length": len(transcription),
            "emotion": client_emotion,
            "energy": client_energy
        })
        lead_manager.update_activity(user.id)
        
        context.user_data['prefers_voice'] = True
        context.user_data['voice_message_count'] = context.user_data.get('voice_message_count', 0) + 1

        try:
            from src.session import save_client_profile
            save_client_profile(user.id, prefers_voice="true")
        except Exception:
            pass

        from src.followup import follow_up_manager
        follow_up_manager.cancel_follow_ups(user.id)
        follow_up_manager.schedule_follow_up(user.id)

        from src.context_builder import build_full_context, parse_ai_buttons
        client_context = build_full_context(user.id, transcription, user.username, user.first_name)

        emotion_hint = EMOTION_TO_VOICE_STYLE.get(client_emotion, "")
        if emotion_hint:
            emotion_context = f"\n[–≠–ú–û–¶–ò–Ø –ö–õ–ò–ï–ù–¢–ê] {emotion_hint} –≠–Ω–µ—Ä–≥–∏—è: {client_energy}."
            if client_context:
                client_context += emotion_context
            else:
                client_context = emotion_context

        from src.ai_client import ai_client

        messages_for_ai = session.get_history()
        
        voice_instruction = {
            "role": "user",
            "parts": [{"text": VOICE_CONTEXT_INSTRUCTION}]
        }
        voice_ack = {
            "role": "model",
            "parts": [{"text": "–ü–æ–Ω—è–ª, –≥–æ–≤–æ—Ä—é –∫–∞–∫ –∂–∏–≤–æ–π —á–µ–ª–æ–≤–µ–∫ ‚Äî –∫–æ—Ä–æ—Ç–∫–æ, –ø–æ –¥–µ–ª—É, —Ä–∞–∑–≥–æ–≤–æ—Ä–Ω—ã–º —è–∑—ã–∫–æ–º –±–µ–∑ —Ä–∞–∑–º–µ—Ç–∫–∏."}]
        }
        
        if client_context:
            context_msg = {
                "role": "user",
                "parts": [{"text": f"[–°–ò–°–¢–ï–ú–ù–´–ô –ö–û–ù–¢–ï–ö–°–¢ ‚Äî –Ω–µ –ø–æ–∫–∞–∑—ã–≤–∞–π –∫–ª–∏–µ–Ω—Ç—É, –∏—Å–ø–æ–ª—å–∑—É–π –¥–ª—è –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏]\n{client_context}"}]
            }
            response_ack = {
                "role": "model",
                "parts": [{"text": "–ü–æ–Ω—è–ª –∫–æ–Ω—Ç–µ–∫—Å—Ç, —É—á—Ç—É –≤ –æ—Ç–≤–µ—Ç–µ."}]
            }
            messages_for_ai = [voice_instruction, voice_ack, context_msg, response_ack] + messages_for_ai
        else:
            messages_for_ai = [voice_instruction, voice_ack] + messages_for_ai

        from src.tool_handlers import execute_tool_call

        async def _tool_executor(tool_name, tool_args):
            return await execute_tool_call(
                tool_name, tool_args,
                user.id, user.username, user.first_name
            )

        thinking_level = "high" if len(transcription) > 100 else "medium"

        response_text = None
        special_actions = []

        try:
            agentic_result = await ai_client.agentic_loop(
                messages=messages_for_ai,
                tool_executor=_tool_executor,
                thinking_level=thinking_level,
                max_steps=4
            )

            special_actions = agentic_result.get("special_actions", [])

            if special_actions:
                for action_type, action_data in special_actions:
                    if action_type == "portfolio":
                        from src.keyboards import get_portfolio_keyboard
                        from src.knowledge_base import PORTFOLIO_MESSAGE
                        await update.message.reply_text(
                            PORTFOLIO_MESSAGE, parse_mode="Markdown",
                            reply_markup=get_portfolio_keyboard()
                        )
                    elif action_type == "pricing":
                        from src.pricing import get_price_main_text, get_price_main_keyboard
                        await update.message.reply_text(
                            get_price_main_text(), parse_mode="Markdown",
                            reply_markup=get_price_main_keyboard()
                        )
                    elif action_type == "payment":
                        from src.payments import get_payment_keyboard
                        await update.message.reply_text(
                            "–í—ã–±–µ—Ä–∏—Ç–µ —Å–ø–æ—Å–æ–± –æ–ø–ª–∞—Ç—ã:",
                            reply_markup=get_payment_keyboard()
                        )

            if agentic_result.get("text"):
                response_text = agentic_result["text"]
            elif special_actions and not agentic_result.get("text"):
                typing_task.cancel()
                try:
                    await typing_task
                except asyncio.CancelledError:
                    pass
                session.add_message("assistant", "–ü–æ–∫–∞–∑–∞–ª –∑–∞–ø—Ä–æ—à–µ–Ω–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é", config.max_history_length)
                lead_manager.save_message(user.id, "assistant", "–ü–æ–∫–∞–∑–∞–ª –∑–∞–ø—Ä–æ—à–µ–Ω–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é")
                _run_voice_post_processing(user.id, transcription, session)
                return
        except Exception as e:
            logger.warning(f"Voice agentic loop failed, falling back to direct: {e}")

            from src.knowledge_base import SYSTEM_PROMPT
            from google import genai
            from google.genai import types

            from src.config import get_gemini_client
            gemini_client = get_gemini_client()

            history_text = ""
            for msg in session.get_history()[-6:]:
                role = "–ö–ª–∏–µ–Ω—Ç" if msg.get("role") == "user" else "–ê–ª–µ–∫—Å"
                parts = msg.get("parts", [])
                txt = parts[0].get("text", "") if parts else ""
                if txt and not txt.startswith("[–°–ò–°–¢–ï–ú–ù–´–ô") and not txt.startswith("[–ì–û–õ–û–°–û–í–û–ô"):
                    history_text += f"{role}: {txt}\n"

            context_addition = ""
            if client_context:
                context_addition = f"\n[–ö–û–ù–¢–ï–ö–°–¢]\n{client_context}\n"

            full_prompt = (
                f"{VOICE_CONTEXT_INSTRUCTION}\n"
                f"{context_addition}"
                f"–ò—Å—Ç–æ—Ä–∏—è –¥–∏–∞–ª–æ–≥–∞:\n{history_text}\n"
                f"–ö–ª–∏–µ–Ω—Ç —Å–∫–∞–∑–∞–ª –≥–æ–ª–æ—Å–æ–≤—ã–º: {transcription}\n\n"
                f"–û—Ç–≤–µ—Ç—å –∫–∞–∫ –∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç –ê–ª–µ–∫—Å. –ö–æ—Ä–æ—Ç–∫–æ, —Ä–∞–∑–≥–æ–≤–æ—Ä–Ω–æ, –¥–ª—è –æ–∑–≤—É—á–∫–∏."
            )

            response = await asyncio.to_thread(
                gemini_client.models.generate_content,
                model=config.model_name,
                contents=[full_prompt],
                config=types.GenerateContentConfig(
                    system_instruction=SYSTEM_PROMPT,
                    max_output_tokens=1000,
                    temperature=0.7
                )
            )

            if response.text:
                response_text = response.text

        if not response_text:
            response_text = "–ò–∑–≤–∏–Ω–∏—Ç–µ, –Ω–µ —É–¥–∞–ª–æ—Å—å —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –≤–æ–ø—Ä–æ—Å."

        response_text, ai_buttons = parse_ai_buttons(response_text)

        session.add_message("assistant", response_text, config.max_history_length)
        lead_manager.save_message(user.id, "assistant", response_text)

        typing_task.cancel()
        try:
            await typing_task
        except asyncio.CancelledError:
            pass

        voice_sent = False
        if config.elevenlabs_api_key:
            try:
                await update.effective_chat.send_action(ChatAction.RECORD_VOICE)
                voice_response = await generate_voice_response(response_text)
                await update.message.reply_voice(voice=voice_response)
                voice_sent = True
                lead_manager.log_event("voice_reply_sent", user.id)
            except Exception as e:
                logger.error(f"ElevenLabs TTS error ({type(e).__name__}): {e}")
        reply_markup = None
        if ai_buttons:
            keyboard_rows = [[InlineKeyboardButton(text, callback_data=cb)] for text, cb in ai_buttons]
            reply_markup = InlineKeyboardMarkup(keyboard_rows)

        text_summary = _make_text_summary(response_text)
        if voice_sent:
            summary_with_note = f"üëÜ –ì–æ–ª–æ—Å–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ\n\n{text_summary}"
            if reply_markup:
                await update.message.reply_text(summary_with_note, reply_markup=reply_markup)
            else:
                await update.message.reply_text(summary_with_note)
        else:
            if len(response_text) > 4096:
                chunks = [response_text[i:i+4096] for i in range(0, len(response_text), 4096)]
                for i, chunk in enumerate(chunks):
                    if i == len(chunks) - 1:
                        await update.message.reply_text(chunk, reply_markup=reply_markup)
                    else:
                        await update.message.reply_text(chunk)
            else:
                await update.message.reply_text(response_text, reply_markup=reply_markup)

        logger.info(f"User {user.id}: voice processed (emotion={client_emotion}, voice_reply={'yes' if voice_sent else 'no'}, voice_msg#{context.user_data.get('voice_message_count', 0)})")

        _run_voice_post_processing(user.id, transcription, session)

    except Exception as e:
        typing_task.cancel()
        logger.error(f"Voice processing error ({type(e).__name__}): {e}")
        await update.message.reply_text(
            "–ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –≥–æ–ª–æ—Å–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ. –ù–∞–ø–∏—à–∏—Ç–µ —Ç–µ–∫—Å—Ç–æ–º, –ø–æ–∂–∞–ª—É–π—Å—Ç–∞."
        )


def _run_voice_post_processing(user_id: int, transcription: str, session):
    from src.handlers.messages import auto_tag_lead, auto_score_lead, extract_insights_if_needed, summarize_if_needed

    auto_tag_lead(user_id, transcription)
    auto_score_lead(user_id, transcription)

    asyncio.create_task(
        extract_insights_if_needed(user_id, session)
    )
    asyncio.create_task(
        summarize_if_needed(user_id, session)
    )


async def photo_handler(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    user = update.effective_user
    if not user or not update.message:
        return
    user_id = user.id

    if context.user_data.get('broadcast_compose'):
        from src.security import is_admin
        if is_admin(user_id):
            context.user_data.pop('broadcast_compose', None)
            photo = update.message.photo[-1]
            context.user_data['broadcast_draft'] = {
                'type': 'photo',
                'file_id': photo.file_id,
                'caption': update.message.caption or '',
            }
            from src.broadcast import broadcast_manager
            counts = broadcast_manager.get_audience_counts()
            from src.handlers.utils import get_broadcast_audience_keyboard
            keyboard = get_broadcast_audience_keyboard(counts)
            caption_preview = f"\nüìù {update.message.caption}" if update.message.caption else ""
            await update.message.reply_text(
                f"üìã <b>–ü—Ä–µ–¥–ø—Ä–æ—Å–º–æ—Ç—Ä —Ä–∞—Å—Å—ã–ª–∫–∏:</b>\n\nüì∏ –§–æ—Ç–æ{caption_preview}\n\n<b>–í—ã–±–µ—Ä–∏—Ç–µ –∞—É–¥–∏—Ç–æ—Ä–∏—é:</b>",
                parse_mode="HTML",
                reply_markup=keyboard
            )
            return

    pending_review_type = context.user_data.get("pending_review_type")

    if pending_review_type != "text_photo":
        typing_task = asyncio.create_task(
            send_typing_action(update, duration=45.0)
        )
        try:
            photo = update.message.photo[-1] if update.message.photo else None
            if not photo:
                typing_task.cancel()
                return

            file = await context.bot.get_file(photo.file_id)
            photo_bytes = await file.download_as_bytearray()

            caption = update.message.caption or ""

            session = session_manager.get_session(
                user_id=user.id,
                username=user.username,
                first_name=user.first_name
            )

            from src.vision_sales import (
                VISION_CLASSIFICATION_PROMPT,
                get_image_type_from_caption,
                build_vision_system_prompt,
                get_smart_buttons_for_image,
                get_lead_score_boost,
                get_intents_for_image,
                is_hot_image,
                is_warm_image,
                build_manager_notification,
                get_vision_analysis_context,
                ImageType,
            )

            from google.genai import types as genai_types
            from src.config import get_gemini_client
            gemini_client = get_gemini_client()

            image_part = genai_types.Part.from_bytes(data=bytes(photo_bytes), mime_type="image/jpeg")

            caption_hint = get_image_type_from_caption(caption)

            if caption_hint:
                image_type = caption_hint
            else:
                try:
                    classify_response = await asyncio.to_thread(
                        gemini_client.models.generate_content,
                        model=config.model_name,
                        contents=[image_part, genai_types.Part(text=VISION_CLASSIFICATION_PROMPT)],
                        config=genai_types.GenerateContentConfig(
                            max_output_tokens=30,
                            temperature=0.1
                        )
                    )
                    raw_type = (classify_response.text or "general").strip().lower().replace(" ", "_")
                    valid_types = {e.value for e in ImageType}
                    image_type = raw_type if raw_type in valid_types else ImageType.GENERAL.value
                except Exception as classify_err:
                    logger.warning(f"Image classification failed: {classify_err}")
                    image_type = ImageType.GENERAL.value

            logger.info(f"Vision analysis for user {user_id}: type={image_type}, caption={caption[:100] if caption else 'none'}")

            user_text = caption if caption else f"–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —ç—Ç–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ (—Ç–∏–ø: {image_type})"

            session.add_message("user", f"[–§–æ—Ç–æ: {image_type}]{f': {caption}' if caption else ''}", config.max_history_length)
            lead_manager.save_message(user.id, "user", f"[–§–æ—Ç–æ: {image_type}]{f': {caption}' if caption else ''}")
            lead_manager.log_event(f"photo_{image_type}", user.id)
            lead_manager.update_activity(user.id)

            from src.context_builder import build_full_context, parse_ai_buttons
            client_context = build_full_context(user.id, user_text, user.username, user.first_name)

            vision_context = get_vision_analysis_context(image_type)
            full_client_ctx = f"{vision_context}\n{client_context}" if client_context else vision_context

            system_prompt = build_vision_system_prompt(image_type, full_client_ctx)

            analysis_text = genai_types.Part(text=user_text if caption else "–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —ç—Ç–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ.")
            all_parts = [image_part, analysis_text]

            response = await asyncio.to_thread(
                gemini_client.models.generate_content,
                model=config.model_name,
                contents=all_parts,
                config=genai_types.GenerateContentConfig(
                    system_instruction=system_prompt,
                    max_output_tokens=2000,
                    temperature=0.7
                )
            )

            typing_task.cancel()

            if response.text:
                clean_text, ai_buttons = parse_ai_buttons(response.text)
                session.add_message("assistant", clean_text, config.max_history_length)
                lead_manager.save_message(user.id, "assistant", clean_text)

                try:
                    from src.session import save_vision_context
                    save_vision_context(user.id, image_type, clean_text[:300])
                except Exception as e:
                    logger.debug(f"Vision context save error: {e}")

                if not ai_buttons:
                    smart_btns = get_smart_buttons_for_image(image_type)
                    ai_buttons = smart_btns[:3]

                keyboard_rows = []
                for i in range(0, len(ai_buttons), 2):
                    row = []
                    for label, cb in ai_buttons[i:i+2]:
                        row.append(InlineKeyboardButton(label, callback_data=cb))
                    keyboard_rows.append(row)
                reply_markup = InlineKeyboardMarkup(keyboard_rows) if keyboard_rows else None

                await update.message.reply_text(clean_text, parse_mode="Markdown", reply_markup=reply_markup)

                from src.handlers.messages import auto_tag_lead, auto_score_lead
                auto_tag_lead(user.id, user_text + f" [photo:{image_type}]")
                auto_score_lead(user.id, user_text)

                score_boost = get_lead_score_boost(image_type)
                if score_boost > 5:
                    try:
                        from src.propensity import propensity_scorer
                        propensity_scorer.boost_score(user.id, score_boost, f"photo_{image_type}")
                    except Exception:
                        pass

                if is_hot_image(image_type) or is_warm_image(image_type):
                    manager_text = build_manager_notification(
                        user.id, user.username, user.first_name, image_type, caption
                    )
                    if manager_text and MANAGER_CHAT_ID:
                        try:
                            await context.bot.send_message(
                                chat_id=MANAGER_CHAT_ID,
                                text=manager_text,
                                parse_mode="HTML"
                            )
                            await context.bot.forward_message(
                                chat_id=MANAGER_CHAT_ID,
                                from_chat_id=update.effective_chat.id if update.effective_chat else user_id,
                                message_id=update.message.message_id
                            )
                            if is_hot_image(image_type):
                                try:
                                    from src.manager_coaching import generate_coaching_briefing
                                    briefing = generate_coaching_briefing(
                                        user_id=user.id,
                                        trigger_type="high_value",
                                        last_user_message=caption,
                                    )
                                    if briefing:
                                        await context.bot.send_message(
                                            chat_id=MANAGER_CHAT_ID,
                                            text=briefing,
                                            parse_mode="HTML"
                                        )
                                except Exception:
                                    pass
                        except Exception as notify_err:
                            logger.warning(f"Manager notification failed: {notify_err}")

                try:
                    from src.feedback_loop import feedback_loop
                    from src.context_builder import detect_funnel_stage
                    stage = detect_funnel_stage(user.id, user_text, 0)
                    feedback_loop.log_response(
                        user_id=user.id,
                        message_text=f"[photo:{image_type}] {caption[:200] if caption else ''}",
                        response_text=clean_text[:500],
                        variant=f"vision_{image_type}",
                        funnel_stage=stage,
                    )
                except Exception:
                    pass
            else:
                await update.message.reply_text("–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –æ–ø–∏—Å–∞—Ç—å —Å–ª–æ–≤–∞–º–∏ —á—Ç–æ –≤–∞–º –Ω—É–∂–Ω–æ.")
        except Exception as e:
            typing_task.cancel()
            logger.error(f"Photo analysis error: {e}", exc_info=True)
            await update.message.reply_text(
                "–ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å —Ñ–æ—Ç–æ. –û–ø–∏—à–∏—Ç–µ —Å–ª–æ–≤–∞–º–∏ —á—Ç–æ –≤–∞–º –Ω—É–∂–Ω–æ, —è –ø–æ–º–æ–≥—É!"
            )
        return

    photo = update.message.photo[-1] if update.message.photo else None
    if not photo:
        return

    file_id = photo.file_id
    caption = update.message.caption or ""

    try:
        review_id = loyalty_system.submit_review(
            user_id=user_id,
            review_type="text_photo",
            content_url=f"[PHOTO] file_id: {file_id}",
            comment=caption if caption else None
        )

        if review_id:
            context.user_data.pop("pending_review_type", None)

            from src.loyalty import REVIEW_REWARDS
            coins = REVIEW_REWARDS.get("text_photo", 200)

            await update.message.reply_text(
                f"""‚úÖ <b>–û—Ç–∑—ã–≤ —Å —Ñ–æ—Ç–æ –ø—Ä–∏–Ω—è—Ç!</b>

–°–ø–∞—Å–∏–±–æ –∑–∞ –≤–∞—à –æ—Ç–∑—ã–≤! –ü–æ—Å–ª–µ –º–æ–¥–µ—Ä–∞—Ü–∏–∏ –≤—ã –ø–æ–ª—É—á–∏—Ç–µ <b>{coins} –º–æ–Ω–µ—Ç</b>.

–û–±—ã—á–Ω–æ –º–æ–¥–µ—Ä–∞—Ü–∏—è –∑–∞–Ω–∏–º–∞–µ—Ç –¥–æ 24 —á–∞—Å–æ–≤.""",
                parse_mode="HTML",
                reply_markup=get_loyalty_menu_keyboard()
            )

            if MANAGER_CHAT_ID:
                try:
                    manager_text = f"""üì∏ <b>–ù–æ–≤—ã–π —Ç–µ–∫—Å—Ç–æ–≤—ã–π –æ—Ç–∑—ã–≤ —Å —Ñ–æ—Ç–æ!</b>

üë§ {user.first_name or '–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å'} (@{user.username or 'no_username'})
üÜî ID: {user_id}
üí¨ –¢–µ–∫—Å—Ç: {caption or '(–±–µ–∑ –ø–æ–¥–ø–∏—Å–∏)'}"""

                    await context.bot.send_message(
                        chat_id=MANAGER_CHAT_ID,
                        text=manager_text,
                        parse_mode="HTML"
                    )
                    await context.bot.forward_message(
                        chat_id=MANAGER_CHAT_ID,
                        from_chat_id=update.effective_chat.id,
                        message_id=update.message.message_id
                    )
                except Exception as e:
                    logger.warning(f"Failed to notify manager about photo review: {e}")
        else:
            await update.message.reply_text(
                "–í—ã —É–∂–µ –æ—Ç–ø—Ä–∞–≤–ª—è–ª–∏ –æ—Ç–∑—ã–≤ —ç—Ç–æ–≥–æ —Ç–∏–ø–∞ –∏–ª–∏ –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞.",
                reply_markup=get_loyalty_menu_keyboard()
            )
            context.user_data.pop("pending_review_type", None)

    except Exception as e:
        logger.error(f"Error processing photo review: {e}")
        await update.message.reply_text(
            "–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ.",
            reply_markup=get_loyalty_menu_keyboard()
        )
        context.user_data.pop("pending_review_type", None)


async def video_handler(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    user = update.effective_user
    user_id = user.id

    if context.user_data.get('broadcast_compose'):
        from src.security import is_admin
        if is_admin(user.id):
            context.user_data.pop('broadcast_compose', None)
            video = update.message.video or update.message.video_note
            context.user_data['broadcast_draft'] = {
                'type': 'video',
                'file_id': video.file_id,
                'caption': update.message.caption or '',
            }
            from src.broadcast import broadcast_manager
            counts = broadcast_manager.get_audience_counts()
            from src.handlers.utils import get_broadcast_audience_keyboard
            keyboard = get_broadcast_audience_keyboard(counts)
            caption_preview = f"\nüìù {update.message.caption}" if update.message.caption else ""
            await update.message.reply_text(
                f"üìã <b>–ü—Ä–µ–¥–ø—Ä–æ—Å–º–æ—Ç—Ä —Ä–∞—Å—Å—ã–ª–∫–∏:</b>\n\nüé¨ –í–∏–¥–µ–æ{caption_preview}\n\n<b>–í—ã–±–µ—Ä–∏—Ç–µ –∞—É–¥–∏—Ç–æ—Ä–∏—é:</b>",
                parse_mode="HTML",
                reply_markup=keyboard
            )
            return

    pending_review_type = context.user_data.get("pending_review_type")

    if pending_review_type != "video":
        await update.message.reply_text(
            "–ï—Å–ª–∏ —Ö–æ—Ç–∏—Ç–µ –æ—Å—Ç–∞–≤–∏—Ç—å –≤–∏–¥–µ–æ-–æ—Ç–∑—ã–≤, –Ω–∞–∂–º–∏—Ç–µ /bonus ‚Üí –û—Ç–∑—ã–≤—ã –∏ –±–æ–Ω—É—Å—ã ‚Üí –í–∏–¥–µ–æ-–æ—Ç–∑—ã–≤"
        )
        return

    video = update.message.video or update.message.video_note
    if not video:
        return

    file_id = video.file_id

    try:
        review = loyalty_system.submit_review(
            user_id=user_id,
            review_type="video",
            content=f"[VIDEO] file_id: {file_id}"
        )

        if review:
            context.user_data.pop("pending_review_type", None)

            from src.loyalty import REVIEW_REWARDS
            coins = REVIEW_REWARDS.get("video", 500)

            await update.message.reply_text(
                f"""‚úÖ <b>–í–∏–¥–µ–æ-–æ—Ç–∑—ã–≤ –ø—Ä–∏–Ω—è—Ç!</b>

–°–ø–∞—Å–∏–±–æ –∑–∞ –≤–∞—à –æ—Ç–∑—ã–≤! –ü–æ—Å–ª–µ –º–æ–¥–µ—Ä–∞—Ü–∏–∏ –≤—ã –ø–æ–ª—É—á–∏—Ç–µ <b>{coins} –º–æ–Ω–µ—Ç</b>.

–û–±—ã—á–Ω–æ –º–æ–¥–µ—Ä–∞—Ü–∏—è –∑–∞–Ω–∏–º–∞–µ—Ç –¥–æ 24 —á–∞—Å–æ–≤.""",
                parse_mode="HTML",
                reply_markup=get_loyalty_menu_keyboard()
            )

            if MANAGER_CHAT_ID:
                try:
                    manager_text = f"""üé¨ <b>–ù–æ–≤—ã–π –≤–∏–¥–µ–æ-–æ—Ç–∑—ã–≤!</b>

üë§ {user.first_name or '–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å'} (@{user.username or 'no_username'})
üÜî ID: {user_id}"""

                    await context.bot.send_message(
                        chat_id=MANAGER_CHAT_ID,
                        text=manager_text,
                        parse_mode="HTML"
                    )
                    await context.bot.forward_message(
                        chat_id=MANAGER_CHAT_ID,
                        from_chat_id=update.effective_chat.id,
                        message_id=update.message.message_id
                    )
                except Exception as e:
                    logger.warning(f"Failed to notify manager about video review: {e}")
        else:
            await update.message.reply_text(
                "–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –æ—Ç–∑—ã–≤. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ.",
                reply_markup=get_loyalty_menu_keyboard()
            )
            context.user_data.pop("pending_review_type", None)

    except Exception as e:
        logger.error(f"Error processing video review: {e}")
        await update.message.reply_text(
            "–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ.",
            reply_markup=get_loyalty_menu_keyboard()
        )
        context.user_data.pop("pending_review_type", None)
