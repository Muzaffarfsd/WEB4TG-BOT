import asyncio
import logging
from typing import List, Dict, Optional
from google import genai
from google.genai import types
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception

from src.config import config
from src.knowledge_base import SYSTEM_PROMPT

logger = logging.getLogger(__name__)


def is_rate_limit_error(exception: BaseException) -> bool:
    error_msg = str(exception)
    return (
        "429" in error_msg 
        or "RATELIMIT_EXCEEDED" in error_msg
        or "quota" in error_msg.lower() 
        or "rate limit" in error_msg.lower()
        or (hasattr(exception, 'status') and exception.status == 429)
    )


class AIClient:
    def __init__(self):
        self._client = genai.Client(api_key=config.gemini_api_key)

    async def generate_response_stream(
        self,
        messages: List[Dict],
        thinking_level: str = "medium",
        on_chunk=None
    ) -> str:
        if thinking_level == "high":
            model = config.thinking_model_name
            gen_config = types.GenerateContentConfig(
                system_instruction=SYSTEM_PROMPT,
                max_output_tokens=config.max_tokens,
                temperature=config.temperature,
                thinking_config=types.ThinkingConfig(thinking_budget=4096)
            )
        else:
            model = config.fast_model_name
            gen_config = types.GenerateContentConfig(
                system_instruction=SYSTEM_PROMPT,
                max_output_tokens=config.max_tokens,
                temperature=config.temperature
            )

        try:
            import queue
            chunk_queue = queue.Queue()

            def _stream_in_thread():
                full = ""
                try:
                    stream = self._client.models.generate_content_stream(
                        model=model,
                        contents=messages,
                        config=gen_config
                    )
                    for chunk in stream:
                        if chunk.text:
                            full += chunk.text
                            chunk_queue.put(full)
                except Exception:
                    pass
                finally:
                    chunk_queue.put(None)
                return full

            stream_task = asyncio.get_event_loop().run_in_executor(None, _stream_in_thread)

            full_text = ""
            while True:
                try:
                    partial = await asyncio.to_thread(chunk_queue.get, timeout=0.3)
                    if partial is None:
                        break
                    full_text = partial
                    if on_chunk:
                        try:
                            await on_chunk(full_text)
                        except Exception:
                            pass
                except Exception:
                    if stream_task.done():
                        while not chunk_queue.empty():
                            item = chunk_queue.get_nowait()
                            if item is None:
                                break
                            full_text = item
                            if on_chunk:
                                try:
                                    await on_chunk(full_text)
                                except Exception:
                                    pass
                        break

            result = await stream_task
            if result:
                full_text = result

            if full_text:
                return full_text
            return "–ò–∑–≤–∏–Ω–∏—Ç–µ, –Ω–µ —É–¥–∞–ª–æ—Å—å —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –≤–æ–ø—Ä–æ—Å."
        except Exception as e:
            error_type = type(e).__name__
            error_msg = str(e)
            if is_rate_limit_error(e):
                logger.warning(f"Gemini stream rate limit: {error_type}: {error_msg}")
                return "–°–µ–π—á–∞—Å –≤—ã—Å–æ–∫–∞—è –Ω–∞–≥—Ä—É–∑–∫–∞, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ —á–µ—Ä–µ–∑ –º–∏–Ω—É—Ç—É üôè"
            logger.error(f"Gemini stream failed: {error_type}: {error_msg}")
            return await self.generate_response(messages, thinking_level)

    async def generate_response(
        self,
        messages: List[Dict],
        thinking_level: str = "medium",
        max_retries: int = 2,
        retry_delay: float = 0.5
    ) -> str:
        if thinking_level == "high":
            model = config.thinking_model_name
            gen_config = types.GenerateContentConfig(
                system_instruction=SYSTEM_PROMPT,
                max_output_tokens=config.max_tokens,
                temperature=config.temperature,
                thinking_config=types.ThinkingConfig(thinking_budget=4096)
            )
        else:
            model = config.fast_model_name
            gen_config = types.GenerateContentConfig(
                system_instruction=SYSTEM_PROMPT,
                max_output_tokens=config.max_tokens,
                temperature=config.temperature
            )
        
        @retry(
            stop=stop_after_attempt(max_retries),
            wait=wait_exponential(multiplier=retry_delay, min=0.5, max=10),
            retry=retry_if_exception(is_rate_limit_error),
            reraise=True
        )
        async def _generate():
            response = await asyncio.to_thread(
                self._client.models.generate_content,
                model=model,
                contents=messages,
                config=gen_config
            )
            return response
        
        try:
            response = await _generate()
            
            if response.text:
                return response.text
            else:
                logger.warning("Empty response from Gemini")
                return "–ò–∑–≤–∏–Ω–∏—Ç–µ, –Ω–µ —É–¥–∞–ª–æ—Å—å —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –≤–æ–ø—Ä–æ—Å."
                
        except Exception as e:
            error_type = type(e).__name__
            error_msg = str(e)
            
            if is_rate_limit_error(e):
                logger.warning(f"Gemini rate limit hit: {error_type}: {error_msg}")
                return "–°–µ–π—á–∞—Å –≤—ã—Å–æ–∫–∞—è –Ω–∞–≥—Ä—É–∑–∫–∞, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ —á–µ—Ä–µ–∑ –º–∏–Ω—É—Ç—É üôè"
            elif "timeout" in error_msg.lower() or "connect" in error_msg.lower():
                logger.error(f"Gemini connection error: {error_type}: {error_msg}")
                return "–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ —Å–µ—Ä–≤–µ—Ä—É. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ."
            else:
                logger.error(f"Gemini request failed: {error_type}: {error_msg}")
                return "–ü—Ä–æ–∏–∑–æ—à–ª–∞ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â—ë —Ä–∞–∑ –∏–ª–∏ –Ω–∞–ø–∏—à–∏—Ç–µ –ø–æ–∑–∂–µ."
    
    async def generate_response_with_tools(
        self,
        messages: List[Dict],
        thinking_level: str = "medium",
        on_chunk=None
    ) -> dict:
        """Returns {"text": str, "tool_calls": list[dict], "all_tool_calls": list}"""
        try:
            model = config.fast_model_name
            tools = types.Tool(function_declarations=TOOL_DECLARATIONS)
            gen_config = types.GenerateContentConfig(
                system_instruction=SYSTEM_PROMPT,
                max_output_tokens=config.max_tokens,
                temperature=config.temperature,
                tools=[tools],
                tool_config=types.ToolConfig(
                    function_calling_config=types.FunctionCallingConfig(mode='AUTO')
                )
            )

            response = await asyncio.to_thread(
                self._client.models.generate_content,
                model=model,
                contents=messages,
                config=gen_config
            )

            tool_calls = []
            if response.candidates and response.candidates[0].content and response.candidates[0].content.parts:
                for part in response.candidates[0].content.parts:
                    if part.function_call:
                        fc = part.function_call
                        tool_calls.append({
                            "name": fc.name,
                            "args": dict(fc.args) if fc.args else {}
                        })

            if tool_calls:
                return {"text": None, "tool_calls": tool_calls, "all_tool_calls": tool_calls}

            text = response.text if response.text else None
            return {"text": text, "tool_calls": [], "all_tool_calls": []}

        except Exception as e:
            logger.warning(f"Tool calling failed, falling back to regular response: {e}")
            fallback = await self.generate_response(messages, thinking_level)
            return {"text": fallback, "tool_calls": [], "all_tool_calls": []}

    async def agentic_loop(
        self,
        messages: List[Dict],
        tool_executor,
        thinking_level: str = "medium",
        max_steps: int = 4
    ) -> dict:
        """Multi-step agentic loop: AI calls tools, gets results, decides next action.
        
        Returns {"text": str, "special_actions": list, "all_tool_results": list}
        """
        all_tool_results = []
        special_actions = []
        current_messages = list(messages)
        
        for step in range(max_steps):
            result = await self.generate_response_with_tools(
                messages=current_messages,
                thinking_level=thinking_level
            )
            
            if not result["tool_calls"]:
                return {
                    "text": result["text"],
                    "special_actions": special_actions,
                    "all_tool_results": all_tool_results
                }
            
            step_tool_results = []
            for tc in result["tool_calls"]:
                try:
                    tool_result = await tool_executor(tc["name"], tc["args"])
                except Exception as e:
                    tool_result = f"–û—à–∏–±–∫–∞ –≤—ã–∑–æ–≤–∞ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞ {tc['name']}: {e}"
                    logger.error(f"Tool executor error for {tc['name']}: {e}")
                
                if not isinstance(tool_result, str):
                    tool_result = str(tool_result) if tool_result is not None else "–ù–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞"
                
                if tool_result.startswith("[PORTFOLIO:"):
                    special_actions.append(("portfolio", tool_result))
                    step_tool_results.append(f"{tc['name']}: –ø–æ–∫–∞–∑–∞–Ω–æ –ø–æ—Ä—Ç—Ñ–æ–ª–∏–æ")
                elif tool_result == "[PRICING]":
                    special_actions.append(("pricing", None))
                    step_tool_results.append(f"{tc['name']}: –ø–æ–∫–∞–∑–∞–Ω –ø—Ä–∞–π—Å")
                elif tool_result == "[PAYMENT]":
                    special_actions.append(("payment", None))
                    step_tool_results.append(f"{tc['name']}: –ø–æ–∫–∞–∑–∞–Ω–∞ –æ–ø–ª–∞—Ç–∞")
                else:
                    step_tool_results.append(f"{tc['name']}: {tool_result}")
                    all_tool_results.append({"tool": tc["name"], "result": tool_result})
            
            tool_results_text = "\n\n".join(step_tool_results)
            current_messages.append({
                "role": "model",
                "parts": [{"text": f"–Ø –≤—ã–∑–≤–∞–ª –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã:\n{tool_results_text}"}]
            })
            current_messages.append({
                "role": "user",
                "parts": [{"text": "–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã. –ï—Å–ª–∏ –Ω—É–∂–Ω–æ ‚Äî –≤—ã–∑–æ–≤–∏ –µ—â—ë –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞. –ï—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ ‚Äî —Å—Ñ–æ—Ä–º—É–ª–∏—Ä—É–π —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç –∫–ª–∏–µ–Ω—Ç—É."}]
            })
            
            logger.info(f"Agentic loop step {step+1}: {len(result['tool_calls'])} tool calls")
        
        final_response = await self.generate_response(
            messages=current_messages,
            thinking_level=thinking_level
        )
        
        return {
            "text": final_response,
            "special_actions": special_actions,
            "all_tool_results": all_tool_results
        }

    async def analyze_complex_query(
        self,
        query: str,
        context: Optional[str] = None
    ) -> str:
        prompt = query
        if context:
            prompt = f"–ö–æ–Ω—Ç–µ–∫—Å—Ç: {context}\n\n–í–æ–ø—Ä–æ—Å: {query}"
        
        messages = [{"role": "user", "parts": [{"text": prompt}]}]
        return await self.generate_response(messages, thinking_level="high")
    
    async def quick_response(self, query: str) -> str:
        messages = [{"role": "user", "parts": [{"text": query}]}]
        return await self.generate_response(messages, thinking_level="low")


TOOL_DECLARATIONS = [
    {
        "name": "calculate_price",
        "description": "–†–∞—Å—Å—á–∏—Ç–∞—Ç—å —Å—Ç–æ–∏–º–æ—Å—Ç—å —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ Telegram Mini App –ø–æ –Ω–∞–±–æ—Ä—É —Ñ—É–Ω–∫—Ü–∏–π. –í—ã–∑—ã–≤–∞–π –∫–æ–≥–¥–∞ –∫–ª–∏–µ–Ω—Ç —Å–ø—Ä–∞—à–∏–≤–∞–µ—Ç —Ü–µ–Ω—É –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –Ω–∞–±–æ—Ä–∞ —Ñ—É–Ω–∫—Ü–∏–π –∏–ª–∏ —Ö–æ—á–µ—Ç –ø–æ—Å—á–∏—Ç–∞—Ç—å —Å—Ç–æ–∏–º–æ—Å—Ç—å.",
        "parameters": {
            "type": "object",
            "properties": {
                "features": {
                    "type": "array",
                    "items": {"type": "string"},
                    "description": "–°–ø–∏—Å–æ–∫ —Ñ—É–Ω–∫—Ü–∏–π: catalog, cart, auth, search, favorites, reviews, payments, subscriptions, installments, delivery, pickup, express, push, chat, video, loyalty, promo, referral, analytics, admin, crm, booking, queue, calendar, ai, ai_rec, auto_reply, smart_search, voice, tg_bot, whatsapp, maps, sms, email, 1c, api"
                }
            },
            "required": ["features"]
        }
    },
    {
        "name": "show_portfolio",
        "description": "–ü–æ–∫–∞–∑–∞—Ç—å –ø—Ä–∏–º–µ—Ä—ã —Ä–∞–±–æ—Ç –∏–∑ –ø–æ—Ä—Ç—Ñ–æ–ª–∏–æ. –í—ã–∑—ã–≤–∞–π –∫–æ–≥–¥–∞ –∫–ª–∏–µ–Ω—Ç —Ö–æ—á–µ—Ç —É–≤–∏–¥–µ—Ç—å –ø—Ä–∏–º–µ—Ä—ã, –∫–µ–π—Å—ã –∏–ª–∏ –ø–æ—Ä—Ç—Ñ–æ–ª–∏–æ.",
        "parameters": {
            "type": "object",
            "properties": {
                "category": {
                    "type": "string",
                    "enum": ["ecommerce", "services", "fintech", "education", "all"],
                    "description": "–ö–∞—Ç–µ–≥–æ—Ä–∏—è –ø–æ—Ä—Ç—Ñ–æ–ª–∏–æ"
                }
            },
            "required": ["category"]
        }
    },
    {
        "name": "show_pricing",
        "description": "–ü–æ–∫–∞–∑–∞—Ç—å –æ–±—â–∏–π –ø—Ä–∞–π—Å-–ª–∏—Å—Ç —É—Å–ª—É–≥. –í—ã–∑—ã–≤–∞–π –∫–æ–≥–¥–∞ –∫–ª–∏–µ–Ω—Ç —Å–ø—Ä–∞—à–∏–≤–∞–µ—Ç –æ —Ü–µ–Ω–∞—Ö –≤ –æ–±—â–µ–º.",
        "parameters": {
            "type": "object",
            "properties": {}
        }
    },
    {
        "name": "create_lead",
        "description": "–°–æ–∑–¥–∞—Ç—å –∑–∞—è–≤–∫—É –æ—Ç –∫–ª–∏–µ–Ω—Ç–∞. –í—ã–∑—ã–≤–∞–π –∫–æ–≥–¥–∞ –∫–ª–∏–µ–Ω—Ç –≥–æ–≤–æ—Ä–∏—Ç —á—Ç–æ —Ö–æ—á–µ—Ç –∑–∞–∫–∞–∑–∞—Ç—å, –≥–æ—Ç–æ–≤ –Ω–∞—á–∞—Ç—å, –ø—Ä–æ—Å–∏—Ç —Å–≤—è–∑–∞—Ç—å—Å—è —Å –Ω–∏–º.",
        "parameters": {
            "type": "object",
            "properties": {
                "interest": {
                    "type": "string",
                    "description": "–ß—Ç–æ –∏–Ω—Ç–µ—Ä–µ—Å—É–µ—Ç –∫–ª–∏–µ–Ω—Ç–∞"
                },
                "budget": {
                    "type": "string",
                    "description": "–ü—Ä–∏–º–µ—Ä–Ω—ã–π –±—é–¥–∂–µ—Ç, –µ—Å–ª–∏ –æ–∑–≤—É—á–µ–Ω"
                }
            },
            "required": ["interest"]
        }
    },
    {
        "name": "show_payment_info",
        "description": "–ü–æ–∫–∞–∑–∞—Ç—å —Ä–µ–∫–≤–∏–∑–∏—Ç—ã –¥–ª—è –æ–ø–ª–∞—Ç—ã. –í—ã–∑—ã–≤–∞–π –∫–æ–≥–¥–∞ –∫–ª–∏–µ–Ω—Ç –≥–æ—Ç–æ–≤ –æ–ø–ª–∞—Ç–∏—Ç—å –∏–ª–∏ —Å–ø—Ä–∞—à–∏–≤–∞–µ—Ç –∫–∞–∫ –æ–ø–ª–∞—Ç–∏—Ç—å.",
        "parameters": {
            "type": "object",
            "properties": {}
        }
    },
    {
        "name": "calculate_roi",
        "description": "–†–∞—Å—Å—á–∏—Ç–∞—Ç—å –æ–∫—É–ø–∞–µ–º–æ—Å—Ç—å (ROI) Telegram Mini App –¥–ª—è –±–∏–∑–Ω–µ—Å–∞ –∫–ª–∏–µ–Ω—Ç–∞. –í—ã–∑—ã–≤–∞–π –∫–æ–≥–¥–∞ –∫–ª–∏–µ–Ω—Ç —Å–æ–º–Ω–µ–≤–∞–µ—Ç—Å—è –≤ –≤—ã–≥–æ–¥–µ, —Å–ø—Ä–∞—à–∏–≤–∞–µ—Ç '–∑–∞—á–µ–º –º–Ω–µ —ç—Ç–æ' –∏–ª–∏ '–æ–∫—É–ø–∏—Ç—Å—è –ª–∏'.",
        "parameters": {
            "type": "object",
            "properties": {
                "business_type": {
                    "type": "string",
                    "description": "–¢–∏–ø –±–∏–∑–Ω–µ—Å–∞: restaurant, shop, beauty, education, services, fitness, delivery, other"
                },
                "monthly_clients": {
                    "type": "integer",
                    "description": "–ü—Ä–∏–º–µ—Ä–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∏–µ–Ω—Ç–æ–≤ –≤ –º–µ—Å—è—Ü"
                },
                "avg_check": {
                    "type": "integer",
                    "description": "–°—Ä–µ–¥–Ω–∏–π —á–µ–∫ –≤ —Ä—É–±–ª—è—Ö"
                }
            },
            "required": ["business_type"]
        }
    },
    {
        "name": "compare_plans",
        "description": "–°—Ä–∞–≤–Ω–∏—Ç—å —Ç–∞—Ä–∏—Ñ–Ω—ã–µ –ø–ª–∞–Ω—ã –∏ –ø–∞–∫–µ—Ç—ã —É—Å–ª—É–≥. –í—ã–∑—ã–≤–∞–π –∫–æ–≥–¥–∞ –∫–ª–∏–µ–Ω—Ç –Ω–µ –º–æ–∂–µ—Ç –≤—ã–±—Ä–∞—Ç—å –º–µ–∂–¥—É –≤–∞—Ä–∏–∞–Ω—Ç–∞–º–∏ –∏–ª–∏ –ø—Ä–æ—Å–∏—Ç —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ.",
        "parameters": {
            "type": "object",
            "properties": {
                "plan_type": {
                    "type": "string",
                    "enum": ["packages", "subscriptions", "custom_vs_template"],
                    "description": "–ß—Ç–æ —Å—Ä–∞–≤–Ω–∏—Ç—å: packages (MVP/Standard/Premium), subscriptions (–ø–æ–¥–ø–∏—Å–∫–∏), custom_vs_template (–∑–∞–∫–∞–∑–Ω–∞—è vs —à–∞–±–ª–æ–Ω)"
                }
            },
            "required": ["plan_type"]
        }
    },
    {
        "name": "schedule_consultation",
        "description": "–ó–∞–ø–∏—Å–∞—Ç—å –∫–ª–∏–µ–Ω—Ç–∞ –Ω–∞ –±–µ—Å–ø–ª–∞—Ç–Ω—É—é –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏—é —Å –º–µ–Ω–µ–¥–∂–µ—Ä–æ–º. –í—ã–∑—ã–≤–∞–π –∫–æ–≥–¥–∞ –∫–ª–∏–µ–Ω—Ç —Ö–æ—á–µ—Ç –æ–±—Å—É–¥–∏—Ç—å –ø—Ä–æ–µ–∫—Ç –ø–æ–¥—Ä–æ–±–Ω–µ–µ, –∑–∞–¥–∞—ë—Ç —Å–ª–æ–∂–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã –∏–ª–∏ –≥–æ—Ç–æ–≤ –∫ —Å–æ–∑–≤–æ–Ω—É.",
        "parameters": {
            "type": "object",
            "properties": {
                "preferred_time": {
                    "type": "string",
                    "description": "–ü—Ä–µ–¥–ø–æ—á–∏—Ç–∞–µ–º–æ–µ –≤—Ä–µ–º—è (–µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω–æ)"
                },
                "topic": {
                    "type": "string",
                    "description": "–¢–µ–º–∞ –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏–∏"
                }
            },
            "required": ["topic"]
        }
    },
    {
        "name": "generate_brief",
        "description": "–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –∫—Ä–∞—Ç–∫–æ–µ –¢–ó (–±—Ä–∏—Ñ) –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±—Å—É–∂–¥–µ–Ω–∏—è —Å –∫–ª–∏–µ–Ω—Ç–æ–º. –í—ã–∑—ã–≤–∞–π –∫–æ–≥–¥–∞ –∫–ª–∏–µ–Ω—Ç –æ–ø–∏—Å–∞–ª —Å–≤–æ–π –ø—Ä–æ–µ–∫—Ç –∏ –Ω—É–∂–Ω–æ —Ä–µ–∑—é–º–∏—Ä–æ–≤–∞—Ç—å —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è.",
        "parameters": {
            "type": "object",
            "properties": {
                "project_description": {
                    "type": "string",
                    "description": "–û–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞ –∫–ª–∏–µ–Ω—Ç–∞"
                },
                "features": {
                    "type": "array",
                    "items": {"type": "string"},
                    "description": "–°–ø–∏—Å–æ–∫ –Ω—É–∂–Ω—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π"
                },
                "deadline": {
                    "type": "string",
                    "description": "–ñ–µ–ª–∞–µ–º—ã–µ —Å—Ä–æ–∫–∏"
                }
            },
            "required": ["project_description"]
        }
    },
    {
        "name": "check_discount",
        "description": "–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –¥–æ—Å—Ç—É–ø–Ω—ã–µ —Å–∫–∏–¥–∫–∏ –¥–ª—è –∫–ª–∏–µ–Ω—Ç–∞. –í—ã–∑—ã–≤–∞–π –∫–æ–≥–¥–∞ –∫–ª–∏–µ–Ω—Ç —Å–ø—Ä–∞—à–∏–≤–∞–µ—Ç –ø—Ä–æ —Å–∫–∏–¥–∫–∏, –∞–∫—Ü–∏–∏, –ø—Ä–æ–º–æ–∫–æ–¥—ã.",
        "parameters": {
            "type": "object",
            "properties": {}
        }
    }
]


ai_client = AIClient()
